Welcome, uh, to the class. Uh, today we are going to talk about scaling up, uh, graph neural networks. Uh, and in particular, we will be interested in how can we build, uh, graph neural network models that can be applied, uh, to large, uh, scale graphs, and what kind of, um, techniques, uh, can we use, uh, to be able to do that. So, uh, let's first, uh, kind of define and, uh, motivate the problem. So, uh, there are large graphs in many modern applications. Um, if you think about recommender systems, for example, uh, systems that recommend you products at Amazon, systems that recommend you mov- videos to watch on YouTube, uh, pins to look at at Pinterest, posts to examine at Instagram, and so on, um, you can think of these as a- as a- as a way where we wanna connect users, uh, to the content, products, videos that, uh, they might be interested in, right? And, uh, you can think of this recommendation task, um, as a way to, uh, do link prediction, uh, in this, uh, large bipartite graph where you have, you know, hundreds of millions, billions of users on one end, on the other side you have tens of millions, hundreds of millions, tens of billions of items, uh, on the other end, and you would like to predict what, uh, what, uh, mo- what movie, what video, or what product, uh, is, uh, interesting, uh, for what, uh, user, and you can formulate this as a link prediction, uh, task. Another example, uh, where, uh, graph neural networks can be very, uh, uh, well applied is in social networks, for example, uh, Twitter, uh, Facebook, uh, Instagram, where you have users and friend and follow relations, and you want- wanna ask about how do I wanna make, uh, recommend, uh, friends to each other at the link level? Uh, do I wanna do some kind of user property prediction, for example, uh, predict what ads the different users would be interested in or predict what, uh, country they come from, or perhaps I'm doing some kind of attribute imputation in a sense that maybe some- some users tell me their gender but I don't know the gender of other users and I wanna predict it. Um, and again, in this case, these networks have, uh, billions of, uh, users and tens to hundreds of billions, uh, of edges. And then, uh, another application I- you know that can be motivated by what we are going to talk about today is- is a notion of heterogeneous graphs. And, for example, you can- you can- one such, uh, dataset is called Microsoft Academic Graph, which is a set of 120 million papers, 120 million authors, as well as their affiliations, institutions, um, and so on. So, uh, this means now we have a giant heterogenous, uh, knowledge graph on which you can try to do a lot of interesting machine learning tasks. For example, a paper categorization which will be- predict what category, what topic the paper is about, uh, recommend collaborators to authors, uh, predict, uh, what papers should cite each other. You can imagine this to be very useful when you are writing, uh, a new paper or doing a new piece of, uh, research. And these are again, uh, machine learning tasks over this giant heterogeneous graph. And more broadly, right, we can think about, uh, knowledge graphs, the knowledge graphs coming out of Wikipedia, coming out of, uh, Freebase, where again we have, um, 100 million entities and we wanna do knowledge graph completion tasks or we wanna do, uh, knowledge reasoning tasks, uh, over these, uh, uh, over these knowledge graphs. And, you know, what all these, uh, um, applications have in common is that they are large scale, right? That number of nodes is in millions to billions, and, you know, the number of soft edges is, I know, tens- tens or hundreds of millions to, uh, uh, or, uh, to tens- to, uh, hundreds of billions. And the question is, right, like if we have these types of large-scale graph, these large- this set of large-scale data, and we would like to do, um, tasks in- in all these cases that I have motivated, we can do kind of node-level tasks in terms of user, item, paper classification prediction as well as, uh, pair-wise level tasks like, uh, link, uh, recommendation, uh, uh, link prediction, and so on. Um, and, er, the topic of today's class will be about how do we scale up GNNs to- to graphs of this- of this size, right? What kind of systems, what kind of algorithms can be developed that will allow us to learn or over, uh, these kind of, uh, massive amounts, uh, of data? So let me explain why is this hard or why is this non-trivial, um, and it will become clearly- very quickly clear that we need, uh, special methods and special approaches, right? When we have a big, uh, dataset, let just think in general- like in a general deep-learning framework, when we have a large number of N data points, uh, what do we wanna do, right? In- in general, our objective is to minimize the- the- the loss, the average loss, uh, over the training data. So if I have, uh, N training data points, then, you know, I go- I have the summation from 0 to n minus 1 where for every, um, data point i, I'm asking what is the loss of it, right? What is the discrepancy between the true label, the true prediction, and the, um, uh, uh, pre- uh, and the- and the, um, and the prediction made by the model? And- and I would like to compute this total loss, um, under the current model parameters, uh, Theta, so that then as I can compute the loss, I can then compute the gradient with respect to the loss and update, uh, model parameters in such a way that the total loss will be minimized, that the- that the- so basically, the model parameters will change, which means model predictions should change such- such that the loss, the discrepancy between the true labels and the predicted labels, um, gets, uh, smaller. And notice what is the point here, the point here is that we have to sum up these loss contributions over all the training data points. Right? Um, and, uh, because, uh, the number of da- training data points is too big, what we then- what we then do is, um, we would actually select small groups of M data points called the mini-batches and then approximate this big summation with smaller summations where i only goes over the points in the mini-batch, right, uh, over, I know, 100, 1,000, uh, uh, data points we have sampled and then compute the loss over it and then make a gradient step. This means that our, uh, loss, our gradient will be- will be stochastic, it will be imprecise, it will be a bit random because it will depend on what exact, uh, subset, uh, of random points N have we compute- have we selected, but it will be much faster to compute because M, the size of the mini-batch, will be much smaller than the total amount, uh, of data, so our model will, um, converge, uh, faster. Now, what could you do in graph neural networks? You would simply say, if let's say I'm doing a node-level prediction task, let me subsample a set of nodes N, uh, and put them in a mini-batch and let me learn, uh, over them. Um, and, uh, here is the reason why this, for example, won't work in graph neural networks. Right, imagine that in a mini-batch I simply sample some set M of nodes, uh, independently. And what this picture is trying to show, is these are the nodes I'm going to sample. And because, eh, the- the mini-batch is much smaller than the total size- size of the graph, what is going to happen, is that, yes, I'm going to sample these nodes, but the way graph neural networks operate is that they aggregate information from the neighbors, but most likely none or majority of this name neighbors won't be sampled in our, uh, in our mini-batch. So, um, what this means, is that the nodes in a mini-batch will tend to be isolated nodes from each other. And because the GNN generates node embeddings by aggregating, um, information from the neighbors, uh, in the graph, uh, those neighbors won't be part of the- um, of the mini-batch, so there will be nothing to, uh, aggregate and whatever gradient we compute over isolated nodes won't be representative of the- of the full gradient. So this means that the stochastic gradient descent can- can, uh, won't be able to train effectively on graph neural networks. If we do this naive mini-batch implementation, where we simply, uh, select a- a sub-sample of nodes, as I illustrated here, because nodes in the mini-batch won't be connected with each other, so there won't be any message passing to be able, uh- uh to be able to be done between the neighbors, and the - the entire thing, uh, is going to fail. So, um, another way how we could, uh, do this is to say, let's forget mini batching, let's not select a subset of the training data, uh, to compute the gradient on, let's do what is called full-batch implementation, full-batch training. And the problem with full batch training is the following, is that you can think of it that we generate embeddings for all the nodes at the same time. And the important thing here is- to notice that we have embeddings of the nodes at several layers, right? We start at layer 0, which means the embedding of the node is just the node features, then we propagate through the first layer and we get the embeddings of the node at layer 1. Now, to compute the embeddings of the node from layer 2, we have to aggregate the first layer embeddings to compute the second layer, uh, embeddings. So what this means, is that you would first, uh, load the graph in memory and all the features, and then at each layer of the GNN, you would compute the embeddings of all the nodes using the embeddings of all the nodes from the previous layer, right? So that now if I'm computing the layer K, I need to have all the em- embeddings of all the nodes, um, stored for layer K minus 1, so that whenever I'm creating the next layer embedding of a given node, I check who its neighbors are. I take the previous layer embeddings, uh, for those neighbors and aggregate them to create the next layer embedding. And because of this recursive structure, I basically create, um- I need to be able to keep in memory the entire graph plus the embeddings of all the nodes from the previous layer, as well as enough memory to create the embedding so for the nodes, uh, for the next layer. Uh, and then once I have that, I can then compute the loss and I can compute the, perform the gradient descent. So the way you can think of this is, I start with the initial embeddings of the nodes, I perform message-passing, create the embeddings of the nodes at layer K plus 1. Now I put this, uh, here and call this layer K, again, do the message-passing to get to the next, uh, layer. And this- this would be what is called a full-batch implementation of a graph neural network. Um, and why is full-batch implementation problematic? Uh, it is problematic for two reasons. One reason is that it takes, uh, super long and it's kind of, uh, time-wise, data-wise, inefficient to do full-batch training. But imagine, you know, you have all the time in the world, so you say, I don't care if it takes a bit longer. Um, even with that, it is still impossible to implement a full batch training, uh, for large, uh, for large graphs. And the reason for this is because if we want the training to be, uh- uh not- to be kind of, uh, in the order of magnitude of what we are used today, then we wanna use, um, GPUs. So we wanna use um- uh, graphic cards to do this, right? For fast training. But the GPU memory- GPUs have a very special kind of, uh, super-fast, uh, memory, but they have very little of it. They have about 10-20 gigabytes, uh, of memory. And I think the most you can buy today is around 30 gigabytes, uh, bigger GPU cards. And that's, you know, considered super state of the art. Nobody has those graphics cards, right? But the point is that the entire graphs and entire features, if you have a billion nodes, will be far, far bigger, right? If you have a billion nodes, um, and- and, uh, you just spend, let's say one byte, uh, um, uh, uh, or do you have, I don't know, 100 bytes of, uh, features, uh, per node, it will easily- you will need a terabyte of memory, 10 terabytes of, uh, main memory, which is the amount of memory we can get today for large servers using CPUs. So the point is, CPUs have slow computation, large amount of memory, GPUs have very tiny amount of memory, but super-fast. And the point is, if you wanna do full-batch, you need to feed all the features, all the embeddings, and the entire graph structure into the GPU memory, and 10 gigabytes is- is- is- is- is nothing. It's less than what your iPhone has memory. So it is so small that we cannot scale, uh, full-batch implementations beyond, uh, a few thousand node, uh, networks and definitely not to millions and not, uh, to billions. So, um, this motivates the lecture for today, right? So the lecture for today will be, how can we change the way we think of graph neural networks? How do we implement the training? How do we, uh, change the architecture so that we can scale them up, uh, to, uh, large graphs of billions of nodes, so that we can limit this issue with small, um, GPU memory and that we can limit the issue that naive implementation of mini-batching, uh, just doesn't work. So I'm going to talk about two methods. Uh, basically in this lecture, I'm going to talk about three different methods, two of them will be based on performing message-passing over small subgraphs in each mini-batch. So we are going to change how we design mini-batches. Um, and this will lead to fast training time. One- one is technique called neighborhood sampling and the other one is technique called, uh, Cluster-GCN. And then, uh, I'm also going to talk about the third method which is called- which would be about simplifying the GCN, uh, in such a way that we simplified the architecture so that, uh, operations, uh, computation can be efficiently performed on a CPU. And because CPU has a lot of memory, we can scale, uh, to a large graph. 
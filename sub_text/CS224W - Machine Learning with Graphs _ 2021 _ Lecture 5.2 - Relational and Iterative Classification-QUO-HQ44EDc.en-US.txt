So first, we are going to talk about relational classification and iterative classification. So let me explain to you, uh, the ideas, uh, for this part of the lecture. So first, we wanna talk about what is, uh, relation classification, then we'll talk about iterative classification, and then last, I'm going to talk about, uh, belief propagation as I- as I said, uh, earlier. So, uh, probabilistic relational classifier, the idea is the following. Uh, the class probability, um, Y_v of node v is a weighted average of class probabilities of its neighbors, right? So this means that for labeled nodes we are going to fix the class label to be the ground-truth label, the label we are given, and then for, um, unlabeled nodes, we are just initialize the belief that, let's say, they are- uh, they have the color green to be, let say, 0.5, so something, uh, uniform. And then nodes are going to update their belief about what color they are about, based on the- on the colors of the, eh, nodes, uh, in the network, uh, around them. And this is going to iterate until it, uh, converges. And just to note, here we are go- only going to use node labels and network structure, but we have not yet going to use node attributes. So here, this works even if nodes only have colors and there are no, uh, attributes, uh, attached to the nodes, or no features attached to the nodes. So how are we going to do this, uh, update? We are going to, uh, update this formula that I- that I show- uh, that I show here, and the idea is the following. We say that probability that my node of interest v is of class C, is simply, um, 1 over the sum over, let's say, the weights of the edges, um, uh, that- that are adjacent to our node v. So this is basically, uh, counts the degree or the in-degree of node v. Um, and then we say, now let's sum up over this same, uh, set of edges. Here is the weight of every edge, uh times the probability that the neighbor beho- belongs to the same class C. So basically, here we are summing all the neighboring nodes, u of node v. We ask what is the weight of the edge times the probability, the likelihood that node u belongs to class C? So basically what this is saying, is that the node v says the- the likelihood that I belong to a given class C, is the average likelihood that my neighbors belong to that same class C. Uh, and that's it, right? So in some sense here, a node is going to update its belief for a prediction about its own label, about the beliefs or predictions about the labels of its nearby nodes. And here we are making the most basic assumption, which is that the two nodes share a label if they are connected in the- in the network, which is here. And, you know, we can now think of A_uv as a zero, one, kind of as an unweighted graph, or we can think of this as a weight, um, and then, you know, now different nodes, u have different influence on the label of v, because of the co- different connection strength. And this is just a normalization, so that this, uh, weight that summation, uh, is between, uh, zero and one. There are a few just important things to note, is that convergence of this formula is not guaranteed. Um, and also notice as I said, this model cannot use node featured information. It is just using the labels of the nodes and also uses the network information. How does it use the network information? It uses it because these summations go over the edges or over the neighbors, uh, in the network. That's how network information is used. So let me give you now one example how one could go and, uh, implement the simple method. First, you know, we are going to, uh, initialize all labeled nodes, uh, with their, uh, true labels, and we are going to fix this. So this probabilities of green and red nodes remain fixed. So maybe it says, uh, node 7 belongs to green class with probability 1, while the node, uh, 2 belongs to green class with probability 0, because it's red. Right? Um, and then all the unlabeled nodes here, we will assign their, uh, probability to belonging to, uh, to class, uh, green, to the positive class to be, uh, to be 0.5., Right? Nodes are unde- undecided which class they wanna, uh, belong to. So now what we are going to do, is we're going to update the- thi- this probabilities P of the, uh, gray nodes. So let me show you how we are going to do this. Um, of course, when we will be doing this, we need to come up with some order in which we are going to update the nodes. And in our case, let's assume that our order is exactly based on node ID, so it's 1, 2, 3, 4, 5, and so on. So first, we are going to update node 3. And node 3 says the following. It says, "Aha, I have, uh, three neighbors. Um, you know, um, and, uh, how I- how am I going, uh, to do this? I have, um, you know, uh, the- the- two neighbors of me have the probability zero of belonging to the green class, and this- uh, and I have one gray- gray member that has 0.5. So this is 0, 0 plus 0.5, and because I have three neighbors and my graph is unweighted, so this is divided by 3. So now the- the node number 3 has a probability of belonging to green class, uh, o- of 0.17." Now there is- you know, now node 4 wakes up and we're going to update its own, uh, predicted probability of being green. And again, it goes over its four neighbors and we sum up at zero, uh, this is because of this one. Then we sum up 0.17, 0.5, and 1, and divide it by 4. So the, um, uh, node number 4 will say, "my likelihood or my probability that I am, um, labeled green is 0.42." Now, um, now that node 4 has, uh, updated its label, then node 5 is going to update its label, again, summing over its neighbors, ah, and, you know, it will- it will come up with a belief that its probability of being green is 0.73. Then, you know, node 8 is going to update its label and node 9 is going to update its label. And after, you know, the first iteration, updates of all unlabeled nodes, uh, these are our, uh, beliefs, our predicted probabilities of them being green. Again, the- the pre-labeled nodes, they don't get to update, they remain fixed, and all the gray nodes gets to update. And you- for example, node number 9, thinks it's green because the only neighbor it has is green with probability one, so nine is also very sure it's- uh, it's green. Um, while for example, the pa- the eight is a bit less because it has one node who is not- uh, neighboring one node, node number 5, and that node is not super entirely sure about its color yet. So this is, uh, one iteration. Now we do the second iteration when now again, node 3 is going to- to update its by summing 0, 0- 0, 0, um, and 0.42, and dividing it by 3. So what is going to happen, is this probability increases a bit more than, uh, you know, node 4 updated itself, five updated, uh, and eight updated. And nine updated, but its portability didn't change, so we say node- node 9 has converged. It converged to its belief and that's how it would stay. So now you notice how the, uh, beliefs have, uh, changed across the network. And now I can run this for another, uh, iteration, where again, I've update nodes in the order 3, 4, 5, uh, 8. And as I update them, uh, you know, the probability's updated a bit again, uh, but node 8 did not change its, uh, probability, so we say it converged and we fix it. Right? So now, I still have three nodes and I will update this for another iteration, right? Um, and in the other iteration, the four- five and three don't change, so they have also converged. So now, uh, node 3 beliefs it's of green class with probability 0.16. Um, five thinks it's 0.86, so quite high. And then, you know, po- no- node number 4 is undetermined. It thinks it's a bit- leaning a bit more towards green, but it's, uh, but it's unclear. So, um, if I run for one more iteration, um, then the probabilities, um, on these classification task will converge. Here is what- the- the values they converge to, so what do we conclude? We conclude that, uh, 9,8, and 5, uh, belong- uh, belong to green class. Uh, node 4 also belongs to green class because it's just above 0.5, but very slightly. So four is also green. And three says, "You know, I have a very low belief that I am of green color, " so three is of red color, right? So our predictions now would be that 4- 4, 5, uh, 8, and belong to, uh, green color, and three belongs to, uh, red color. And we did this by basically just doing this, um, iterative classification by propagating this label information and nodes updating its belief about its label, based on the labels of other nodes. So in this approach, we used the notion of the network, but we did not use the notion that nodes have any kind of features or any kind of, uh- uh, signal attached to them. So this was in terms of relational classification based on the labels alone. And you saw how basically the- the probabilities kind of spread from labeled nodes to unlabeled nodes through these- um, through this iterative classification, where nodes were updating, uh, the probabilities, based on the probabilities of their nearby nodes or their neighbors in the network. So now, we are going to look at the iterative, uh, classification, a different kind of type of approach that will use more, uh, of the information. In particular, it's going to use both the node features as well as the labels, uh, of the nearby nodes. So this will now combine both the network as well as the, uh, um, the feature information of the nodes. So in the relational classifiers that we have just talked about, they do not use node attributes, they just use the network structure and the labels. So now the question is, how do we label? How do we take advantage? How do we harness the information in node attributes? And the main idea of iterative classification, is to classify node v based on its attributes f, as well as the labels z of the neighboring nodes, uh, N sub v of this, uh, node v of interest. So how is this going to work? On the input, we are given a graph. Each node will have a feature vector associated with it, and some nodes will be labeled with, uh, labels y. The task is to predict labels of unlabeled nodes and the way we are going to do this is that we are going to train two classifiers. We are going first to train a base classifier that is going to predict a label of a node, uh, based on its feature vector alone. That's the classifier Phi_1. And then, we are also going to train another classifier that will have two inputs. It will have inputs about the features of node v and it is also going to have this additional input, this vector z, that will su- summarize the labels of v's neighbors. So this means that now we are making predictions with this classify our Phi_2, um, using two- two types of input. One is the feature information, uh, captured in this feature vector f, and, uh, also the labels of the name of its neighbors captured in the vector, uh, z. And this way, this approach will now be using both the feature information as well as the network information. So let me tell you now how do we compute these labels summary vector z, uh, of a- of a given node v. So the idea will be that, you know, I'm a given node here in the network, let's say blue and I have, um, some nodes that believe they are green and other nodes that believe they are red. So now what I need is to create this- this- this summary vector z, uh, that will tell me- what are the beliefs, what are- what does my- what do my neighbors think about their labels. So for example, there are many ch- choices I can make to determine- to set this, uh, vector, er, z. For example, I could simply have it to be a vector, to be a histogram which would count the number or the fraction of each label in the neighborhood. So for example, in this case, um, for this node, uh, the blue node we could say a-ha, I have two neighbors who think they are green, and I have one neighbor who thinks it's red. Or another way would be to say, you know, 66 percent of my, um, neighbors think they are green, and 33 think they are red. And this is what this vector z would capture. We could also, for example, um, extend z to say what is the most common label among, uh, these neighbors, or how many different labels are among these neighbors. But essentially here the idea is that this vector z captures how the labels around the node of interest, uh, v are- are distributed among, uh, the neighbors of node v in the network. Um, and there are a lot of different, uh, choices, uh, how we come up the, uh- how we can come up with this vector. Um, so this iterative classifiers are going now to work in two steps. In the first step or in the first phase, we are going to classify node labels based on node attributes alone. So it means using the training data, we are going to train two classifiers. For example, using a linear classifier, a neural network, or support vector machine or a decision tree that given a feature vector of a given node, it's going to predict its label. And in phase 1, we are going to apply this classifier Phi_1, so that now every node in the network has some predicted, uh, label. And then using the training data, we are also going to train this classifier Phi_2, that is going to use two inputs. It's going to use the feature of the node of interest, as well as this summary vector z that captures or summarizes the labels of the nodes, um, in it's network neighborhood. And this Phi_2 is going to predict the label of the node of interest v based on its features, as well as the summary z of its, uh- of the labels of its neighbors. And then- right, so we have first applied Phi_1 then we also have trained Phi_2 so that now we will go into phase 2 where we are going to iterate and we are going to iterate until convergence, uh, the following- uh, the following, uh, iteration where on the, uh, test set, which means on the unlabeled nodes. Uh, we are going to use this, uh, first, the classifier Phi_1 to assign initial, uh, labels. We are going to compute the label summaries z, and then we are going to predict the labels with the second classifier, Phi_2. And we are going now to repeat this process until it converges in a sense that we are going to update the vector z. We are going to, uh, apply the classifier Phi_2 to update the label of the node. And, uh, now the no- node labels have updated, we are going to update the label summaries z and again apply Phi_2 to update the node predi- the- the node predictions, update z, update the prediction, and keep iterating this until this, uh, class labels stabilize, converge or some maximum number of iterations is reached. Um, note that in general or in worse-case, uh, convergence is not guaranteed, so we would set some maximum, uh, number of iterations, you know, maybe 10, 50, 100, something like that, something like that. Not too many- not too many. Um, so that's the idea. So what I wanna do next is actually show you this to give you a sense how this work on a simple, uh, toy example. So the idea here is that we have an input graph, uh, let's say that we are thinking about web pages. So we have a graph of how web pages link to each other. Each node will be a web page. An edge will be a hyperlink, uh, between the web pages. And, uh, uh, we will have a directed edge, which means that one page points to another page. And imagine that every node in my network is, uh, is, uh, is described with the set of features. In our case, you know, we could simply assume that these features captures, uh, what words, what texts a web page is using. And imagine that the task we wanna do is to predict the topic of the web page. And what we would like to do in this prediction is use two facts. First is, of course, we'd like to use the words of the web page to predict the topic. And then the second thing we would like to do is we also can assume that pages that are on similar topics or on the same topic, they tend to link one another. So when we make a prediction about the topic of a page, we don't only rely on the words that the page users itself. But if you also wanna rely on the labels, on the topics of the pages that link to this, uh, page of interest. So that is, uh, the idea. Uh, this is what we would like to do. So let me now, uh, show you how- how this would work, uh, in this, uh, example of a web page classification. So the idea here is that first, we are going to train this baseline classifier, this classifier Phi 1, that is going to- to use and classify pages, classify nodes based on their attributes. So just to give you uh, to explain what- what is happening here, we have this graph on four nodes. We will do the following. It's- the color will denote the ground truth color. So this is the ground truth topic of the real topic of the web-page. Um, the- the- then each- each page also has some feature- feature vector, let's say describing the words on that page. And let's assume these are the feature vectors of my four pages. For- the pages also have hyperlinks that are directed and point to each other. And then for example, based on the labeled data, we can uh, apply our Phi 1 and Phi 1 will say, this- this web-page belongs to topic o and you know, the ground truth is zero, so this is a correct classification. But for example, this- this web-page here, uh, based on it's features alone um, we will predict that it is ah, of topic 0, but in reality it is of topic 1. And the question will be, can we using kind of network information- could network information help us to decide that this should really be green topic, not the red topic as it is predicted based on the features alone. And then we have this four other pages, here are their corresponding featured vectors um, and you know, here the classifier would predict that they're both green um, while here the classifier would predict that these two should be red and, perhaps, you know, one way to think of this is that it's really whether the first feature uh, is set to one, you're red and if it's set to zero, you are green. That would be one way to think of the uh, what the classifier is learning. So now, in the first step of this or in the first phase, um, we- we came up with predictions of the labels uh, based only on the features alone. So we applied the classifier Phi 1 from the previous- from the previous slide. So now what we wanna do next is we wanna create our feet labeled summary vectors z. So for each node v, we are going to create this vector z that captures the neighborhood labels. And given that this is a directed graph, we are going to use a four-dimensional vector to capture the statistics. Basically, we are going to have two parts to this vector z; a part I and O. I is about incoming and O is about outgoing neighbor labeling formation. And we will say that for example, I of 0 is set to 1 if at least one of the incoming pages- one of the pages that links to this node of interest is also labeled 0. And we will use similar definitions for I1, O0, and O1. So give you an idea. So this is now same graph as before. These are the features of the web page, but now this four more uh, numbers appear here and let me again explain what do they mean. So this is now- this four numbers are basically my vector z that summarizes the labels of the neighbors around it. And we have I, which is for incoming neighbors, I wanna say is any of the incoming neighbors uh, of plus zero, is any of the incoming neighbors of plus one, because this node here has one neighbor that is green, we set value one here, right? It's an incoming neighbor. And then on the- on the uh, outgoing neighbor side, this is this edge, we have one neighbor that believes it's class is zero. So here it is, right? So notice that colors correspond to ground truth and the numbers correspond to predicted labels, right? So that's why here I have one, zero because the node of interest has one outgoing edge to our node of class zero and has zero outgoing edges to nodes of class 1 of the green class. Similarly, for example, you can look at this node here, uh, it's feature vector and then the summary of the incoming edges and the summary of the outgoing edges, this- this node has here- has one incoming neighbor that is of class zero. This is this one and has zero incoming edges of class 1 and in terms of the outgoing edges, it has zero outgoing edges to class, uh, to class 0, and it has one outgoing edge to class 1. So this is now, you know, the four-dimensional representation, uh, the vector z, the summary of the labels around this particular node. Analogously, you can compute it for this node and you can compute it for that node. So now that you have for every node both the vector f and the vector z, what you do, you now train, uh, the second um, uh, classifier. And again you only train it on the labeled training set. And this classifier, you train in such a way that it uses both the information in f as well as the information in z. So now you are basically training the classifier Phi 2 that uses both the feature information as well as the label summary information about what do the neighbors of a node of interest think about their own labels. And this now gives me the classifier Phi 2. So now um, in the second step, I'm going now to apply my uh, classifier Phi 2 on the- on the graph of interest on the unlabeled nodes. So right, so we've- I'm going to use trained node features to apply the classifier Phi 1 on the unlabeled- unlabeled set and I'm going to predict the labels. And now- now I'm going to use trained node feature vectors as well to- to predict the labels and now given the predicted labels, I'm going to um, update the feature descriptors- the feature descriptors in a sec- in a sense of uh, class summaries around each node. Now that I have created this vector z, I can now apply my classifier Phi 2 to update the predicted label of every node. And if I do this, the labels of certain nodes may change. And basically the idea is right now that I have used all these data, meaning the feature vector as well as the summary vector of the labels around the given node, I will be able to more robustly predict the label of the node of interest. And now, right, I- basically the idea is that now I can go and update the summaries because some labels might have changed. I update the summary and reapply the Phi 2 predictor- Phi 2 classifier. And I keep doing this, basically reclassifying nodes with Phi 2 until the process converges. Right? So I'll be updating this until- until the label prediction stabilize. I think because if our prediction for a given node changes, then its vector z is also going to change. And if the vector z is going- changes then the- I have to apply the predictor Phi 2 to update the belief or the prediction about the label of a given node. And I keep iterating this until the process stabilized, right? So here the prediction for this node flipped from 0 to 1. Now this vectors z have changed, have been updated. So I have to re-update my classifier Phi 2 again over all these nodes. And I keep doing this until the process converges meaning until no node labels change, or until some maximum number of iterations is reached. And that is essentially the idea of what we are trying to do here and how do we arrive to the final prediction. So to summarize, so far, we have talked about two approaches to collective classification. First, we talked about relational classification, where we are iteratively updating probabilities of nodes belonging to a labeled class based on the labels of its neighbors. This approach uses the network structure and the labels of the nodes, but it does not use the features of the nodes. So then in the second part, we talked about iterative classification, where we improve over the relational classification to handle attribute or feature information of nodes as well. And here the idea was that we classify a given node, v or a given node i, based on its features, as well as the labels of its neighbors. So here, the way we achieve these was to train two classifiers. The initial classifier that given the features of a node predicts its label. Now that the nodes have its labels predicted, for every node, we can now create this summary vector z that describes the labels of the nodes that are neighbors of a node of interest. And then that we have that, we can then use the classifier Phi 2 to re-update the prediction on a given node. And this classifier Phi 2, uses both the feature information of the node as well as the summary vector of the labels of its neighbors, the summary vector z. And we then keep applying this classifier Phi 2 over the network until the entire process converges or until the entire- the maximum number of iterations is reached. 
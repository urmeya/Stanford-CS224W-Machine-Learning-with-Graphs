It is great to have, uh, uh, everyone here today, and, uh, we are going to continue, um, our investigation of, uh, machine learning with graphs. And in particular today we're going to look at how we can represent graph as a matrix. And, um, how- uh, we- we will then define the notion of PageRank, Random Walks and then connect it back to the previous lecture, when we talked about, uh, node embeddings. So this is, uh, the plan for, uh, today. So let's start. Today, I wanna talk about looking at graph as a matrix. So we- we are going to investigate the graph analysis and learning from the matrix linear algebra perspective. And we are going to treat graph as a matrix, and this will allow us to, uh, define node importance via random walks, and this is the famous PageRank algorithm. We are then going to extend this into the notion of matrix factorization, and then we are going to see how node embeddings, meaning node2vec and DeepWalk are essentially a form of, uh, implicit, uh, matrix factorization. Um, and, um, this is the exciting thing here, as in some sense will bring together graphs, linear algebra, as well as node embeddings, uh, that we have, uh, discussed in the last lecture and show how they are all, uh, closely-related. So first, let's talk about PageRank, which is basically the main innovation that let Google Search become, uh, Google Search. And it was developed by two students here at Stanford Computer Science, uh, Larry Page and, uh, Sergey Brin. So let me tell you about, uh, what- what did they did as PhD students at Stanford. So first is, we'll- we will talk about the context of Google and we'll talk about the context of the web, uh, search and, uh, thinking about the web as a graph. So the question is, what does the web look like, uh, at the global level, right? Like how would you represent the web? What do I mean by the web? Is I want to represent it as a graph, where nodes would be web pages and links between the nodes would be hyperlinks so that you can navigate from one web page to another. Um, of course, today, the web has evolved a lot and there is- there is the issue, you know, what is a node? We have a lot of dynamic pages created on the fly, and there is also a lot of the dark web, so a lot of, uh, places, uh, that are password protected, that are inaccessible, uh, by basically us just browsing the web. But let's think of the web, you know, as it was in the older days when we had a set of static pages. You could imagine, here are some, um, static pages, uh, that ap- that a- that appear at, uh, Stanford University. And then, you know, the static pages have hyperlinks, uh, so basically, links that you can click that move you to the next page, right? So in the early days, this is how- uh, how basically web was structured with a lot of static web pages and a lot of these navigational, uh, links. Today, many links are transactional, meaning they are used not to navigate from page to page, but they are used to make transactions like post something, comment something, like something, buy something, and then you get moved to the next page. But let's today focus on these navigational, uh, set of links. Um, and this means that we can now represent the web as this type of directed graph of web pages and, uh, hyperlinks, uh, between them. And the- there are many other places or other types of information that you can represent in the same way. For example, you can take citations where nodes would be, uh, papers, and a paper is citing another paper, which means that is a directed link from the source paper to the paper it cites, and this is called citation networks. And, uh, also you could take let say, references in- in Encyclopedia like Wikipedia, and represent them as a graph of how one concept is related or links to another concept. So the, you know, the Wikipedia as we know it, um, is a perfect example, uh, of this. So now the question is, what does the- what does the web look like? What is kind of the- the map of the web? And, uh, how do we- and- can analyze and understand the web, uh, as a directed graph? So essentially, we are going to represent this as- as- as- as a set of no- nodes based vertices, uh, web pages and a set of directed hyperlinks, uh, between them. So, um, the important question from the viewpoint of let say, web searches to understand what nodes on the web are more important than then the others, right? Like, um, some unknown domain name might be a priori less important, less trustworthy than, uh, well-established domain name, well-established website that a lot of other, uh, web pages link. So, you know, thispersondoesnotexist.com versus stanford.edu, they might have very different a priori, uh, importance. And because there is this large diversity in the web-graph connectivity, the question is, could I somehow rank the pages using the web-graph link structure in a sense, which ones are more important, more popular, more trustworthy, and which ones are less important, less trustworthy, in a- in a sense that then when I, let say, rank search results, I can use this as a signal into what to put on the top and what to put on the bottom? That is essentially the- uh, the idea and the goal, how this was developed, uh, you know, uh, 20 years ago. So, um, what are we going to discuss today is a set of link analysis algorithms or approaches to link analysis, uh, where the goal is to compute importance of nodes in a graph. So, uh, we are going to talk about PageRank, then an extension of it called personalized PageRank, and then a further kind of extensional variant called Random Walk with Restarts. But essentially, all these three different approaches algorithms under the hood are the same thing and they only differ in one slight variation that I'm going to explain. So we are going to talk about the PageRank algorithm, the personalized PageRank algorithm, as well as, uh, a Random Walk with Restarts, uh, as it is called. So, um, first we- our goal is to, uh, compute the importance of a web page on the web. And one way to do this would be to say, the idea would be that, uh, we think of links, uh, as votes. So basically, a page is more important if it has more, uh, links. And then you could say, is it incoming links or outgoing links, right? Incoming links are kind of harder to fake because other people on the web have to link to you, and outgoing links are easier to kind of fake because you can just generate them on your website. So you would say, perhaps, let's use links-in-links as votes. So you could say, "Aha, I know stanford.edu has, you know, 23,000 in-links and thispersondoesnotexist.com has one in-link, so perhaps stanford.edu is more important." And then in the- in the next iteration of this type of thinking, you can say, "Oh, but all in-links are not equal, like an in-link from an important page should count more." And this is now recursive because you say the- the- the confidence, the- the amount of a vote I receive from someone depends on their importance. And that same- that same web page will say, "Aha, my important depends on the incom- on the incoming votes of the importances of the pages that link to me." So kind of, this is now a recursive question because every no- importance of every node depends on the importance of other nodes that link to it. So, um, this is the interesting, uh, approach carries this recursive nature of this. So let's- uh, let's work it out mathematically and figure out how, uh, we can do this. So the idea is that we wanna, uh, that the- a vote, a link from an important page is worth more. So the way we formalize this is to say that each link's vote is proportional to the importance of the source page, all right, because these are directed links. And so, if a page i, ah, has an importance, ah, let's say r_i, ah, and it has d_i out-links, each of its targets, each of its endpoints, ah, each link gets, ah, ah, r_i divided by d_i, ah, fraction of votes. So essentially, this means every page has some, uh, importance and then its importance gets equally split along its out-links. So in this example, for example, k takes its importance because it has four out-links. It splits it four ways, and 1/4 of it flows along this edge. And then, node i here has, ah, three, ah, out-links, so, ah, its, ah, its importance r_i gets split three ways and 1/3 flows over this link. So the, let's say, importance of this node r_j is now the sum of the in-links, ah, that it receives. So is- so it's r_i divided by 3 plus r_k divided by 4, because these are the two in-links. And then, similarly, now r_j, um, node j, takes its importance, and it- because it has three out-links, it divides it three ways and sends it forward to the light blue nodes, right? And this is essentially the idea here. So, ah, your importance, you collect it from the people that points to you. And then, you take your importance, you split it, um, equally among everyone you point to and pass it on to them. So that's the- that's the, uh, idea of this flow model of, ah, PageRank. So, what this means is that a page is important if it is pointed to by other important pages. Um, and we can define the notion of a rank r_j of a page or of node j simply as a- as a summation over- over the nodes i that point to j, ah, importance of each node i divided by- by its out-degree, ah, d. And- and this way, um, we can now written it out, ah, in terms of this type of, uh, summation. To give you an example, if I have these, you know, small, uh, small example of the web graph, uh, back in the old days when web was very small, and there were just, you know, three web pages on the web, and perhaps this is the hyperlink structure of that, uh, web graph, um, then you could say, Aha, here is how I can write this out, right? Importance of y is half of the- half of the, uh, importance of y, because of the self-loop, uh, plus 1/2 of the importance of a, because a has two out-links, one to y and one to m. The importance of a is, ah, you know, it's importance of m, because m has one out-link and it's, uh, uh, half importance of y, because y has two out-links, one to a and one self-loop. And then, importance of m is simply half importance of a, because again, a has two, uh, out-links, right? So now, uh, you can say, Aha, I have three unknowns and I have three equations, of why don't I solve this using, uh, uh, Gaussian elimination? Basically, why don't I solve this as a system of equations? And in principle, yes, you could- you could do this. You need the fourth constraint that you could say, Oh, these importances have to sum to, uh, to be equal to one, and you could use a large-scale equation solver, but this is kind of a bad idea. Nobody does it this way because it's not scalable. So there exists a much more elegant way to solve this system of equations, and I'm going to show you, uh, how to do that. So, uh, to be able to do that, we are now not- we will stop looking at the graph as these set of nodes and edges, but we are going to represent it as a matrix. And we will define this notion of stochastic adjacency matrix M, where if a page j has, uh, out-degree d_j, then, you know, in the entries of the- of the- of the matrix, so if i poi-, i- if j points to i, then the entry i_j of the matrix M will have the value of 1 over d_j. So this means this matrix is what is called column stochastic, right? Every column represents two, uh, out-links of the node, um, and, uh, each n, number of non-zero entries, is the number of out-links of that node j, and the value is 1 over, uh, d su- d_j. So it means that these entries, because there is d of them- d_j of them, and each one has value 1 over d_j, they will exactly say, uh, be, um, say, uh, uh, summed up to have value one. So this is why this is called column stochastic, because every column of this matrix sums up to one. So it means every column you can think of it as a probability distribution over the neighboring nodes, right? So this is now the definition of matrix, uh, m. So now, we are also going to define this notion of a rank vector that will have one entry per page, and the- the entry i of this vector r will be the importance score of page i. All right, so, ah, basically, matrix M has size n by n number of nodes, and then ve- vector r has also, uh, the size number of nodes, where for every node there is an entry telling- telling or storing how important is that node. Um, and one other thing we will do is we are going to say that the entries of the vector r have to sum to one. So again, the way you can think of this is that this is a probability distribution of, uh, over the nodes, uh, in the network. And then what is interesting is that the equations I- I showed, uh, on the previous slide can actually now be written in this very simple matrix form that r equals M times r, right? So basically, you say, Aha, you know, entry of, uh, of, uh, j is simply a sum, um, over the- over the nodes i that, uh, point to the, uh, to the node j. And in the matrix form, this is how, uh, you would save it- say it because you basically go over the- over the row and sum of the importances of the nodes, uh, that points to you. So, um, you know, these two representations are equivalent. We can write it as a- as an equation, or we can write it in this, uh, matrix form. So now, let me just give you an example how this would look like. Here is my previous, you know, example of the web graph on three nodes. Here is my system of equations that I had before. And now, this is my, er, stochastic adjacency matrix, um, meaning that you notice that every column now sums to one. Um, you, um, and then the way I could write the- the- the system of equations is simply to say r equals M times r. And if you- if you see this, for example, y is, you know, 1/2- 1/2, uh, uh, of r_y plus 1/2 of, um, uh, r_a, which is exactly what this equation says, right? So basically, these two representations, uh, are equivalent. So now, I want to kind of stop this mathematical discussion and give you some intuition. And the int- intuition here will be the connection back to random walks that we talked about, uh, in the last lecture. So, let me make a connection to random walks and see how this mathematical formulation of this Gaussian system of equation, um, how- how is that really corresponds to random walks. And this is so fascinating, because it's mathematically kind of the same thing, algorithmically, the same thing, but we can think about it in so many different ways and we can use so many different intuitions about the same kind of underlying, uh, object or underlying thing. So let's, uh, let's kind of, uh, uh, pause a bit and think about, uh, this hypothetical case of a random surfer, uh, on the web. So the idea is a random surfer would be a- would be a person who basically randomly navigates the web pages of the web. So this is, you know, somebody or this surfer who at, you know, at some time t is one of the pages on the web. And then because this person is, let's say, I know, so bored, they decide to randomly surf the web. What this means is that when they are at page i, they- then they at next time stay- they decide to pick an out-link of this page i, uh, uniformly at random and- and click it and this means they navigate now to the new page, uh, j, right? So the idea is, again, I'm, uh, uh, re- I have this random walk, this random surfer, that whenever the surfer arrives to a page, picks one of its outgoing links uniformly at random and navigates, uh, to the target. And this process, ah, you know, repeats indefinitely. It kind of runs, ah, infinitely long. So let's try to ask where is the web surfer, right? So this random surfer surfs the web, at what node is this web surfer? So let's say that p of t is a vector whose ith coordinate is the probability that a surfer is at page i at time t. So what this is means is that p of t is a probability distribution over pages. It tells me with what probability is a s- a web surfer at that given page at time t. So now, let's work it out and ask, Okay, so how- how likely- how can I think about this page j, right? This page j, the way we formulated it in the, um, in the, ah, in our, uh, PageRank algorithm is to say that there are these, uh, pages i. Each page i has importance. And then, you know, 1 over d out-degree of this page i gets sent to the, uh, to the page, uh, j. And this is the key to be able to make the correspondence between the random walk and these, uh, flow-based equations that I talked about. So let me tell you how to think about this. So we want to compute where is the surfer at time t plus 1. And basically, the way we know where is surfer at time t plus 1 is the following, right? Is, um, the surfer follows links uniformly at random. So this means that, um, wherever is the surfer at time t, the surfer will- will pick, uh, out-links uniformly at random and navigate over them. So the way you can say, Aha, how likely is surfer at time t plus 1 out-link j, the, ah, the answer to that is it is the- the likelihood that the surfer was what, at node i_1 and then, you know, picked a random link out of i_1 to arrive to j plus the probability they were at, uh, i_2 and picked a random out-link that led them to j, or they were at i_3, and again, they were lucky to pick the right out-link to get them to j. So the way you can write this out is exactly the following. You say this is the stochastic, um, um, ah, adjacency matrix that we talked about. This is the probability vector where the- where the random walker was at the previous time step, and this gives you now the probability distribution of where the walker, ah, will be in the next time step. Right? So, um, you notice how now we can basically say the- the random walker started somewhere, we multiply it with matrix M, and we get the probability distribution where the walker is going to be in the next time step. So now, let's suppose that this random walk process reaches a steady state. What do I mean by that? Is, um, that the- that this probability distribution of where the random walker is, converges. So I'm saying M times probability- the random walker, ah, probability distribution of the random walker at time t equals now, right? To probability where- probability distribution where- over where the random walker is going to be at the next time step. And let's just assume that this is actually equal to where they- where they used to be. And technically p of t is called a stationary distribution of a random walk, right? So random walker walks around so long that kind of time doesn't really matter, and this distribution of where the random walker is kind of stabilizes, uh, over the graph. And, ah, now, um, why is this interesting? Because in our original, uh, PageRank vector that we talked about, we said r equals M times r. And notice what I've wrote here is basically, p of t equals M times p of t. So now, because we have this correspondence, this means that r is a stationary distribution of the random walk. So basically, it means that this flow based equations can be interpreted based on the flow, or can also be interpreted as this intuition that there is a random walker walking around infinitely long over the graph, so that after some time, basically, it doesn't matter where the walker- random walker started, but it's really all about this distribution of where the random walker is, is going to converge to this stationary, uh, distribution. And in order for you to compute the stationary distribution, is the same problem as it was before to solve that system of equations or to solve this recursive equation of r equals m times r. Right? So, ah, this is the interesting, ah, correspondence that I wanted, ah, to talk about, right? And now, this is very interesting because this, ah, also links back to the lecture number 2, when we talked about eigenvector centrality, and we said, let adjacency matrix a be, um, an adjacency matrix of an undirected graph. As an example is here. And we said, ah, eigenvectors of- eigenvectors, ah, centrality of a, um, of a- of a graph with adjacency matrix A, simply satisfies this equation. We said A, ah, we said Lambda times c equals, ah, A times c, right? Where, ah, c is a vector and Lambda is of value. And solutions to these equations, um, where we have a scalar and a vector, um, are called, um, eigenvector eigenvalue, ah, equation. So right? So c in this case would be an eigenvector, and lambda would be an eigenvalue. So what this means is that the eigenvector centrality that we talked about is- is- has very similar structure to the, ah, to the PageRank equation, right? Here we are using this matrix M, that is stochastic matrix, but we have basically the same equation, ah, than what we had in the eigenvector centrality. The only difference is that we have this, ah, factor Lambda, and that we have this, um, adjacency matrix A, rather than the stochastic matrix M. So that's, ah, the connection now back to, ah, lecture 2, which is also, ah, super fascinating. So now, lets keep, uh, working. How are we going to solve this, right? So if I write this r equals M times r, and I include, ah, you know, implicitly basically a constant one here, then notice that this is also an eigenvalue eigenvector problem, right? Now, ah, r is an eigenvector that corresponds to the eigenvalue, ah, one of these stochastic, ah, matrix, ah, M. So this is- this is very interesting, right? Because this rank vector r, as I said, is the eigenvector of the stochastic adjacency matrix M, with the corresponding eigenvalue one, right? This is the Lambda constant from the previous, uh, slide, implicitly one. Um, and now, you know, what is the intuition of the- of the eigenvector? So the idea is that imagine you start from some vector, ah u, and you wanna compute this, uh, product of M times u, times M, times M, times M, times M. So basically you are just taking u and keep multiplying it with M over and over and over again. Um, so the question then becomes- right what this will do if u is the starting distribution of the random walker, then the question is, what is the long-term distribution of where the surfer is going to be in my network? And if you are multiplying with M, essentially you are making the surfer, uh, keeps surfing around, uh, longer and longer. Um, and just to- to tell you, right? Like this is also interesting because it connects to the notion of ka- Katz, uh, Katz, uh, centrality, when we talked about this, or Katz similarity in the- in the lecture 2, because there we also said that if you take an adjacency matrix and power it, it counts the number of paths between a pair of nodes, all right? So here in some sense we are also, ah, evolving and making this random walker walk longer and longer, uh, one step longer whenever we multiply with the matrix m. So what did we learn so far? We learned that PageRank is a limiting distribution, a stationary distribution of this random walk process, that it corresponds to the principal eigenvector of the stochastic, uh, adjacency matrix M. Principal eigenvector means eigenvector associated with the eigenvalue 1 of the- of the matrix. Um, and, ah, also notice if r is the limit of this, ah, ah, product of M, ah, you know, of number of multiplications of matrix M and u, then r satisfies this flow equation that we have, uh, that we have written up here that, you know, 1 times r equals M times r. So what this means is that our vector r is the principal eigenvector, uh, of matrix M and that it corresponds to eigenvalue 1. So, um, now, we brought three different things together. We brought things- we brought together the random walk equation, we've brought together this flow based equation that I've wrote here, and we brought together this concept of eigenvectors and eigenvalues from, uh, linear algebra. And it's super fascinating that this very different intuitions all converge, uh, in- in the same- in the same spot and they are just different interpretation of the same underlying mathematics. So now, what I wanna say is, how can we solve for this vector r? How do we determine vector r? And it turns out there is a very elegant, uh, and powerful, uh, algorithm to do this. It's called power iteration. Um, and it's- the approach is amazingly simple and amazingly new scalable. So, um, let me summarize, um, what we learned so far. We defined this notion of PageRank that measures importance of nodes in a graph using, uh, link structure of the graph. We talked today about directed graphs. Ah, PageRank models are random web surfer using this stochastic adjacency matrix M, and this random website for basically, uh, resides at the page, picks, uh, uh, out link at random and makes a transition. And then we said, how we- how could we compute the stationary distribution of this random walk process of this random surfer process on the graph? Um, we saw that PageRank solves this equation, ah, r equals M times r, uh, where r can be viewed both as the principal eigenvector of M, as well as the stationary distribution of this random walk process of the graph. And this is the fascinating thing, right? It's- it's just an eigenvector, but it has this very rich interpretation of the random walker, random surfer, ah, surfing around. So, um, this is the summary, ah, so far. 
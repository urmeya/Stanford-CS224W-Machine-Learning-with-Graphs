Uh, hi, everyone. My name's Jiaxuan, and I'm the head TA of this course. And really an amazing experience to work with you guys and I hope you learn a lot from the course. Uh, today I'm excited to present, uh, my recent, uh, research, Design Space of Graph Neural Networks. [NOISE] So in this lecture, uh, we cover some key questions for GNN design. Uh, specifically, we want to answer how to find a good GNN design for a specific GNN task. Uh, this problem is really important, but also challenging, uh, because domain experts want to use state-of-art GNN on their specific task. However, there are tons of possible GNN architectures. Uh, for example, in this lecture, we have covered GCN, GraphSAGE, GAT, GIN, etc. Uh, the issue here is that the best GNN design in one task can perform badly for another task. And redo the hyperparameter grid search for each new task is not feasible. And I'm sure you have some hands-on experience in your, uh, final project and, you know, tuning the hyperparameter of GNNs is notoriously hard. Uh, In this lecture, our, uh, key contribution in this work is that the first systematic study for a GNN design space and task space. And in addition, we also released the called platform GraphGym, uh, which is a powerful platform for exploring different GNN designs and tasks. [NOISE] Uh, to begin, we first introduce the terminology that we'll use in this, uh, lecture. Um, so a design means a concrete model instantiation. For example, a four-layer GraphSAGE is a specific design. Design dimensions characterize a design. For example, a design dimension could be the number of layers, L, which could take values among 4, uh, 2, 4, 6, 8. And design choice is the actual selective value in the design dimensions. For example, the number of layers, L equals 2. Design space, uh, consists of a Cartesian product of all the design dimensions relate to enumerate all the possible designs within the space. A task is the, a- a- a specific type of interest, which could be, for example, uh, node classification on Cora data set, graph classification on ENZYMES data set. And the task space consists of all the tasks that we care about. And in this paper we introduced the notion of GNN design space, and actually, we have, uh, go into much deta- detail in the previous lecture, so here we'll just do a quick recap. Uh, so in the d- GNN design space, we consider first, uh, the intra-layer, uh, design. And we have introduced that a GNN layer can be understood as, uh, two parts, the transformation function, and the aggregation function. And, uh, here we propose a general instantiation under this perspective. So concretely, it contains four different dimensions. Uh, so we have, uh, whether to add BatchNorm, uh, whether to add dropout, uh, the exact selection of the acti- activation function, and the selection of the aggregation function. Next, uh, we are going to design the inter-layer connectivity. And the lecture, we have also introduced, uh, different ways of organizing GNN layers. And in- in this, uh, in this work, uh, we consider adding some, uh, pre-process layers and post-process layer, uh, in addition to the GNN layers, uh, which can, uh, jointly, uh, form a complete graph neural network. So the intuition of adding pre-process layer is that it could pretty important, uh, when expressing node feature encoders are needed. So for example, when our nodes are extracted from images or text, we'll be consider using some, uh, expressive, say, convolutional neural networks or transformers to encode these node features. And then we may also add some post-process layer after applying graph neural network computation, which are important when we are going to say a reason or transformation over node embeddings. And some example, uh, are, say, uh, doing gra- graph classification, or some applications around are not expressed. And the core, uh, core of the graph neural network are, uh, GNN layers. And there we consider different strategies to add skip connections. And we found that this really helps improve, uh, deep GNNs performance. Uh, then finally, we'll cover different learning configurations for GNNs. And actually, this is often neglected, uh, in current literature, uh, but in practice, we found that these learning configurations have high impact on- on a GNN's performance. So specifically, we'll consider, uh, the batch size, the learning rate, the optimizer for gradient update, and- and how many epochs do we train our models. So in summary, uh, we have proposed a general GNN design space that consist of, uh intra-layer design, inter-layer design, and learning configuration. And If you com, uh, consider all the possible combinations, this really lead to a huge space, so it contains, uh, 315,000 possible GNN designs. And, um, to clarify, our purpose here, uh, is that we don't want to and what cannot cover all the possible GNN designs because for example, we can even add more, uh, design dimension, say rather to add attention, how many attention it has to use, and etc. So this space is really a very- very huge. So what we're trying to do is to propose a mindset transition. So we want to demonstrate that studying a design space is more effective than studying individual GNN designs, such as, uh, uh, considering- only considering GraphSAGE, GAT, those individual designs. So after introducing the GNN design space, we'll then introduce the GNN task space. And we'll categorize GNN task, uh, into different, uh, categories. Um, so the common practice is to categorize GNN task into node classification, edge, uh, prediction, and graph level prediction tasks. And we have covered how do we do this in previous lectures. Although this, uh, this technology is reasonable, it is not precise. So for example, if we consider a node prediction and we could do say, predict node clustering coefficient. Another task could be, uh, we will predict a node subject area in a citation network. So although these tasks are all node classification, uh, they are completely, uh, completely different in terms of their semantic meaning. However, creating a precise taxono- ta- taxonomy of GNN tasks is very hard because first, uh, this is really subjective how you want to categorize different task, and second there is normal GNN task can always merge and you cannot, uh, uh, predict the future of the- the unknown, uh, GNN tasks. So our innovation here is to propose a quantitative task similarity metric. And our purpose here is to understand GNN task and, uh, uh, uh result we can transfer the best GNN models across different tasks. And- so here's a concrete, uh, our innovation, uh, where we propose, uh, quantitative task similarity metric. So to do this, uh, we will first select a notion called anchor models. So here's a concrete example. Suppose we want to, uh, measure the similarity between tasks A, B, and C, and then the anchor models are M_1 through M_5. The second step is that we'll characterize a task by ranking the performance, uh, of anchor models. So here I say task A have the ranking of say, 1, 2, 3, 4, 5. Task B have the ranking, uh, which is different, which is a 1, 3, 2, 4, 5. And task C again has another ranking among the anchor models in terms of their performance. And our argue-, uh, the key insight here is that, uh, the task with simi- similarity rankings, uh, similar rankings are considered as similar. So for example, um, here we can see the similarity between the rankings of, uh, task A and task B is pretty high. And the similarity between task A and C is pretty low. And this way, we can give a quantitative measure between different- different tasks. Uh, the next question is that how do we select the anchor models? So more concretely, we will do, uh, three steps to select the anchor models. Uh, first, we'll pick a small dataset that it easy to work on. And second, we'll randomly sample N models from our design space and we'll run them on our dataset. For example, we can sample 100 models, uh, from our entire design space. The third step is that we'll sort these models based on their performance and then we'll evenly select M models as the anchor models whose performance range from the worst to the best. So for example, we have picked, uh, random 100 models, we will sort them by their performance and then say we'll pick the top model as the first anchor set- anchor model and then set the 10th percentile, uh, model as the second anchor model. And then up to the worst model among 100 models. And our goal here, is really to come up with a wide spectrum of models. And our integration is that a bad model in one task could actually be great for another task. And we have verified this, uh, with our experiments results. Contrarily, we co- can collect, uh, 32 tasks, uh, which are, uh, nodes and graph classification tasks. And we have six real-world node classification tasks, uh, 12 synthetic node classification tasks, uh, including a predicting node clustering coefficient, and node PageRank. And then we also have six real-world graph classification tasks and eight synthetic graph classification tasks, uh, including, uh, predicting graph average path lengths. The final topic we will cover is that having defined, uh, our GNN design space and task space, how do we evaluate the GNN designs? For example, we want to answer the question like, uh, is graph- is BatchNorm generally useful for GNNs? Um, here the common practice is just to pick one model, for example, a five layer, 64-dimensional GCN, and compare two models, uh, with or without BatchNorm. Uh, our approach here is that, uh, is more rigorous. Uh, that is, uh, we know that we have defined 300,000 models and 32 tasks, and this data leads to, uh, uh, about 10 million model-task combinations. And what we are gonna do is to first sample from the 10 million possible model-task combinations and we'll rank the models with BatchNorm equals true or false. The next question is that how do we make it scalable and convincing? [NOISE] And more concretely, our proposed approach called, uh, controlled random search. So the first step, is to sample random model-task configurations from the entire design space. And we perturb the BatchNorm equals true or false. So for example, uh, we have different, uh, uh, models with different, uh, GNN designs, such as, uh, ReLu activation, PReLu activation, and different number of layers, different la- layer connectivity and they're applied to different GNN tasks. What we're going to do is that, we will fix the all- the rest of design and task dimensions, but only perturb its BatchNorm dimensions into true or false. And in the meantime, we will control the computational budget for all the models so that this comparison is really rigorous. And then we will rank BatchNorm equals true or false by their performance. Here, lower ranking is better. So for example, we can see, okay, uh, in one application BatchNorm equals true have validation accuracy of 0.75, uh, but false with only, uh, 0.54, which means that BatchNorm equals true is better. So it has a lower ranking of 1. And sometimes there could be a tie because the two, uh, choices are pretty close in terms of their performance. The final step is to plot average or distribution of the ranking of the BatchNorm equals true or false. So for example, here we see the average ranking of the BatchNorm is true is lower, which means that, uh, in general BatchNorm is equals true often provokes better. So to summarize, um, here, we really propose an approach to convincingly evaluate any new design dimensions. And for example, we can use the same strategy to evaluate a new GNN layer that we propose. So here are the, uh, key results. First, we will demonstrate a general guideline for GNN designs. So, uh, we showed that certain design choices exhibit clear advantages. So we'll first look at those intralayer designs. Um, the first, uh, conclusion that, uh, BatchNorm equals true, are generally better. And our explanation is that GNNs are hard to optimize, therefore, batch normalization can really help, um, the gradient update. And then we found that dropout equals 0, which means no dropout is often better. Because we found that GNNs actually experience under fitting more often than over fitting. So BatchNorm-, uh, sorry. So our drop out doesn't-, uh, it doesn't help too much. And then we found that, uh, PRelu activation actually really stands out. And this is our new findings in this paper and, ah, versus the common practice of only using the ReLu activation. And finally, we found that sum aggregation is always better because we have explained in the lecture, that sum is the most expressive agg- aggregator that we could have. And then we'll go on to look at the inter-layer designs. Um, first, we found that the optimal number of layers is really hard to decide. You can see their rankings are pretty, uh, even. And we argue that this is really highly dependent on the task that we have. And also we find that, uh, sk- skip connections can really enable hierarchical node representation therefore is much desired. And finally, we will look at the learning configurations. We found that the optimal batch size and learning rate is also hard to decide. And therefore it's highly dependent on the task. And we found that the Adam optimizer and training more epochs are generally better. The second key result is the understanding of GNN tasks. First, we found that GNN designs in different tasks vary significantly. So this motivates that studying the task space is really crucial. So if we look at design, um, tradeoff in different tasks, like BZR proteins and smallworld, sometimes max aggregation is better, sometimes mean is better and sometimes sum is better. And similarly for a number of layers, sometimes a eight-layer is better, sometimes two-layer is better, uh, and etc. So this, uh, argues that our GNN task space is pretty helpful. So what we're going to do is to compute pairwise similarities between all GNN tasks. So, uh, recall how we compute GNN task. We will measure the similarity based on anchor model performance. And then, uh, the argument is that our task similarity computation is really cheap. And we found that using 12-anchor models is already a good approximation. And our key result is that the proposed GNN task space is pretty informative. So we identify two group of GNN task. Group A relies on feature information. Uh, and these are some node cla- or graph classification task where input graphs have high dimensional features. And Group B, our task relies on structural information where nods have fewer of, uh, features but predictions are highly dependent on the graph structure. And then we'll do PCA and do dimension, uh, reduction, uh, to visualize this in 2D space. And indeed we verified that similar tasks can have similar best architecture designs. And finally, we will go on to transfer, uh, our approach to novel task. So here we conduct a case study that is to generalize the best models to unseen OGB task and to, uh, that the observation that the OGB, uh, molecule, uh, prediction task is unique from other tasks. So it's 20 times larger, highly imbalanced, and requires out-of-distribution generalization. So this is really a novel task compared to the tasks that we have seen. And here's a concrete step to apply our approach to a novel task. So the first step is to measure 12, uh, anchor model performance on a new task. And then we're going to compute similarity between the new task and the existing task. Finally, we'll recommend the best design from the existing task with the highest similarity. So here are the concrete results. Um, so we'll pick two models, uh, using our task similarity metric. So task A is highly similar to OGB and task B are not similar to OGB. And our finding is that transferring the best model from task A, really achieves SOTA performance on OGB. However, uh, transfer the best model from task B performs badly on OGB. So this really, uh, illustrates that the proposed task metric is really helpful. And our task space can really guide the best model transfer to node tasks. To summarize- to summary, uh, in this paper, we proposed the first systematic investigation of general guidelines for GNN design. And the understandings of GNN tasks as well as transferring best GNN designs across tasks. In addition, we also released GraphGym as an easy to use code platform for GNNs. Uh, thank you for your attention. 
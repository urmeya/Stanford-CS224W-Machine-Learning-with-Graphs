So we are going to talk about random walk with restarts and personalized PageRank. And this will be extensions of our initial, uh, PageRank idea that we have just discussed. So let me give you an example how this would be very useful. So we are going to talk about a problem of recommendations. Imagine you- you have a set of users, customers, uh, and a set of items, uh, perhaps products, movies. And then you create interactions between users and items by basically saying a given user perhaps purchased a given item, or a given user watched a given movie, right? So this is now our bipartite graph representation, uh, here of this user to item relation. And then what we want to do is somehow measure similarity or proximity on graphs. Why is this useful? This is useful because if you are a online store or if you are Netflix, you want to ask yourself, what items should I recommend to a user who, you know, purchased an item Q. And the idea would be that if you know two items, P and Q, are purchased by a lot of, uh, similar users, a lot of other users have, let say, bought or enjoyed the same- the same item, the same movie, then- then whenever a user is looking at item Q, we should also recommend, uh, item P. So now, how are we going to quantify this notion of proximity or relatedness of different items in this graph. So the question is, for example, if I have this graph as I show here, and I have items, you know, A and A prime and B and B prime. The question is which two are more related? So you could do- one thing to do would be to say, you know, let's measure shortest path. So A has a shorter path than B to B-prime, so, you know, A and A prime are more related. However, the issue becomes, is that- that you could then say, oh, but if I have another example, let's say this one where I have C and C prime. And now C and C prime have to users that bo- that both of let say purchased these two items, then C and C prime intuitively are more related, are at closer proximity than A and A prime, right? So now, the question is, how would I develop a metric that would allow me to kind of say, hi, it's kind of the shortest path, but it's also about how many different, uh, neighbors you have in common and how many different paths allow you to go from one, uh, to another. And they- the idea here is that, er, PageRank is going to solve this because in this third example, you know, if you would just say, let's count common neighbors, then let say, uh, C and C prime are related as D and D prime. And again, this is- um, this is perhaps intuitively not what we want because, er, you know item D, this user has enjoyed a lot of different items as well. This other user has enjoyed a lot of different items there. So this relationship is less strong than the relationship here because here, it's really two items, two items that- that's all- that's all there is. So, you know, how could we capture this mathematically algorithmically to be able to run it on networks? And, uh, this is where the notion of extension of PageRank happens. Um, so PageRank tells me the importance of a node on the graph and ranks nodes by importance. And it has this notion of a teleport where we discuss that- that, um, a random surfer teleports uniformly over any node in the graph. So now, we will have- we will first define a notion of what is called personalized PageRank, where basically, the only difference with the original PageRank is that whenever we teleport or whenever the random walker teleports, it doesn't teleport anywhere in the graph, but it only teleports, jumps back to a subset of nodes S. Okay? So basically, we say, you know, there is a set of nodes S that are interesting to the user. So whenever the random walker teleports, it teleports back to that subset S and not to, uh, every node in the graph. And then in terms of, uh, you know, er, proximity in graphs, you can now take this notion of a teleport set S and you can shrink it even further and say, what if S is a single node? So it means that the random walker can walk, but whenever it decides to teleport, it always jumps back to the starting point S. And this is what is called a random walk with restart, where basically, you always teleport back to the starting node S. So essentially, PageRank, personalized PageRank, and random walk with restarts are the same algorithm with one important difference, that in PageRank, teleport set S is all of the nodes of the network, all having equal probability. In personalized PageRank, the teleport set S is a subset of nodes, so you only can jump to the subset. And in a random walker with restart, the teleportation set S is a simple node- is simply one node and that's the starting node, our, you know, query node item, uh, Q, uh, from the previous slide. So let me now talk more about random walks with restarts. So the idea here is that every node has some importance, and the importance gets evenly split among all edges, uh, and pushed to the neighbors. And this is essentially the same as what we were discussing in, uh, page- in the original PageRank formulation. So in our case, we are going to say let's have a set of query nodes. Um, uh, this is basically the set S. And let's now physically simulate the random walk over this graph, right? We will make a step at random neighbor, um, and record the visit to that neighbor. So we are going to increase the visit count of that neighbor. And with some probability alpha, we are going to restart the walk, which basically means we are going to jump back to any of the query nodes and restart the walk. And then the nodes with the highest query- highest visit count will have the highest proximity to the- uh, to the query- to the nodes in the query nodes, uh, set. So this is essentially the idea. So let me now show you graphically, right? So we have this bipartite graph. Imagine my query nodes set Q is simply one node here. Then we are going to simulate, really, like a random walk that basically says, I'm at Q. I pick one of its, uh, links at random, and I move to the user. Now, I am at the user. I pick one of the links at random, uh, move to, uh- to the- to the other side and I increase the visit count, uh, one here. And now I get to decide do I restart, meaning go back to Q, or do I continue walking by picking one of- one link, um, to go to the user, pick another link to go back, and increase the visit count? And again, ask myself do I want to restart or do want to continue walking? So the pseudocode is written here and it's really what I just say. It's basically, you know, pick a random neighbor for- for- for, uh, start at a- at a query, pick a random user, uh, pick a random item, increase the revisit count of the item, uh, pick a biased coin. If the coin says, uh, let restart, you'll simply, uh, jump back to the query nodes. You can jump, uh, uniformly at random to any of them, or if they have different weights, you can sample them, uh, by weight. And that is- that is this notion of a random walk, uh, with, uh, restart. And if you do this, then you will have the query item and then you will also get this visit counts and- and the idea is that items that are more, uh, related, that are closer in the graphs, will have higher visit counts because it means that the random walker will visit them more often, which means you have more common neighbor, more paths lead from one to the other, these paths are short so that, uh, the random walker does not decide to restart, uh, and so on and so forth. And this allows us to measure proximity in graphs very efficiently. And here, we are measuring it by actually, uh, un- kind of simulating this random walk physically. But you could also compute this using the power iteration where you would represent this bipartite graph with a matrix, uh, M, you would then start with, uh, um, rank vector, um, uh, to be- to have a given value. You would then, uh, transfer them to the stochastic adjacency matrix with teleportation, uh, matrix, and then round power iteration on top of it. And it would, um, converge to the same- uh, to the same set of, uh, uh, node importance as we- as we show here by basically running this quick, uh, simulation. Um, so what are the benefits of this approach? Um, this is a good solution because it measures similarity by considering a lot of different, um, things that are important, right? It considers how many connections or how many paths are between a pair of nodes. Um, what is the strength of those connections? Are these connections direct or are they indirect? They also- it also considers the- the degree of the nodes on the path. Because, uh, the more edges it has, the more- the more likely we- for the random walker, is to kind of walk away and don't go to the node. Let's say that- that we are interested in. So in all these cases, um, this is a very- uh, has a lot of properties that we want. It's very simple to implement, it's very scalable, and, uh, works, uh, really well. So let me summarize this part of the lecture. So basically, here, we talked about extensions of PageRank. We talked about classical PageRank where the random walker teleports to any node. So, you know, if I have a graph with 10 nodes, then its teleport set S. You can think of it is- it includes all the nodes and each node has, uh, equal probability of the random walker landing there. This is called PageRank. Then the personalized PageRank, sometimes also called topic-specific PageRank, is basically, the only difference is that now the teleport vector only has a couple of- of non-zero elements. And this now means that whenever a random walker decides to jump, you know, 50 percent of the times, it will jump to this node, 10 percent to this node, 20 percent to this one and that one. So that's, uh, what is called personalized PageRank. And then random walk with restarts is again PageRank. But here, the teleportation vector is a single node. So whenever the-the surfer decides to teleport it always teleports to the- to one, uh, single node. But mathematically, all these formulations are the same, the same power iteration can solve them. Uh, we can also solve, for example, especially the random walk with restarts by actually simulating the random walk, which in some cases, might be- might be, um, faster, but it is approximate. Um, and the same algorithm works, only thing is how do we define the set S, the teleportation, uh, set. So to summarize, uh, a graph can naturally be represented as a matrix. We then define the random walk process over, er, the graph. We have this notion of a random surfer moving across links, uh, with- er, together with having a-a way to teleport, uh, out of every node. This defined- allowed us to define this stochastic adjacency matrix M that essentially tells us with what probability the random surfer is going to navigate to each edge. And then we define the notion of PageRank, which is a limiting distribution of a- of the surfer location. Um, and this limiting distribution of the surfer location represents node importance. And then another beautiful thing happened is that we showed that this limiting distributions- distribution can be computed or corresponds to the leading eigenvector of the transform adjacency matrix M. So it basically means that by computing the eigenvector of M, we are computing the limiting distribution of this, uh, random surfer, and we are also computing this solution to the system of equations, of, uh, flow equations where the importance of a node is, you know, some of the importances of the other nodes that point to it. So all these three different, uh, intuitions. So the linear algebra, eigenvector eigenvalue, the Random Walk intuition, and these links as votes intuition are of the same thing. They all boil down to the same optimization problem, to the same algorithm, to the same formulation that is solved with this iterative approach called power iteration. 
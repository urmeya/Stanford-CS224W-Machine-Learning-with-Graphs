So now let's start talking about these null-models, generative models for graphs that can serve as reference points. So the first- ah, we are going to talk about is the Erdős-Rényi ah, random graph model. Basically mention- named after two famous Hungarian mathematicians who developed a lot of mathematical theory around the random graph generation, er, processes. And random graph model- Erdős-Rényi random graph model is kind of the simplest way to generate the graph, right? We have two variants of it. One called G_np and the other one called G_nm. G_np says it is an undirected graph on n nodes where every edge appear iid with probability p. So basically for every pair of nodes, I get to flip this biased coin p, and I get to observe whether the edge is created. And then a different version of this model is called G_nm, where it's an undirected graph with n nodes and m edges picked uniformly at random. What's the difference between the two models? They both have the same number of nodes, um, they both have the same number of edges in expectation, ah, and they both have edges placed at random between the nodes. So the point being because in G_np the edge creation is stochastic, you know, in expectation I will have n times- n times p number of edges. But because of some variance of this random process, I may have a bit less or I may have a bit more while in G_nm, I always have the same number of edges. So now with this, ah, simple generative model, let's look at what kind of networks does this model produce, right? So basically, the point is that these models are stochastic. So it means that these random graph models do not uniquely determine the graph, right? The graph is a result of a random process. So we can have many different realizations for the same n and p value. So here for example, I show you different random graphs, um, generated by, you know, n equals 10 and p equals 1 over 6. So what we wanna do next we're gonna say, okay, if I take this um, G_np model and I generate random graphs from it, you know, what would be the degree distribution? What would be clustering coefficient? What would be the average path length? What would be the connectivity? Do I get the giant connected component? And what is beautiful about G_np, the Erdős-Rényi random graph model, it's that- it is that it is kind of so simple that you can mathematically analyze its properties, right? You can mathematically derive these quantities using algebra alone, you don't have to do simulations and measurements. So it's kind of the simplest model, but it's very- leads to very rich, er, mathematical literature, um, and very rich mathematical analysis. So there are kind of- and it's an entire sub field that studies random graphs arising from this model. So, er, first, you know, what would be the degree distribution of the G_np model? The degree distribution of G_np model would- is binomial. So if you diff- you say y here is like an argument, you know, let P of k denote the fraction of nodes with a given degree k. Then if you see how many nodes do I- would I have in the given degree? You say, ah-ha, out of n minus 1 possible members of a given node, I get to sample or pick k of them. So n minus 1 choose k is the number of times I can select k nodes of out of n minus 1. And then, you know, because I'm saying my deg- my node has to have degree k, it has to have k edges, er, be created. So the likelihood of that is p raised to the power of k, and then the other coin flips have to result in not-edges so that would mean it's 1 minus n minus 1 minus k, right? So that's the probability of, er, missing the rest of the edges. And if you look at this formula, this is exactly the formula for a binomial distribution. If you look at binomial distribution, it has this kind of bell-like shape. So essentially, it's like a discrete analog of our Gaussian distribution. It has, you know, average- average or mean of the distribution is simply p times, er, n minus 1. So average degree in a G_np network will be, er, p times n minus 1. And if n is massive, then this is essentially, er, n times p. So this is about binomi- er, about the degree distribution. And again, notice that, you know, in MSN network, we didn't see like this bell-shaped like curve, we saw something very skewed. So clearly, er, G_np does not generate graphs with the degree distribution that is similar to the MSN network. How about clustering coefficient, right? Um, remember the definition of clustering coefficient; it is twice the number of edges between the neighbors of a given node divided by k times k minus 1, where k is the degree of that node. And remember also that edges in G_np appear iid with probability p. So the expected number of edges between, um, um, these neighbors, e sub i of a given node i is simply, er, the total number of possible edges times the probability that, you know, each one of them comes- comes up, er, and actually materializes. So the expected number of edges, so the expected e sub i is simply k times k minus 1 divided by 2, where k is the degree of node i times the probability that actually this- this particular pair of nodes, this particular edge appears, and that is k choose 2 type number of such pairs, er, as written here. So now if you put these two together, er, what you get is you get the- the- the, um, ah, clustering coefficient is er, er, er relates to, er, to the graph as average degree divided by the number of nodes. So what does this mean, is that the average- what, we can conclude is that the average clustering coefficient of a random graph will be small, right? So basically, if you generate bigger and bigger graphs and keep the average degree constant, um, then, er, the clustering will decrease with the size of the graph, right? And we could even take the values from MSN network and plug them here, right? So average degree was 14, number of nodes was 180 million, right? So this means that, um, our, er, clustering coefficient should be 10 to the minus 6, right? It's 14 divided by 180 million. Um, so, um, that's kind of what is the clustering coefficient of, a Erdős-Rényi random graph. So this means that clearly our MSN network is not a random graph. And then the third property I wanna comment on is connected components, um, and this is very interesting because in, er, um, in G_np you can actually analyze how is the graph going to be connected as a function of the edge probability p, right? If the p is 0, then the graph will be empty. No, there would be no edges. It could be just a set of isolated nodes. If the p equals 1, then every pair of nodes will be connected and that's a complete graph. What the result is that the- the connected component is going to appear when p is, um, is of the order 1 over n. Which basically means as soon as the average degree gets above 1, the giant component will start to appear. And if the average degree is below 1, there won't be this large giant component. So we get these kind of phase transition behavior, uh, in G_np. And then of course, you can also analyze what is happening as the, er, edge probability is increasing. And for example, you can find out if you know, er, edge probabilities of the order 2 times log n divided by n minus 1. Then at that point in time, your graph will be- um, um, er, won't have any isolated nodes. It may still have a couple of disconnected components, but every- every node in- will have some edges. So the point being, is that in G_np, the largest connected component occurs exactly when the average degree is greater than 1. And here's the result of a simulation where we change the average degree. And here we are asking what is the fraction of nodes in the largest connected component? And you see how largest connected component is kind of small, small, small. But as soon as the average degree hits value 1, which means that, er, p is greater than 1 minus 1 over n minus 1, this giant component starts to emerge, and you know what? When the average degree is 2, it's already 80% of the nodes in- is in the big, er, largest component. So the point being, G_np is connected as soon as or has a large component, as soon as, er, average degree is greater than 1. So, er, what have we learned so far? We learned that G_np has a binomial degree distribution. We learned that clustering coefficient of G_np is average degree divided by n. We learned that large connected component will exist if average degree is greater than 1. So now, let's talk about, er, average, uh, path length, so the diameter. So to be able to talk about diameter, we need to define this notion of expansion. And the way expansion is defined mathematically is to say for a given graph, uh, it will have expansion Alpha. If for every subset of, uh, vertices S, the number of edges leaving S is, um, greater than Alpha times, uh, size of the S, right? Uh, and here, uh, I have the- this minimum here, just to account for the fact that S can be more than half of the size of the graph, but if I assume that S is, uh, less than, uh, half of the nodes of the graph, so it's, er, the smaller part of the graph, then the- the- the point is that if I pick any set of S nodes, there would be at least Alpha times S edges sticking out of that set. So, uh, equivalently, Alpha is the minimum over all possible subsets of nodes, number of edges leaving that subset dividing by the size of the set, and the point is that this is min- this is defined as the minimum overall, uh, subsets. Um, so the way to think of this is to say here is a set S, here's- here are the rest of the nodes that are not in S, and we a- we ask how many edges cross between S, uh, and the rest of the graph, and this ratio between edges crossing and the size of S is called, uh, expansion. So, uh, you know, and the way you think of expansion, you can think of it as a measure of robustness, right? Uh, for example, Uh, do I do mean- it would mean that if you want to disconnect l nodes, uh, from the graph, well that means you need to- to cut alpha times l, uh, edges, right? Because if you say I want to disconnect a set S from the graph, then I need to- I need to cut alpha times, uh, uh, size of that set number of edges so that that- piece that set of nodes is disconnected from the rest of the graph. Um, that's another way to think of a- a- a- notion of expansion because it tells me how robust is the network. So let me now give you some examples of networks with high expansion, low expansion, and show you some- you know, some- something in between. So this would be a network of low expansion because I had this one bridge edge, right? So basically, because Alpha is defined as a minimum over all, uh, subsets of nodes, basically if I pick this as a set of S, then, uh, this is the edge that sticks out of the set. So this is an example of a graph with low expansion. Uh, this is an example of a graph with high expansion, right? Kind of everything connected to everything, uh, out of every set S, there is a lot of edges, uh, sticking out. Um, and then, you know, something that's kind of in-between at these types of networks with community structure where you have the subsets with high expansion and then perhaps the expansion between the clusters, uh, is a bit, uh, lower. So, um, you know, so real networks will be kind of somewhere in-between these two extremes. Um, that is a fact, um, or one can mathematically show that for a graph on n nodes with expansion alpha for, um, between all pairs of nodes, then it means that, um, for any pair of nodes in the graph, there is a path length, the shortest path length order, uh, log n divided by Alpha. So basically it means, um, the bigger the graph, the longer the the path, right? We have log n, but also the bigger the expansion, bigger Alpha, the shorter the path length will be, and, uh, now given this fact what you can then, uh, what is then also been shown is that for, uh, graphs where, um, log n is greater than, uh, n times p is, er, greater than some constant, the diameter of, uh, G_np will be log n divided by log, uh, np, right? So basically the way to think of this, if, uh, the average degree, right- sorry, if, uh, average degree is greater than, um, uh, log n, uh, then the, um, then the, um, diameter, uh, will- diameter of the graph will be, uh, order log n, and this means that random graphs have good expansion, so it takes a logarithmic number of steps of- from- of a breadth-first search from a given node to basically visit all nodes of the graph, right? So basically the diameter is logarithmic in the number of nodes. Um, and this is interesting because it means the diameter is exponentially smaller than the number of nodes, right? Because n- log n is kind of a, er, is exponentially less. Log n is- is exponentially less, um, than m. So it means that shortest paths are in G_np, are very short. They are only logarithmic, uh, in the size of the graph. So, um, this is the notion, uh, of expansion, and it turns out that G_np, random graphs have high expansion and that's why, uh, the diameter is, uh, order log n. Um, again, there is, um, a super cool, uh, theory and ways to prove this. Uh, we can link to some papers, uh, where you could kind of work out and- and learn how to make, uh, these proofs, but doing them is kind of outside the scope, uh, of this class. So to give you an example, here is the, uh, number of nodes in a- in a, uh, G_np graph here. Um, uh, and this is the length of the average, uh, shortest path, and here we keep the average degree constant. So all these graphs have, um, ah, different number of nodes, but average degree in all of them is constant and you see how basically the diameter grows very, very slowly as the graph sizes, uh, increases, and you see this- this basically is like a logarithmic, uh, shape. So now that we have seen, uh, what the G_np is and we've kind of worked out these, uh, properties of it, let's compare it to the MSN network. So, uh, let's compare. For example, in the MSN network, the degree distribution is heavily skewed, right? If I plot a histogram, it is just kind of axis aligned. A G_np degree distribution would- has this nice bell shape. So clearly the two don't match. The average shortest path length in MSN network is 6.6. In, uh, in G_np, we said the average shortest path length will be, uh, order log n, right? Log, you know, let's say log base 10 of 180 million is, you know, 6, 7, 8. So it's close, right? So we say, yes, the MSN network has short paths and the G_np also has, uh, short paths. So that sounds great. How about clustering coefficient? Clustering coefficient of MSN network was 0.11, and we've worked out that, um, that the, um, clust- that clustering of a G_np graph would be average degree divided by, uh, n, where n is the number of nodes. If we- if we enter the average degree of MSN which is 14 and divide by 180 million, we get, uh, we get average clustering coefficient to be, um, 10, uh, 10 to the minus 8. So clearly we are nowhere close, right? We are, you know, uh, six, er, orders of magnitude apart. We are a million- you know, we have missed the mark by a million times. I mean, um, you coul- you cannot miss smaller, right? It's like as- as different as possible, so clearly totally different, and then in terms of largest connected component, we saw that in MSN, largest connected component has 99.9% of the nodes, um, in- in Erdos-Renyi, the largest connected component starts existing when average degree is greater than 1, MSN, average degree is around 14. So yes, uh, you know that- that is something that is matched. So basically what we can conclude is MSN, um, and the G_np, uh, can- G_np can match MSN in terms of shortest path lengths, in terms of connectivity, it totally misses clustering, it totally misses degree distribution. So why- why this was interesting is because is now we know that MSN network is not a random network. It's fundamentally different from the, uh, random graph. So let's now, um, see if we can extend or fix this G_np model to match more of the properties of our MSN network. Um, and one sentence I will add here it is, even though we know we've worked with a single instance of a network, this MSN network, it turns out that these properties are quite shared across huge class of networks in a sense that you have skewed degree distribution, that you have short- shortest path lengths, that you have clustering coefficients that are quite high and that the graphs are connected. So these properties that the MSN network has are kind of universal across, uh, real-world graphs. Okay, so, uh, to summarize, what have we learned so far? You know, if you ask, are real networks like random graphs? Uh, the answer is giant component, yes, they- they- it behaves like in a random graph. Every shortest path length also behaves like in a random graph. Clustering coefficient, totally different, degree distribution, totally different, right? So the problem with the random graph model is that the degree distribution differs significantly from real network, that, um, and also that in real networks, giant component, uh, does not emerge through some kind of phase transition, um, and the other biggest problem is that, um, random graphs have no local structure, right? Like there is no friend of a friend is- is my friend type effect, so there is no triangles. The- the clustering coefficient is way too low. So the question is, are real- real world networks random? Do they look like G_np? The answer is no, no way. However, this model is still very useful because it serves as a first basic reference point, whenever you wanna generate, uh, a graph and, uh, compare it to something, you- you would use a G_np model. 
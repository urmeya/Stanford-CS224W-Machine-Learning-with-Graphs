So let's talk next about knowledge graph completion tasks and various methods to accomplish, uh, knowledge graph completion. In particular, we'll talk about methods that are, uh, that right now will have cryptic names, but it will become, uh, clear: TransE, TransR, DistMul, and, uh, ComplEx. So I'll, I'll introduce what these methods are, and we'll be in particular interested, what kind of relationships can these methods capture and predict? So let's think about, uh, knowledge graph completion task. Where basically we are given an enormous knowledge graph, and the question is, can we impute- can we predict missing information? Because the knowledge graph is incomplete. And the way the knowledge graph completion task works is that we are given the head node, we are given the relation, uh, type and we'd like to predict the missing tail. So notice that this is slightly different from classical link prediction task where nothing is given, we just say, predict what links are missing in the entire graph. Here we would actually say, aha, we are interested in this particular node, we are interested in this particular, uh, relation type and we wanna ask, you know, where is, uh, this particular author? Which other, let's say, genres is this author connected to? So for example, uh, JK Rowling, right, the, uh, Harry Potter author is connected to, let's say, uh, genre science fiction. So that would basically be, uh, something we would wanna predict. So what we are given is the head node, we are given the relation type and we wanna predict the tail. Um, the way we are going to do this is we are going to do through node embeddings, right? So every node, every entity in the knowledge graph will have an embedding. And we are going just to use shallow embeddings. So basically it means we will wanna learn an embedding vector for every, uh, entity type in the, uh, in the knowledge graph so that we can then do prediction well. So the point is, we won't be using GNNs, um, we'll just assume we are doing shallow encodings. But you could also, uh, use GNNs if you would like. So, uh, knowledge graph representation will be such but knowledge graph is represented as a set of triples: head, relation, tail, right? So head is the source, relation is the type of a relation, and tail is the endpoint of the relation. So head could be, you know, I know Barack Obama and, uh, relation would be nationality and the tail would be United States. So the key idea how we wanna do this is that we wanna model entities and the relations as embedding points and vectors in the embedding space. And we wanna associate entities and relations with shallow embeddings, as I mentioned. So we are not using GNNs just for simplicity to keep our life simple. Um, and then the idea is that given a true triple: head, relation, tail, the goal is that the embedding of head, relation should be close to the embedding of the tail. Um, so I know this is a bit abstract right now. So let me give you, um, explanations how can we embed head, relation and how do we define this closeness, uh, that the embedding of head, relations should be close, uh, to the tail. And really, all different methods we'll talk about differ how these closeness and how this, uh, embedding is produced. The simplest idea, and the kind of the most natural one, is called TransE. And Trans- TransE basically has this intuition of translation. Where basically the idea is that for our triple: head, relation, tail, we wanna learn the embedding of the head, we wanna learn the embedding of the tail, and we wanna learn also the embedding vector r. Um, and we, and the idea would be that we want it- we want to find these embeddings in such a way that head plus the, uh, um, of- plus vector r is- gives me the, uh, the point, uh, where the t is embedded, right? So for example, if, uh, um, um, Barack Obama, uh, nationality United States, then I want, I want them to be embedded in such a way that if I start with the head, we start with Barack Obama, I move according to this relation vector r, I would end at the point where United States is embedded. And of course, I don't want this to only work for, uh, let's say Barack Obama, I'd like to w- this to work to be true for any person and their, uh, their country of birth, right? So for example, if in this graph, I don't know, um, it would be, uh, myself here as well, maybe on a point here. Then if I would move according to this vector r here, I'm from Slovenia, so my nationality should be Slovenian. So the, the country of Slovenia should be mapped somewhere here, right? Basically I would apply the same translation and, uh, hopefully the- the- the- the country that is embedded close to that point is the country of my, uh, nationality, all right? So here's, uh, here's, uh, here's the idea. So the scoring function, uh, that would measure proximi- that would measure proximity between the head and tail, would simply be head plus this vector r minus, minus the tail, right? It basically says what is the distance between h plus r, uh, and, uh, the point, uh, t? And here I could use, let's say Euclidean distance if I would like. So this is what is Trans, uh, TransE. Basically it wants to learn these translations in the embedding space so that you can get from head, according to a translation that's- that is the relation specific, to the, uh, to the tail, uh, of that, uh, relation. Um, s- I will- I- I won't go into details basically of the learning algorithm. But essentially the idea is that you wanna have entities and relations that are first initialized, uh, uniformly, right? You want an embedding for an entity, you want to learn the translation vector for a relation. You s- you- these are all the parameters of your model. You would, you know, sample a positive example, a positive triplet. You would, for example, sample ,uh, what is called a corrupted triplet, where you would have the head, the relation the same, but you would pick kind of a random tail. You would pick a country that is not the- the- the true country, uh, of nationality, and then you would say, let me try to, uh, find the embeddings, uh, the values for head, relation, and tail such that the distance between, uh, the real tail and the corrupted tail, uh, the real tail, uh, and the embedding of h, uh, plus r is smaller than the distance between the h plus r and the corrupted tail, which basically means a country that is not, uh, the true nationality. And you can basically do, uh, the learning. So now, uh, that we have done this, this sounds like a great, very natural idea. What is cool about it is that now we don't only think of- of the embeddings as points in the space, but we also learned this vector r that allows us to move or translate, uh, between different points. It allows us to move in a given direction. Um, and of course the question is, what kind of relationships is this type of approach able to, uh, learn? And when I say what kind of relationships, I really mean relationships with different, uh, properties. For example, some properties might be symmetric or reciprocal, right? If, uh, person 1 is a roommate of person 2, then person 2 is also a roommate of person 1, right? It's mutual. Um, for example, then there's- there might be relations that are called inverse, right? If I am someone's advisor, then that other person is my, uh, advisee, right? So advisor and advisee are kind of inverse relations. Whoever is in a relation of advisor, in one way, advisee is in the other, uh, direction. So the question then becomes, can we corre- categorize this type of relation better than sort of relation properties? And, uh, are these knowledge graph embedding methods like the TransE that I discussed? Are they expressive enough to model these patterns? So the- the relation patterns, uh, we are going to discuss are, uh, four. First is symmetric and antisymmetric relations, right? Like, uh, family, roommate are what we call symmetric because they are reciprocal, right? If I'm your roommate, you are also my roommate. It cannot be only one way. Um, antisymmetric would be, uh, would be part of hypernym, hyponym. So it's basically one is with relation, uh, with the other, but not the other way. Uh, inverse, uh, relations would be like advisor advisee. Where if I'm related to you with one relation, you are automatically related to me with the- with a different relation, uh, then it would be, uh, transitive relations. For example, uh, friend of, which would be if x is friends with y and y is friend with z, then, uh, x and z are also friends, right? So this would be one way of having the notion of transitive relations. Or if you think about relationship type, trans- uh, those are also transitive. That for example I say, my mother's husband is my father. So, um, you know, that's, uh, that- that's also saying, aha, if- if you start with me and you- you tra- you transform according to the mother relation, and then you take the mother point and transform according to the fa- husband relation, uh, you should arrive to the same place if you take myself and transfor- me- me according to the father relation, right? The two, uh, the two, uh, uh, the two embeddings should meet. And then another important thing is that there are, uh, 1-to-N relations. For example, student off, right. I can, uh, have multiple students or I can have multiple advisees. So I can have 1-to-N, uh, relations. So these are different relation patterns we are going to consider, and what we are going to do next, is- is we are going to look whether TransE is able to capture these type of relations. So, for example, let's first talk about, uh, antisymmetric relations. Uh, what would this mean is, uh, h is, uh, related with t, uh, t cannot be related, uh, with h according to the same relation, like, uh, part of, uh, hyponym is an example. And- and TransE captures this, right? Because if I go from head, um, according to the relation vector r, I arrive to the tail but now we find that the tail and again, apply this relation transformation, uh, r, I don't get back to h. I get to some other point. So this means that, uh, um, um, TransE naturally captures this antisymmetry, uh, of relations that have this property. Another of the questions is, can TransE capture inverse relations like advisor/advisee, right? So if, uh, if h and t are related by r2, then, you know, there's some other r1 that makes, uh, tail and the head related, right? Can we model, uh, inverse relations? Yes, we can. Uh, in simply the r2- the- the vector r2 should simply be the negative, uh, of the vector r1. So it means that from head according to r1, I move to tail. Then if I wanna move from tail back to head, I just move in the opposite direction. So in TransE, this is naturally captured and naturally doable. So inverse relationships are possible. How about composite or transitive relations? So I, uh, where I say if x and y are related and y and z are related, then x and z should be related as well, right? So in TransE you can simply do this by, uh, by saying r3, uh, is simply a summation of r1 and r2, right? So if I go from x via r1 to y- to y, and then I go from y via r2, uh, to, uh, z, then I can basically simply go from x to z according to, uh, r3 and r3 is simply the vector r1 plus vector r2. So again, we can do, uh, composite relationships with TransE, uh, easily. So how about some relationships types that are- uh that TransE cannot capture? So for example, interestingly, uh, TransE cannot capture symmetric relationships, right? Relationships, uh, um, uh, where it's- uh, where like family or roommate that are reciprocated, right? Basically saying if, uh, h- h- h is related with r to t, then t should be related to h with r as well and- and the point is this right? If you go from h according to r to t, then what you'd like to do is apply the same r to t as well and get back to h and the only able- way you are able to do this is to make r to be 0, which basically means that h and t are embedded on top of each other. But that is not good because h and t are different entities. They are distinct entities with distinct properties. So this means that TransE cannot embed and cannot do this kind of, um, sy- uh, symmetric, uh, reasoning or it cannot model, uh, symmetric, uh, relations. And then another part that, uh, TransE cannot do is 1-to-N relations, right? The idea is that if I have a head, um, uh, and, uh, and, uh, relation, and maybe this is a 1-to-N relation. Uh, then basically I wanna- I wanna go from head according to relation r to t1 and I also wanna be able to go from head according to relation 1 to t2, right? Both exist in the knowledge graph. For example, student of, right? I can be a mentor or I can have two students simultaneously. And again, TransE cannot model this type of relations because again, the only way for this to work would be that t1 and t2 map into the same vector, they map into to the same point in the embedding space so that when you go from h according to r, you arrive to the same, uh, point. But again, this is not- this is not good because this would mean that t1 and t2 are the entities embedded in the same point. So they are indistinguishable from one another. But in reality t1 and t2 are completely different entities. So this is what TransE cannot, uh, model. So let's look at a different method called TransR are that will allow us to fix some of these issues, all right? So what we have seen so far is that the TransE models, uh, trans- a translation of any relation in the same embedding space. So the question is, can you design a new space for each relation and do the translation in a relation-specific space? And the method that proposed this idea is called TransR, and it models entities as vectors in the entity space, so entities as points and models each relation as a vector in rela- in the relation space together with a relation specific, uh, transformation or projection matrix. So, um, let me give you the idea how this works. So the idea is, as I said, TransR models entities as points, uh, and for every relation, we are actually going to learn two things. We are going to learn the translation, uh, relation vector and we are going also to learn this projection matrix, uh, M for every relation. So the idea is, right, if I want to, uh- uh- what- the way I'm going to now predict relations. If I say are h and t related, uh, by relation r, I'm first going to apply matrix, uh, matrix M to both points and basically transform them into a new space and then in this new transformed space, I'm going to kind of apply the TransE, um, intuition. So basically I'm going to use this vector to move around. So, um, right, I basically, take the original embedding. I transform it according to the matrix M, which basically means I scale it, I rotate it, I translate it, and then I apply my vector r, uh, in there. So, uh, you know, what will this buy me? For example, what this buys me is that it allows me to model, uh, symmetric, um, relations because imagine, uh, again a symmetric, uh, relation like roommate reciprocated right? Then basically, I can run a roommate specific, uh, function, uh, specific, uh, projection matrix, uh, M that will take two different points and map them into the same, uh, underlying point, into the relation specific, uh, space. So it means that two people that are roommates, um, will be able to be mapped into the same point. So that now here, uh, I'll be able basically to say, uh, no translation needed, r is 0 and I'm, um, I have this, uh, symmetric, uh, relationship, uh, right now. So this is- what this allows us to do, which for example, TransE was not able to do. Um, how about anti-symmetric relations? Um, again, we can easily do this, right? Because for example, the translation matrix, uh, can- can, if I have- if I take two different points, the translation matrix can- can again transfer, uh, into this relation specific spa- space in such a way that the two points don't collide and then the same way as in TransE, um, uh, we are able to do, uh, the same thing here, right, from head according to the relation I get to t, but from t according to the relation, I don't get back to, uh, head. So this is also possible, so it can model both symmetric as well as anti-symmetric, uh, relations. Can, uh, uh, um, uh, this TransR also model 1-to-N relations, right? Uh, like student of? Uh, and actually again it turns out it can. TransR can model this because again, we have enough flexibility in the projection matrix r that can learn how to take, let's say t_1 and t_2 and map them into the same point so that when in this relation specific space, I go from head according to the relation, I arrive exactly to t_1 and t_2 that are embedded to the same position. So in the original embedding, these are two different points but in the, uh, let's say the relation specific embedding that is achieved by this transformation matrix M, I can enforce t_1 and t_2 to be in the same point. So these 1-to-N relations can naturally be captured. So, um, this sounds great, right? It seems we have kind of fixed, uh, all the issues with, uh, trans, uh, -TransE. We can also do inverse relations because, uh, the same as we were do- able to do them in TransE, we can do them in TransR, right? So basically we just, uh, uh, -the transformation matrices for both relations have to be the same and then, you know, one translation vector is the reciprocal or the inverse, uh, of the other, uh, translation vector. Um, what is the issue? What trans - what tra- what does, uh, TransR- what is it not able to do that TransE, was able to do? Is one thing: is composition relations, right? In, in TransR, you cannot do this type of compositional relations, right? Meaning, uh, remembering, uh, uh, where you say, a-ha, if x, x is related by y with r_1 and y is related with z with r_2, then, uh, x and z are related by r_3. This was very easy to do in, uh, TransE because everything was in the same space and you just kind of moved, uh, around. Uh, but in TransR we cannot do these compositional relations. Um, and the reason for this is that each relation has a different space and we know how to model from the original space to the relation-specific space, but we don't know how to model between different, uh, relation-specific, uh, spaces. So these relation-specific spaces are not naturally, uh, compositional. So, uh, this means that the, uh, compositional relations, uh, TransR, uh, cannot, uh, do. So, um, this was second method, uh, we discussed. So let's now talk, uh, about the third method as well that, uh, is, uh, -that is based on what is called bilinear modeling. Um, and so far, right, we were using, uh, the scoring function simply as a, as a distance in the embedding space. Like in TransE and TransR we said the head plus relation should be close to the tail. But, um, now what if we change, uh, the embedding, uh, the, the way we're scoring different relations? What if we change this function n-l, f? But it won't be, uh, distance anymore but something more interesting. So for example, uh, this DistMult, uh, is simply saying let's embed entities and relations as vectors in the, uh, in the embedding space. Kind of similar to TransE. But let's use a different, uh, scoring function rather than using the distance, uh, as in TransE. Uh, let's use, uh, this type of scoring function where we basically just, uh, have this bi- bi-linear model when we say the score is simply, a coordinate wise product of h, r, and t, right? So the idea is if I have, uh, h, uh, r and t, I simply multiply these things, uh, uh, entry by entry, sum it up, and that is my score. And the idea is if, uh, head relation tail, uh, is true, then the score should be high and otherwise, the sh- the score, uh, should below. The way you can think of this scoring function is that you can think of it as a hyperplane, uh, in the embedding space. So basically, wha-the way you can think of it is you can think of it as cosine similarity between h times r, uh, and t. And cosine similarity is simply a cosine of the angle between two vectors. Um, and what this means is that if the two vectors are orthogonal, then the cosine similarity will be 0, and if two vectors are completely aligned, uh, the cosine, uh, similarity will be, uh, will be 1. Uh, another thing that is important is that because this now defines like a hyperplane in the embedding space, this dot product can either be positive or it could be negative. And whether it's positive or negative, this tells me whether the, the point lives, lies on the left-hand side or whether it lies on the right-hand side of the, um, of the hyperplane. And hyperplane is really defined as a vector that is, um, uh, that is orthogonal to this hyperplane and this vector is simply h times r, right? So basically h times r, uh, defines a hyperplane that is orthogonal, uh, orthogonal to it and then, uh, the, the tails, uh, that, that are related to h according to relation r should fall on the same side of the hyper- hyperplane as this, uh, um, uh, uh, normal vector is, and tails that are not related to it should fall on the other side, uh, of the hyperplane. So that's, uh, the intuition and the idea how this DistMult, uh, works. So can it do 1-to-N relations? uh, yes it can. Uh, the reason, the reason for this is because we can think again of, of a, of a, of a hyperplane and if, uh, if, uh, I have 1-to-N relations, then t_1 and t_2 should fall on the same side or should fall on the same distance on or, uh, from the, uh, hhyperplane and that's easy to achieve. Um, in terms of symmetric relations, again, we can, uh, naturally model this, uh, because, uh, multiplication is commutative, right? So we can flip the order in which we multiply things and we'll get, we'll get to the same value. So because, uh, um, summation and multiplication are commutative, um, this is, uh, naturally true. Uh, however, the limitation are anti-symmetric relations, right? Like a hypernym, part of where uh, uh and, uh, the-the idea here is that DistMult cannot model antisymmetric relations. Um, again, because of the, uh, commutativity, uh, of, uh, summation and, and product, it means you get symmetry but you don't get, uh, antisymmetry, right? Um, h times r times t will always be the same as t times r times h and that's, that's the issue. So, uh, we can- DistMult cannot do that. And then the other thing DistMult cannot do is modeling inverse relations, right? If, uh, uh, h and t are related with r_2 then they should also be related, uh, in the different direction with r_1. Like advisor, advisee type of relation. Um, and the reason, uh, um, uh, DistMult cannot, uh, model this is, uh, because, uh, uh, for- because of the fact that it does not model rel- uh, inverse relations so the only way to model inverse relation would be that r_2 and r_1 are equal. But semantically this does not make sense because then the embedding of the relation advisor is the same as the embedding of the relation advisee. So basically this would mean that if I'm your advisor, you are my advisee but at the same time, if I say, are you my advisor as well, the answer would be yes and am I your advisee, the answer would be yes as well. So it will be like two symmetric relations rather than a inverse relation. So this is where, uh, DistMult fails [NOISE]. Um, and then the last place, uh, where, uh, DistMult fails is, uh, composition, uh, relations, right? And the problem is that DistMult defines hyperplane for, uh, each, uh, head, er, relation pair and the union of hyperplanes induced by this, er, by this composition of relations cannot be expressed by a single, uh, hyperplane. So kind of, you know, a union or an intersection of hyperplanes is not a hyperplane, uh, anymore. So that's why- th- that's the intuition why composition relations, uh, are also not possible. So the last method I want to talk to you about is called complEx. And the idea here- the reason why I want to do this is that we don't necessarily have to embed points into the simple Euclidean er, you know, er, real, uh, space. We can also, um, have complex vector, uh, embeddings. We can embed them in complex spaces, right? So ComplEx embeds entities using in the complex vector space, right? Where every- every point now has, uh, two, er- it has an imaginary part and a real part. And the one concept from, um, com- uh, ComplEx algebra that is important to keep in mind is this concept of co- complex conjugate, where you basically say, if u is a complex number with a real part a and imaginary part, uh, b, then a complex conjugate of it is simply that it's not plus b times i, but is minus b times i. And this, uh, notion of complex conjugate will be important as we analyze, uh, this model. So the way you can now think of this is that in ComplEx we can actually use, um, the scoring function that is simply, uh, similar to the, uh- to the ComplEx- uh, to the scoring function in DistMult. But we are only then taking the real part of the- of the ComplEx function because, uh, this is- er, this will give me a complex number. So I wi- want to only take, uh, the real, uh, part of it. So let's quickly analyze, uh, this approach as well. So can it model antisymmetric relations? Uh, yes, it can. Uh, the way- the way we, uh- we can do this is that basically we can achieve that for, uh, different, uh, um, uh, relations. Uh, we get basically high or low value of our scoring function, uh, basically due to the, uh, asymmetric modeling of complex, uh, conjugates. Because here we are using, uh, the complex, uh, conjugate. Uh, can we model symmetric relations? Again, um, yes, yes, we can because we can simply say the imaginary part of the relation, uh, to be 0, um, and then, uh, because everything works in- in- in- in the real part, uh, we are able to model, uh, symmetric relations, um, as well. And then, you know, can we mo- can complex model, uh, symmetri- uh, inverse relations as well? Uh, yes, it can. How? By setting, uh, r- um, uh, by setting r_2 to be, uh, com- complex conjugate of, uh, r_1. Um, and this basically means that, uh, the real part of the two er, will be, uh, exactly, uh, the same because what differs is only, uh, the complex, uh, part. So that is, um, uh, in terms of modeling inverse, uh, relations. Uh, how about composition and 1-toN relations? Um, because, uh, ComplEx shares this property with DistMult, meaning it uses the same, uh, scoring function, then it turns out that ComplEx also cannot model composition relations as well as, uh, 1-to-N uh, relations because it uses kind of a very similar, uh, scoring function. So this basically now concludes, uh, our discussion of different knowledge graph embedding, uh, methods for knowledge graph completion. We- we went through four of them. We went through TransE, TransR, DistMult and ComplEx. And what is interesting is that each of these methods has a bit different idea, right? Um, the- the two- first two methods embed into the- into the, um, real, uh, space, so into the Euclidean space. Uh, but one uses the translation, uh, idea for every relation, while TransR also uses this, er, um, transformation matrix, er M. And, uh, here I show different properties or different relation types that these methods can model. Um, TransE is the only one that can model composite relations where they can be composed. Uh, DistMult changes the scoring function, um, and, uh, and ComplEx actually embeds, uh, not into the Euclidean space, but in the complex space. And the reason why I wanted to show you these different methods is to see kind of the diversity of options that, uh, embeddings, uh, allow you, uh, to work with and, uh, a lot- give you a lot of freedom how you define your predictive model. So how do you do now knowledge graph embeddings in practice? Uh, different knowledge graphs may have drastically different relational patterns and different properties. So it is not that there is one method that works best, uh, on all of them. It really depends on what kind of, uh, relationship types you are interested in modeling and, uh, what kind of relations do you wanna predict. If you really want to predict, uh, 1-to-N relations and model 1-to-N relations, then don't use TransE. if you really care about composite relations and these kind of compositions, then use, for example, TransE. If what you care about, uh, for example, would be antisymmetric relations, don't use DistMult, um, and so on. So it really depends, uh, on- on the graph and what you wanna do. In general, you know, TransR are and ComplEx have the most kind of- are able to model the most, uh, diverse set of different, uh, relation types. So what you can always try is try with TransE because it's very, uh, simple and has a very nice intuition. And then you can use more expressive model like complEx, uh, TransE. You- there's also an notion, a model called trans- RotatE, which is TransE, but with complex embeddings. Um, so there is a lot of different approaches and different, er, uh, ideas here. There is a paper linked up here that s- uh, that serves as a very good follow-up, uh, reading if you are interested in this topic. So to summarize, we talked about the, uh, knowledge graph, embedding a knowledge graph completion task as one of the very important task in knowledge graphs. And we talked about four different methods. TransE, TransR, DistMult and ComplEx, er, models with different embedding space, and with different type level of expressivity, allowing us to mi- model, uh, different types of um, relationships. 
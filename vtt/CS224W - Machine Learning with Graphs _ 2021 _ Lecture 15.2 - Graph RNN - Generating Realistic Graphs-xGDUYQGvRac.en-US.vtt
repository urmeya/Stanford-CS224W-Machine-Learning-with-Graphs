WEBVTT
Kind: captions
Language: en-US

00:00:05.060 --> 00:00:05.720
So.

00:00:05.720 --> 00:00:10.340
Now, we are ready to look
about our generative model

00:00:10.340 --> 00:00:14.300
for graphs, and this generative
model is called graphRNN.

00:00:14.300 --> 00:00:18.170
And, it will allow us to
generate realistic graphs

00:00:18.170 --> 00:00:23.420
without making any kind of
inductive bias assumptions.

00:00:23.420 --> 00:00:25.670
And the model will
be as we will see--

00:00:25.670 --> 00:00:28.760
extremely general and
scalable, and being

00:00:28.760 --> 00:00:31.940
able to generate
a vast diversity

00:00:31.940 --> 00:00:35.570
of different types of graphs,
of different sizes and shapes,

00:00:35.570 --> 00:00:38.370
and structures, and so on.

00:00:38.370 --> 00:00:42.290
So the key idea that
this is-- that we

00:00:42.290 --> 00:00:44.360
are going to harness
here, is that we

00:00:44.360 --> 00:00:47.180
want to generate
graphs via sequential

00:00:47.180 --> 00:00:48.800
adding of nodes and edges.

00:00:48.800 --> 00:00:51.290
And why do we want
to do it this way?

00:00:51.290 --> 00:00:54.020
We want to do it this
way-- exactly what I

00:00:54.020 --> 00:00:56.600
explained at the end of
the previous segment.

00:00:56.600 --> 00:00:58.190
Which is, that
basically, we want

00:00:58.190 --> 00:01:01.340
to model a complex distribution
that captures distribution

00:01:01.340 --> 00:01:02.360
over graphs.

00:01:02.360 --> 00:01:04.170
Modeling that complex
distribution--

00:01:04.170 --> 00:01:05.330
we don't know how to do.

00:01:05.330 --> 00:01:07.760
So we are going to break
that complex distribution

00:01:07.760 --> 00:01:09.380
into many small parts.

00:01:09.380 --> 00:01:11.960
And the way we break
creation of the graph,

00:01:11.960 --> 00:01:14.000
we are going to break it
into many small parts,

00:01:14.000 --> 00:01:15.590
into many small actions.

00:01:15.590 --> 00:01:16.610
And we are simply--

00:01:16.610 --> 00:01:20.660
all we'll have to do is model a
model transition probabilities.

00:01:20.660 --> 00:01:22.610
Basically saying,
given the graph,

00:01:22.610 --> 00:01:26.090
given the actions we have made
so far, conditioning on that,

00:01:26.090 --> 00:01:28.760
what is the next
action we want to take?

00:01:28.760 --> 00:01:30.990
And this way, by
multiplying these together,

00:01:30.990 --> 00:01:34.160
we are able to model the
entire distribution--

00:01:34.160 --> 00:01:36.170
all the dependencies in it.

00:01:36.170 --> 00:01:40.250
So in our case, the
idea is that, imagine I

00:01:40.250 --> 00:01:42.440
have this graph, and
I want to generate it.

00:01:42.440 --> 00:01:45.320
The way I'm going to describe
the generation process,

00:01:45.320 --> 00:01:47.360
is that, I'll start
with the first node,

00:01:47.360 --> 00:01:49.640
and then, I'll add
the second node.

00:01:49.640 --> 00:01:52.080
The second node will
link to the first one.

00:01:52.080 --> 00:01:54.830
Now I have already built
a bit of the graph,

00:01:54.830 --> 00:01:57.440
a third node is added and
links to the first node,

00:01:57.440 --> 00:02:00.560
then node number 4 is added
and links to two and three,

00:02:00.560 --> 00:02:04.980
and then node number 5 is added
and it links to three and four.

00:02:04.980 --> 00:02:06.710
So that's basically,
the generative

00:02:06.710 --> 00:02:09.650
process I want to model
and I want to capture.

00:02:09.650 --> 00:02:11.960
And in this part
of the lecture, I'm

00:02:11.960 --> 00:02:18.440
going to explain to you
the model called graphRNN.

00:02:18.440 --> 00:02:21.090
And, here is the
link to the paper,

00:02:21.090 --> 00:02:24.290
so if you want more details
you can read the paper

00:02:24.290 --> 00:02:27.280
that I'm citing here.

00:02:27.280 --> 00:02:31.470
So let's think about this
modeling graphs or generating

00:02:31.470 --> 00:02:33.030
graphs as sequences.

00:02:33.030 --> 00:02:35.940
A graph with node
ordering pi can

00:02:35.940 --> 00:02:39.660
be uniquely mapped into a
sequence of nodes and edge

00:02:39.660 --> 00:02:43.110
additions S. What
do I mean by this?

00:02:43.110 --> 00:02:46.320
Graph is a set of nodes
and a set of objects,

00:02:46.320 --> 00:02:50.140
and apriori graph has
no ordering to it.

00:02:50.140 --> 00:02:53.580
So we need to this node
ordering, this permutation pi,

00:02:53.580 --> 00:02:57.300
that determines the order
in which the nodes appear.

00:02:57.300 --> 00:03:00.540
And given that ordering
in which the nodes appear,

00:03:00.540 --> 00:03:04.590
then the sequence to generate
the graph is uniquely defined.

00:03:04.590 --> 00:03:07.030
Because, first, I
add the second node.

00:03:07.030 --> 00:03:07.530
Sorry.

00:03:07.530 --> 00:03:09.863
First, I add the first node,
then I add the second node,

00:03:09.863 --> 00:03:10.950
I add the third node.

00:03:10.950 --> 00:03:14.370
And then, in terms of adding
edges I can always say I go--

00:03:14.370 --> 00:03:15.950
I ask, should the
link the node 1?

00:03:15.950 --> 00:03:16.480
Yes, no.

00:03:16.480 --> 00:03:17.480
Should I link to node 1?

00:03:17.480 --> 00:03:18.120
Yes, no.

00:03:18.120 --> 00:03:22.210
Until all the nodes after
already present in the graph.

00:03:22.210 --> 00:03:25.860
So that is basically the
key-- the key idea is

00:03:25.860 --> 00:03:28.260
that we need some node
ordering, and given

00:03:28.260 --> 00:03:30.810
that no node ordering,
then generating the graph

00:03:30.810 --> 00:03:34.400
is simply a sequence problem.

00:03:34.400 --> 00:03:37.520
Another observation that is
important for graph generation,

00:03:37.520 --> 00:03:41.210
is that, this sequence is
actually a two-level sequence.

00:03:41.210 --> 00:03:43.370
There is a sequence
of adding nodes,

00:03:43.370 --> 00:03:45.740
and there is a sequence
of adding edges.

00:03:45.740 --> 00:03:51.890
Right at node level sequence,
at each step we add a new node.

00:03:51.890 --> 00:03:55.040
And then, we also
have the second level

00:03:55.040 --> 00:03:59.690
after a node is added the edges
need to be added for that given

00:03:59.690 --> 00:04:00.290
node.

00:04:00.290 --> 00:04:03.590
So this means we s a two-level
sequence where we will first

00:04:03.590 --> 00:04:06.420
have the-- we have the high
level node level sequence.

00:04:06.420 --> 00:04:09.230
And then, the low level
edge level sequence.

00:04:09.230 --> 00:04:11.840
Node level, a step--

00:04:11.840 --> 00:04:14.390
we do one node
level step and then

00:04:14.390 --> 00:04:15.903
we do a lot of edge level steps.

00:04:15.903 --> 00:04:17.320
And then, we do a
node level step,

00:04:17.320 --> 00:04:19.190
and we do a lot of
edge level steps.

00:04:19.190 --> 00:04:21.860
And each edge level
step is simply

00:04:21.860 --> 00:04:23.720
an addition of a new edge.

00:04:23.720 --> 00:04:28.280
So you can think of node
level steps being, add node 1,

00:04:28.280 --> 00:04:30.770
add node 2, and node 3.

00:04:30.770 --> 00:04:36.680
While the sequence elements
in the edge level are,

00:04:36.680 --> 00:04:38.690
should I add this node--

00:04:38.690 --> 00:04:41.720
this new node that I have added,
should be connect it to node 1?

00:04:41.720 --> 00:04:43.310
Should I connect it to node 2?

00:04:43.310 --> 00:04:44.790
Should I connect it to node 3?

00:04:44.790 --> 00:04:46.730
And you can think of
this as generating

00:04:46.730 --> 00:04:51.080
a binary sequence that says,
don't connect, connect,

00:04:51.080 --> 00:04:54.810
connect, So that's the idea.

00:04:54.810 --> 00:04:57.660
So a summary of what
we have learned so far,

00:04:57.660 --> 00:04:59.790
is that, a graph
plus a node ordering

00:04:59.790 --> 00:05:02.700
gives us a unique
sequence of sequences

00:05:02.700 --> 00:05:04.410
to generate a given graph.

00:05:04.410 --> 00:05:07.860
A node ordering for now, let's
assume is randomly selected

00:05:07.860 --> 00:05:10.800
or somehow it's given to us,
will come to this question

00:05:10.800 --> 00:05:11.782
later.

00:05:11.782 --> 00:05:13.740
And the way you can think
of this is right if--

00:05:13.740 --> 00:05:16.290
basically is to say, Oh, if I
have a partially built graph

00:05:16.290 --> 00:05:19.410
and I want to add a new
node, then adding this node

00:05:19.410 --> 00:05:22.560
to the graph simply
means I have to print out

00:05:22.560 --> 00:05:25.325
this particular column
of the adjacency matrix.

00:05:25.325 --> 00:05:27.510
So I'd basically
saying, node 4 doesn't

00:05:27.510 --> 00:05:30.180
link to node 1, node
4 links to node 2,

00:05:30.180 --> 00:05:32.370
and node 4 links to node 3.

00:05:32.370 --> 00:05:35.010
This is first node,
second node, third node,

00:05:35.010 --> 00:05:38.700
and we just generated the
column of the adjacency matrix

00:05:38.700 --> 00:05:40.380
for the node number 4.

00:05:40.380 --> 00:05:44.520
So node level sequence is
going to add one new column

00:05:44.520 --> 00:05:46.110
to the adjacency matrix.

00:05:46.110 --> 00:05:49.620
And edge level sequence is
going to kind of print out

00:05:49.620 --> 00:05:52.440
the rows-- the entries
of this column, where

00:05:52.440 --> 00:05:55.290
0 means the edge
does not exist and 1

00:05:55.290 --> 00:05:58.510
means that the edge exists.

00:05:58.510 --> 00:05:59.740
So.

00:05:59.740 --> 00:06:02.380
So far we have transformed
the graph generation problem

00:06:02.380 --> 00:06:04.300
into a sequenced
generation problem.

00:06:04.300 --> 00:06:09.310
Now, we need to model the
two processes, the process

00:06:09.310 --> 00:06:11.830
of adding new nodes, where
basically, we want to generate

00:06:11.830 --> 00:06:13.690
a state for a new node.

00:06:13.690 --> 00:06:17.230
And then, we want to generate
edges for the new node

00:06:17.230 --> 00:06:18.610
based on it's state.

00:06:18.610 --> 00:06:20.860
And will be the
edge level sequence.

00:06:20.860 --> 00:06:24.070
So the point will be that, an
approach we are going to use

00:06:24.070 --> 00:06:25.480
is-- we are going
to use what are

00:06:25.480 --> 00:06:27.700
called recurrent neural
networks to model

00:06:27.700 --> 00:06:29.380
this two-level sequence.

00:06:29.380 --> 00:06:33.640
And we are going to do use a
nested recurrent neural network

00:06:33.640 --> 00:06:35.690
as I'm going to explain.

00:06:35.690 --> 00:06:38.110
So the idea is the
following, what

00:06:38.110 --> 00:06:40.560
are recurrent neural networks?

00:06:40.560 --> 00:06:44.230
Recurrent neural networks are
designed for sequential data.

00:06:44.230 --> 00:06:46.270
And a recurrent neural
network sequentially

00:06:46.270 --> 00:06:50.480
takes an input sequence to
update its hidden state.

00:06:50.480 --> 00:06:52.623
And based on the hidden state--

00:06:52.623 --> 00:06:54.040
the idea is that
this hidden state

00:06:54.040 --> 00:06:58.390
summarizes all the information
input to the RNN so far.

00:06:58.390 --> 00:07:02.050
And then, the
update is conducted

00:07:02.050 --> 00:07:04.850
via what is called RNN
cells, for every time

00:07:04.850 --> 00:07:06.760
step I have a new RNN cell.

00:07:06.760 --> 00:07:08.410
So the way you can
think of it is,

00:07:08.410 --> 00:07:11.020
I initialize the RNN
with some hidden state,

00:07:11.020 --> 00:07:13.410
I give it the input.

00:07:13.410 --> 00:07:17.620
The RNN is going to update its
hidden state and put it here,

00:07:17.620 --> 00:07:19.990
and it's going also
to create an output.

00:07:19.990 --> 00:07:23.500
And now, the second time
step this hidden state--

00:07:23.500 --> 00:07:25.900
next cell is going
to take as the input.

00:07:25.900 --> 00:07:29.770
It's going to take as the input
the input from the environment,

00:07:29.770 --> 00:07:34.840
it's going to update its hidden
state, here denoted as S2,

00:07:34.840 --> 00:07:38.140
and it's going also to
produce me an output.

00:07:38.140 --> 00:07:40.390
So basically, the idea
is that this hidden state

00:07:40.390 --> 00:07:45.340
s keeps memory of what the
RNN-- what kind of inputs

00:07:45.340 --> 00:07:47.890
the RNN has seen so far.

00:07:47.890 --> 00:07:52.200
So to tell you more and
give you more mathematics,

00:07:52.200 --> 00:07:55.990
S sub t is the state of
the RNN at the step t.

00:07:55.990 --> 00:07:59.590
X sub t is the input to
the RNN at that time,

00:07:59.590 --> 00:08:03.550
and Y sub t is the output
of the RNN under that time.

00:08:03.550 --> 00:08:06.670
And the RNN is-- has
several parameters,

00:08:06.670 --> 00:08:09.370
and these parameters
are called W, U, and V.

00:08:09.370 --> 00:08:11.350
These are trainable
parameters, this

00:08:11.350 --> 00:08:14.950
could be trainable vectors,
trainable matrices.

00:08:14.950 --> 00:08:16.690
And the way the
update equations go

00:08:16.690 --> 00:08:21.040
is the following, hidden state
gets updated by basically

00:08:21.040 --> 00:08:24.430
saying, what is the hidden
state in the previous time step?

00:08:24.430 --> 00:08:27.070
Let's transform it.

00:08:27.070 --> 00:08:30.700
What is the new input
of the current time?

00:08:30.700 --> 00:08:32.370
Let's transform it.

00:08:32.370 --> 00:08:34.299
Pass It through
the nonlinearity,

00:08:34.299 --> 00:08:36.760
and that is my
updated hidden state.

00:08:36.760 --> 00:08:40.840
And then, the output Y is
simply a transformation

00:08:40.840 --> 00:08:44.007
of the hidden state
and this is what the--

00:08:44.007 --> 00:08:45.820
what we are going to output.

00:08:45.820 --> 00:08:50.110
In our case, S, X,
o t will be scalars.

00:08:50.110 --> 00:08:52.090
So basically, we
are going to output

00:08:52.090 --> 00:08:59.170
a scalar that will tell us
whether there is an edge

00:08:59.170 --> 00:09:00.310
or not.

00:09:00.310 --> 00:09:02.320
Of course you can have
more expressive cells

00:09:02.320 --> 00:09:05.330
like GRU, LSTM, and so on.

00:09:05.330 --> 00:09:08.050
But here I am talking
about RNN, which

00:09:08.050 --> 00:09:12.810
basically, is the most basic
of this sequence based models.

00:09:12.810 --> 00:09:13.710
So.

00:09:13.710 --> 00:09:16.560
So far we learned what is RNN.

00:09:16.560 --> 00:09:18.420
Now, we want to talk
about how to use

00:09:18.420 --> 00:09:22.860
the RNN to model the node level
and the edge level sequence.

00:09:22.860 --> 00:09:26.400
And the relationship between
two RNNs is the following,

00:09:26.400 --> 00:09:29.670
is that, node level RNN
generates the initial state

00:09:29.670 --> 00:09:33.750
for the edge level RNN,
and then, edge level RNN

00:09:33.750 --> 00:09:38.260
is going to produce the sequence
of edges for that new node.

00:09:38.260 --> 00:09:41.820
And then, it's going to pass
its state back to the node level

00:09:41.820 --> 00:09:46.560
RNN, who's going to generate
a new node, update the state,

00:09:46.560 --> 00:09:50.050
and push it down to
the edge level RNN.

00:09:50.050 --> 00:09:53.710
So this is how this
is going to work.

00:09:53.710 --> 00:09:58.580
So to give you an idea, we will
initialize the node level RNN

00:09:58.580 --> 00:10:02.750
with a start of a
sequence, and we will--

00:10:02.750 --> 00:10:05.840
the node level RNN is
going to create a node.

00:10:05.840 --> 00:10:09.050
Now, the edge level RNN--

00:10:09.050 --> 00:10:10.850
given this node is
created a node level

00:10:10.850 --> 00:10:14.090
RNN will add a new node, and
then, ask the edge level RNN

00:10:14.090 --> 00:10:15.470
to generate edges for it.

00:10:15.470 --> 00:10:17.660
So node 1 is already there.

00:10:17.660 --> 00:10:19.790
Now, node level
RNN decides to add

00:10:19.790 --> 00:10:21.470
one more node, node number 2.

00:10:21.470 --> 00:10:25.130
Now, the edge level RNN has
to say, does 2 link to 1,

00:10:25.130 --> 00:10:31.460
and this number 1 here,
would say that 1 links to 2.

00:10:31.460 --> 00:10:32.970
And, here is now the new graph.

00:10:32.970 --> 00:10:37.850
Now the node RNN says, OK,
let's add a new node, number 3.

00:10:37.850 --> 00:10:40.820
Now, we ask the edge
level RNN to generate

00:10:40.820 --> 00:10:43.490
the edges for the node 3.

00:10:43.490 --> 00:10:46.760
And node 3 will say, it
links to the first node,

00:10:46.760 --> 00:10:48.690
but does not link
to the second node.

00:10:48.690 --> 00:10:50.800
So now our new
graph is like this.

00:10:50.800 --> 00:10:52.820
And again, the node
level RNN decides,

00:10:52.820 --> 00:10:55.370
let's add one more
node, let's add node 4.

00:10:55.370 --> 00:10:58.550
And the edge level RNN
prints out-- basically,

00:10:58.550 --> 00:11:01.400
a sequence of zeros
and ones that will mean

00:11:01.400 --> 00:11:03.350
does falling to node 1?

00:11:03.350 --> 00:11:04.520
Does it link to node two?

00:11:04.520 --> 00:11:05.810
Or does it link to node 3?

00:11:05.810 --> 00:11:07.760
And 4 links to 2 and 3.

00:11:07.760 --> 00:11:09.200
So, here is now
the current graph.

00:11:09.200 --> 00:11:12.530
And then, node level RNN says,
OK, let's add one more node,

00:11:12.530 --> 00:11:13.820
node 5.

00:11:13.820 --> 00:11:16.550
Edge level RNN
outputs the-- four

00:11:16.550 --> 00:11:19.550
every previous node it tells
us, whether 5 should link to it

00:11:19.550 --> 00:11:21.250
or not.

00:11:21.250 --> 00:11:23.630
And that's how we get the graph.

00:11:23.630 --> 00:11:25.100
And the generation
will stop when

00:11:25.100 --> 00:11:27.200
the node level RNN
we'll say, I'm done,

00:11:27.200 --> 00:11:29.510
I won't create any new nodes.

00:11:29.510 --> 00:11:33.140
So this is the idea, where
edge level RNN sequentially

00:11:33.140 --> 00:11:35.180
predicts if a new
node will connect

00:11:35.180 --> 00:11:37.790
to each of the previous nodes.

00:11:37.790 --> 00:11:41.840
Now, how do we do this with RNN?

00:11:41.840 --> 00:11:44.790
So let me give you more ideas.

00:11:44.790 --> 00:11:49.490
So in terms of how we use the
RNN to generate the sequence,

00:11:49.490 --> 00:11:51.980
we are basically going
to take the output

00:11:51.980 --> 00:11:54.320
from-- of the previous
cell, and put it

00:11:54.320 --> 00:11:56.360
as an input of the next cell.

00:11:56.360 --> 00:11:59.720
So whatever the
previous decision output

00:11:59.720 --> 00:12:01.850
was, for example, whatever
was the previous edge--

00:12:01.850 --> 00:12:04.850
that would be the input to
the next cell or the next time

00:12:04.850 --> 00:12:05.700
step.

00:12:05.700 --> 00:12:08.030
How do we initialize
the input sequence?

00:12:08.030 --> 00:12:10.310
We initialize it to
have a special token.

00:12:10.310 --> 00:12:11.990
We basically train
the neural network

00:12:11.990 --> 00:12:14.120
that when it gets
this special token--

00:12:14.120 --> 00:12:17.120
we'll call this token
SOS, so start of sequence.

00:12:17.120 --> 00:12:21.410
As the initial
input, the network

00:12:21.410 --> 00:12:22.730
started generating the output.

00:12:22.730 --> 00:12:25.850
Usually, SOS-- when I
say a special token,

00:12:25.850 --> 00:12:27.590
it could be a
vector of full zeros

00:12:27.590 --> 00:12:29.270
or it could be a
vector full ones.

00:12:29.270 --> 00:12:31.670
For example, just
something you reserve.

00:12:31.670 --> 00:12:34.310
And then, when do we
stop the generation--

00:12:34.310 --> 00:12:39.230
we stop the generation when
the end of sequence token

00:12:39.230 --> 00:12:40.370
is produced.

00:12:40.370 --> 00:12:44.090
And if end of sequence is
0, then RNN will continue

00:12:44.090 --> 00:12:48.980
generation , and if end of
sequence is one the RNN will

00:12:48.980 --> 00:12:51.810
stop the generation.

00:12:51.810 --> 00:12:53.930
So to give you an idea now.

00:12:53.930 --> 00:12:56.510
How do this-- how does
this fit together?

00:12:56.510 --> 00:13:00.020
Will somehow initialize the
hidden state, will input

00:13:00.020 --> 00:13:02.030
the start of sequence token.

00:13:02.030 --> 00:13:04.250
The RNN is going to
generate some output

00:13:04.250 --> 00:13:08.480
and then we are going
to connect this output

00:13:08.480 --> 00:13:10.070
as the input to the next step.

00:13:10.070 --> 00:13:12.560
And of course, the hidden
state will get updated.

00:13:12.560 --> 00:13:14.600
And now, given the
hidden state, and given

00:13:14.600 --> 00:13:16.760
the output from
the previous state,

00:13:16.760 --> 00:13:18.800
this is again going
to be combined here,

00:13:18.800 --> 00:13:20.660
another output is
going to be produced,

00:13:20.660 --> 00:13:24.920
and another hidden state
is going to be updated.

00:13:24.920 --> 00:13:26.553
This is all good.

00:13:26.553 --> 00:13:28.970
But the problem with the model
as I've wrote it right now,

00:13:28.970 --> 00:13:30.620
is that, it's all deterministic.

00:13:30.620 --> 00:13:32.750
It's all a set of
deterministic equations

00:13:32.750 --> 00:13:36.920
that are in a deterministic
way generating these edges

00:13:36.920 --> 00:13:37.760
and nodes.

00:13:37.760 --> 00:13:39.950
So what we want is, we
want the stochastic model.

00:13:39.950 --> 00:13:43.300
We want a model that will
have some randomness to it.

00:13:43.300 --> 00:13:52.090
So our goal is to use RNN to
do-- to model this p model

00:13:52.090 --> 00:13:54.850
and RNN really is--
we are using it

00:13:54.850 --> 00:13:59.640
to model this product of
conditional distributions.

00:13:59.640 --> 00:14:05.980
So this marginal distribution of
p of Xt given everything else.

00:14:05.980 --> 00:14:10.120
And let's write that
Y sub t is the p

00:14:10.120 --> 00:14:16.360
model of X sub t
given all the X's that

00:14:16.360 --> 00:14:18.400
were previously generated.

00:14:18.400 --> 00:14:22.990
Then this means we need to
be able to sample Xt plus 1

00:14:22.990 --> 00:14:24.040
from Yt.

00:14:24.040 --> 00:14:26.590
So essentially, we
want to be able to get

00:14:26.590 --> 00:14:31.630
to sample from this p model,
from this single dimensional

00:14:31.630 --> 00:14:32.650
distribution.

00:14:32.650 --> 00:14:34.930
This means that
each step of RNN--

00:14:34.930 --> 00:14:38.380
Now, it's going to output a
probability of a single edge.

00:14:38.380 --> 00:14:41.020
So each step of
the edge level RNN

00:14:41.020 --> 00:14:43.540
is going to output the
probability of a single edge.

00:14:43.540 --> 00:14:45.400
And then, based on
that probability,

00:14:45.400 --> 00:14:48.460
we are going to
flip a coin, if we

00:14:48.460 --> 00:14:50.440
are going to sample from
the Bernoulli defined

00:14:50.440 --> 00:14:52.070
by that probability.

00:14:52.070 --> 00:14:55.030
And then, whether
we get 0 or 1, that

00:14:55.030 --> 00:14:57.600
is actually going to be
input to the next step.

00:14:57.600 --> 00:15:01.750
So rather than saying the
RNN generates an edge,

00:15:01.750 --> 00:15:03.970
RNN generates a
probability, and then, we

00:15:03.970 --> 00:15:08.320
have a stochastic event,
a coin flip without bias,

00:15:08.320 --> 00:15:11.320
that then lands heads
or tails, zero or one,

00:15:11.320 --> 00:15:14.330
and we use that as an
input to the next step.

00:15:14.330 --> 00:15:20.030
So that is essentially the idea.

00:15:20.030 --> 00:15:22.730
So at the generation
time, at the test time,

00:15:22.730 --> 00:15:26.030
let's assume we have
already trained the model.

00:15:26.030 --> 00:15:27.860
Then, how this is
going to work, is

00:15:27.860 --> 00:15:31.370
that, our Y sub t will be a
probability, will be a scalar,

00:15:31.370 --> 00:15:33.890
will be basically a
Bernoulli distribution.

00:15:33.890 --> 00:15:40.250
And let's use this square to
mean that this is a probability

00:15:40.250 --> 00:15:44.270
and this square takes
value 1 with probability p,

00:15:44.270 --> 00:15:47.510
and it takes value 0 with
probability 1 minus p.

00:15:47.510 --> 00:15:49.970
So the idea will be
that, we will start--

00:15:49.970 --> 00:15:53.690
we'll start our RNN, it
will output a probability.

00:15:53.690 --> 00:15:57.290
And now, we are going to
flip a coin without bias

00:15:57.290 --> 00:15:59.750
to create-- to decide whether
there is an edge or not.

00:15:59.750 --> 00:16:02.030
And whatever is the
output of this coin,

00:16:02.030 --> 00:16:05.330
we are going to use this as
an input to the next state--

00:16:05.330 --> 00:16:06.290
to the next cell.

00:16:06.290 --> 00:16:08.570
And again, we are going to
get another probability,

00:16:08.570 --> 00:16:11.930
flip the coin, get
the realization,

00:16:11.930 --> 00:16:15.680
and include that as an input
to the RNN cell who's going

00:16:15.680 --> 00:16:17.943
to give me another probability.

00:16:17.943 --> 00:16:19.610
So this is how we are
going to generate,

00:16:19.610 --> 00:16:22.640
or how we are going
to unroll this RNN.

00:16:22.640 --> 00:16:26.120
Now, the question is, how
do we use the training data?

00:16:26.120 --> 00:16:29.560
How do we use the training
graphs that we are given?

00:16:29.560 --> 00:16:31.270
Write this Xs.

00:16:31.270 --> 00:16:34.150
And, in order to
train the model,

00:16:34.150 --> 00:16:36.760
we are assuming that
we are observing

00:16:36.760 --> 00:16:38.510
the graph that was given to us.

00:16:38.510 --> 00:16:40.930
So this means we observe
a sequence of edges,

00:16:40.930 --> 00:16:42.070
we basically assume--

00:16:42.070 --> 00:16:45.320
we observe how the given
graph was generated.

00:16:45.320 --> 00:16:47.800
So we observe zeros
and ones whether--

00:16:47.800 --> 00:16:50.650
that corresponds to where
the edges exist or edges

00:16:50.650 --> 00:16:51.640
don't exist.

00:16:51.640 --> 00:16:54.670
And we are going to use this
notion of teacher forcing,

00:16:54.670 --> 00:16:57.400
this technique of feature
forcing that place--

00:16:57.400 --> 00:17:02.830
that replaces the input and the
output by the real sequence.

00:17:02.830 --> 00:17:04.790
So the point will
be the following.

00:17:04.790 --> 00:17:08.619
We are going to start
the model, and the model

00:17:08.619 --> 00:17:11.560
is going to output some
probability of an edge.

00:17:11.560 --> 00:17:14.710
But at the training time, we
are not going now to flip a coin

00:17:14.710 --> 00:17:16.810
and use that as an
input to the next cell.

00:17:16.810 --> 00:17:20.020
We're actually going to say, OK,
what was the true value there?

00:17:20.020 --> 00:17:22.630
Oh, the true value
was, there was no edge.

00:17:22.630 --> 00:17:28.640
So we are going to force this
as an input to the next step.

00:17:28.640 --> 00:17:31.930
So teacher is kind of,
forcing the student, whatever

00:17:31.930 --> 00:17:34.150
the student does
in the next step--

00:17:34.150 --> 00:17:36.220
teacher correct the
student and the student

00:17:36.220 --> 00:17:38.590
starts from the right--

00:17:38.590 --> 00:17:41.160
with the right input
for the next step.

00:17:41.160 --> 00:17:43.210
So the point, is
that, the inputs will

00:17:43.210 --> 00:17:46.210
be the correct sequence,
not the sequence generated

00:17:46.210 --> 00:17:51.110
by the model, and this is
called teacher forcing.

00:17:51.110 --> 00:17:54.320
Now, of course we need
to define the laws that

00:17:54.320 --> 00:17:57.110
measures the discrepancy
between the output of the model,

00:17:57.110 --> 00:17:59.900
the output of the student,
and the ground truth

00:17:59.900 --> 00:18:07.100
sequence that we try to
teach the model to generate.

00:18:07.100 --> 00:18:12.090
And we are going to use binary
cross entropy, loss function,

00:18:12.090 --> 00:18:13.980
which you can write
out the following.

00:18:13.980 --> 00:18:16.820
You can-- the star is the--

00:18:16.820 --> 00:18:19.430
why is a binary variable 0, 1.

00:18:19.430 --> 00:18:23.030
1 means edge exist, 0 doesn't--
means that does not exist.

00:18:23.030 --> 00:18:27.590
And Y without the star is
the probability of the edge.

00:18:27.590 --> 00:18:31.250
And basically, the idea is
that because Y star is either

00:18:31.250 --> 00:18:32.270
0 or 1--

00:18:32.270 --> 00:18:35.300
if only one of
these two terms is

00:18:35.300 --> 00:18:38.750
going to survive when we
actually implement it,

00:18:38.750 --> 00:18:41.480
because when the
Y star comes in.

00:18:41.480 --> 00:18:45.080
And basically, the idea is, if Y
star is 1, then we are really--

00:18:45.080 --> 00:18:48.860
the loss really boils
down to minus log Y1.

00:18:48.860 --> 00:18:51.440
So basically, in order for
us to minimize the loss,

00:18:51.440 --> 00:19:00.180
we want to make log of Y1 to
be as close to 0 as possible.

00:19:00.180 --> 00:19:04.160
Which means we want to make
Y as close to 1 as possible.

00:19:04.160 --> 00:19:08.040
And then, if we want to--
now, on the other hand

00:19:08.040 --> 00:19:10.250
if the edge is not
present, then we

00:19:10.250 --> 00:19:15.620
want to minimize minus
log of 1 minus Y1.

00:19:15.620 --> 00:19:18.350
And here we want
to make Y1 lower so

00:19:18.350 --> 00:19:21.290
that the entire expression
again is log of something

00:19:21.290 --> 00:19:24.200
close to 1, which
will be close to 0.

00:19:24.200 --> 00:19:30.320
So this way, Y1 is fitting
to the data samples Y star.

00:19:30.320 --> 00:19:34.040
And again, just to remind you
Y1 or this Y's are computed

00:19:34.040 --> 00:19:38.540
by RNN, and the loss is going
to adjust the RNN parameters,

00:19:38.540 --> 00:19:42.320
those matrices W, U, and
V, using back propagation

00:19:42.320 --> 00:19:43.880
to try to minimize the loss.

00:19:43.880 --> 00:19:47.090
Minimize the discrepancy
between the true sequence

00:19:47.090 --> 00:19:48.890
and the generated
probabilities .

00:19:48.890 --> 00:19:51.080
And basically, the point
is that the loss will say,

00:19:51.080 --> 00:19:53.540
wherever there is a 0,
generate a probability

00:19:53.540 --> 00:19:56.180
that's close to 0, and
wherever there is a 1,

00:19:56.180 --> 00:20:00.670
generate a probability
that is close to 1.

00:20:00.670 --> 00:20:01.420
OK.

00:20:01.420 --> 00:20:04.390
So we do this, let's
put things together.

00:20:04.390 --> 00:20:06.950
So do-- our plan
is the following,

00:20:06.950 --> 00:20:09.460
we want to have two RNNs.

00:20:09.460 --> 00:20:12.430
First, we want to
have an RNN that

00:20:12.430 --> 00:20:16.330
will add one node
at each step, and we

00:20:16.330 --> 00:20:22.450
will use the output of it to
initialize the edge level RNN.

00:20:22.450 --> 00:20:24.700
And then, the edge
level RNN is going

00:20:24.700 --> 00:20:29.450
to predict what other nodes--
what existing nodes does

00:20:29.450 --> 00:20:31.720
the new connect to?

00:20:31.720 --> 00:20:34.270
And then, we are going
to add another node,

00:20:34.270 --> 00:20:37.960
and we will use the last hidden
state of the edge level RNN

00:20:37.960 --> 00:20:41.980
to initialize the node
level RNN for one more step.

00:20:41.980 --> 00:20:43.450
And then, how is this--

00:20:43.450 --> 00:20:46.450
how are we going to
stop the generation--

00:20:46.450 --> 00:20:49.930
if the edge level RNN is going
to output the end of sequence

00:20:49.930 --> 00:20:53.620
at step 1, we know that no edges
are connected to the new node,

00:20:53.620 --> 00:20:55.570
and we are going to
stop the generation.

00:20:55.570 --> 00:20:59.540
So it's actually the edge level
RNN, and that, we have decided

00:20:59.540 --> 00:21:04.080
will determine whether we stop
generating the graph or not.

00:21:04.080 --> 00:21:06.760
So let me give you
now an example.

00:21:06.760 --> 00:21:08.920
So that you see how
all this fits together.

00:21:08.920 --> 00:21:13.170
So this is what is going to
happen under the training time.

00:21:13.170 --> 00:21:15.480
For, let's say, a
given training--

00:21:15.480 --> 00:21:17.430
observed training graph.

00:21:17.430 --> 00:21:23.100
We are starting with the start
of sequence and a hidden state.

00:21:23.100 --> 00:21:26.760
The node level RNN
will add the node,

00:21:26.760 --> 00:21:29.610
and then, the edge
level RNN will be asked,

00:21:29.610 --> 00:21:32.490
shall this node that
has just been added,

00:21:32.490 --> 00:21:34.960
shall it link to
the previous nodes?

00:21:34.960 --> 00:21:35.790
Yes or no.

00:21:35.790 --> 00:21:37.990
It will update the probability.

00:21:37.990 --> 00:21:43.450
And we are then going to flip
a coin that will determine--

00:21:43.450 --> 00:21:46.500
with this given bias that will
determine whether the edge is

00:21:46.500 --> 00:21:47.080
added or not.

00:21:47.080 --> 00:21:51.510
And then, and then, we'll take
this and use it as an input--

00:21:51.510 --> 00:21:54.150
as an initialization
back to the node level

00:21:54.150 --> 00:21:57.580
RNN who's now going to
add the second node,

00:21:57.580 --> 00:21:59.290
and this would be node number 3.

00:21:59.290 --> 00:22:01.210
And then, the edge
level RNN is going

00:22:01.210 --> 00:22:04.060
to tell us, will node
3 link to node 1,

00:22:04.060 --> 00:22:06.340
will node 3 link to node 2.

00:22:06.340 --> 00:22:08.350
And again, it's
outputting probabilities,

00:22:08.350 --> 00:22:10.300
we are flipping
the coins, whatever

00:22:10.300 --> 00:22:13.210
is the output of that coin is
the input to the next level

00:22:13.210 --> 00:22:13.840
RNN.

00:22:13.840 --> 00:22:16.480
So here are the
probabilities 0.6.

00:22:16.480 --> 00:22:18.910
Perhaps you were lucky,
the output was 1,

00:22:18.910 --> 00:22:22.850
so this is the input
for the next state.

00:22:22.850 --> 00:22:26.800
And then, after
we have traversed

00:22:26.800 --> 00:22:29.620
with all the previous
edges, we are going over

00:22:29.620 --> 00:22:31.210
all the previous
nodes, we are again

00:22:31.210 --> 00:22:34.540
going to ask node RNN
to generate a new node.

00:22:34.540 --> 00:22:36.830
And so on and so forth.

00:22:36.830 --> 00:22:41.980
And this is going to continue
right until the last node has

00:22:41.980 --> 00:22:45.160
been added, then the node
RNN will add one more node,

00:22:45.160 --> 00:22:47.320
but the edge level
RNN will say, I'm

00:22:47.320 --> 00:22:49.700
not willing to connect
it to anyone else.

00:22:49.700 --> 00:22:51.880
So this is the end of
sequence and we stop.

00:22:51.880 --> 00:22:54.890
So basically, it's like,
when we add an isolated node,

00:22:54.890 --> 00:22:58.780
we know that's the signal
that we want to stop.

00:22:58.780 --> 00:23:02.140
So that's the idea.

00:23:02.140 --> 00:23:05.260
And then, for each
prediction, here we

00:23:05.260 --> 00:23:07.420
are going to get
supervision from the ground,

00:23:07.420 --> 00:23:09.070
from the ground truth.

00:23:09.070 --> 00:23:11.560
So the point is that, we will
be doing teacher forcing.

00:23:11.560 --> 00:23:14.420
So, even for example,
here, when this was 0.6

00:23:14.420 --> 00:23:16.420
and we did a coin flip
and maybe we were unlucky

00:23:16.420 --> 00:23:22.780
and we got a zero, we are going
to output the true edge as we

00:23:22.780 --> 00:23:24.070
said, the teacher--

00:23:24.070 --> 00:23:25.360
the teacher forcing.

00:23:25.360 --> 00:23:27.520
And then, the structure
of our neural network

00:23:27.520 --> 00:23:29.920
when we do back propagation,
will be basically doing back

00:23:29.920 --> 00:23:32.500
propagation through time.

00:23:32.500 --> 00:23:36.400
We are going to basically back
prop from all these events all

00:23:36.400 --> 00:23:40.150
the way to the beginning--
to the beginning of time,

00:23:40.150 --> 00:23:44.450
to the firr-- to the first
node in the graph to update

00:23:44.450 --> 00:23:48.250
the parameters of the RNN.

00:23:48.250 --> 00:23:51.850
And then, how about at the test
time, at the generation time?

00:23:51.850 --> 00:23:54.640
We are basically going to
sample the connectivity based

00:23:54.640 --> 00:23:57.760
on the predicted distributions,
predicted probabilities,

00:23:57.760 --> 00:24:00.310
and we are going to replace
the input at each step

00:24:00.310 --> 00:24:04.090
by the RNNs own
prediction or own output.

00:24:04.090 --> 00:24:08.050
So here we are going
to flip the coin,

00:24:08.050 --> 00:24:11.203
the coin will say what it will
say, and it will go as an input

00:24:11.203 --> 00:24:11.870
to the next one.

00:24:11.870 --> 00:24:14.800
And I think, here, we are
going to do the coin, whatever

00:24:14.800 --> 00:24:18.030
is the output here, should be
the input to the next step,

00:24:18.030 --> 00:24:20.210
so it should be a 0
here, it's a mistake.

00:24:20.210 --> 00:24:20.710
OK.

00:24:20.710 --> 00:24:22.850
So that's the idea.

00:24:22.850 --> 00:24:27.640
So the summary is we have costed
the problem of graph generation

00:24:27.640 --> 00:24:29.380
as a problem of
sequence generation.

00:24:29.380 --> 00:24:32.710
Actually, a problem of a
two-level sequence generation,

00:24:32.710 --> 00:24:35.710
node level sequence, and
an edge level sequence,

00:24:35.710 --> 00:24:37.780
and we use recurrent
neural networks

00:24:37.780 --> 00:24:39.970
to generate the sequences.

00:24:39.970 --> 00:24:42.820
And what I want to
discuss next is, how do we

00:24:42.820 --> 00:24:45.040
make the RNN tractable?

00:24:45.040 --> 00:24:47.670
And how do we evaluate?


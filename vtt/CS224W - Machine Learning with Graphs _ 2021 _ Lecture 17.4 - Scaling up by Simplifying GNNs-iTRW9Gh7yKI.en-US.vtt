WEBVTT
Kind: captions
Language: en-US

00:00:04.130 --> 00:00:08.880
So, um, the third topic I wanna discuss is

00:00:08.880 --> 00:00:13.140
scaling up graph neural networks by simplifying their architecture.

00:00:13.140 --> 00:00:15.820
Um, and this is kind of an orthogonal approach,

00:00:15.820 --> 00:00:17.520
uh, to the first two approaches.

00:00:17.520 --> 00:00:19.770
And here is how we are going to do this.

00:00:19.770 --> 00:00:21.869
We will start from a graph convolutional,

00:00:21.869 --> 00:00:24.210
uh, uh, network, uh,

00:00:24.210 --> 00:00:26.100
just as an example architecture,

00:00:26.100 --> 00:00:28.335
and we are going to simplify it by removing

00:00:28.335 --> 00:00:31.410
the non-linear activation from the GCN, right?

00:00:31.410 --> 00:00:33.210
And actually, there's been a paper, uh,

00:00:33.210 --> 00:00:37.500
two years ago or a year and a half ago that demonstrates that

00:00:37.500 --> 00:00:42.660
the performance on- on benchmarks is not too much lower because you have,

00:00:42.660 --> 00:00:44.920
uh, removed the non-linearity.

00:00:44.920 --> 00:00:46.590
Uh, and this means that now,

00:00:46.590 --> 00:00:50.840
this simplified GCN architecture turns out to be an extremely scalable,

00:00:50.840 --> 00:00:53.225
uh, model that we can train very fast.

00:00:53.225 --> 00:00:57.050
So basically, the idea here is you are going to simplify

00:00:57.050 --> 00:01:01.445
the expressive power of the graph neural network so that you can train it faster.

00:01:01.445 --> 00:01:05.390
Of course, this kind of deb- defeats the purpose of deep-learning a bit

00:01:05.390 --> 00:01:10.250
because the point of deep learning or representation learning is a lot of data over,

00:01:10.250 --> 00:01:15.770
uh, over complex models that can model complex, um, uh, representations.

00:01:15.770 --> 00:01:19.910
Here, we are saying, simplify the model so it's fast to run on big data,

00:01:19.910 --> 00:01:22.015
and of course, there is a trade off there.

00:01:22.015 --> 00:01:25.230
So let me, uh, introduce the, uh,

00:01:25.230 --> 00:01:28.350
GCN, uh, and then we'll continue from there, right?

00:01:28.350 --> 00:01:30.360
So GCN takes in, uh, a graph,

00:01:30.360 --> 00:01:33.135
uh, with, uh, node features.

00:01:33.135 --> 00:01:36.155
And let's assume that every node includes a self-loop.

00:01:36.155 --> 00:01:38.840
Uh, this would be, uh, convenient for,

00:01:38.840 --> 00:01:40.715
uh, mathematical notation later.

00:01:40.715 --> 00:01:43.520
And let's think of this as a full batch implementation, right?

00:01:43.520 --> 00:01:45.950
So we basically set that the node embeddings

00:01:45.950 --> 00:01:48.685
at layer- at layer 0 to be simply node features,

00:01:48.685 --> 00:01:50.990
and then we are going to iterate for K,

00:01:50.990 --> 00:01:53.375
uh, K layers, uh, where at, uh,

00:01:53.375 --> 00:01:56.840
every- every node at layer k plus 1 is going

00:01:56.840 --> 00:02:00.890
to take the embeddings of its neighbors from the previous layers,

00:02:00.890 --> 00:02:03.350
sum them up, um,

00:02:03.350 --> 00:02:07.070
and divide by the number of layers- number of neighbors, uh,

00:02:07.070 --> 00:02:11.825
um, transform by this learned matrix and pass through a non-linearity.

00:02:11.825 --> 00:02:13.910
And then we're going to run this, uh,

00:02:13.910 --> 00:02:18.950
recursion for k iterations and whatever we end up in the end with,

00:02:18.950 --> 00:02:20.825
whatever embedding we end up at the end,

00:02:20.825 --> 00:02:24.040
that is what we call, uh, final embedding.

00:02:24.040 --> 00:02:28.070
Now, what is- benefit of GCN is that it is so simple.

00:02:28.070 --> 00:02:31.970
We can very nicely write it into the- in the matrix form.

00:02:31.970 --> 00:02:35.450
So the way we are going to write it in the matrix form is that we are going to take

00:02:35.450 --> 00:02:40.010
these embeddings and we are going to stack them into an embedding matrix.

00:02:40.010 --> 00:02:45.580
And then A is our adjacency matrix where every node also has a self-loop.

00:02:45.580 --> 00:02:51.900
Then the way you can write the sum over the neighbors of a given node- sum

00:02:51.900 --> 00:02:57.840
of the embeddings of a- of a given no- of the neighbors of a given node,

00:02:57.840 --> 00:03:02.395
you can simply write this as a- as a product between the adjacency matrix A

00:03:02.395 --> 00:03:07.585
and the- and the embedding matrix H. And, uh,

00:03:07.585 --> 00:03:09.445
what this means is that now,

00:03:09.445 --> 00:03:13.810
we can also define this notion of D to be

00:03:13.810 --> 00:03:18.640
a diagonal matrix where we- it is all of 0 only on the diagonal,

00:03:18.640 --> 00:03:21.685
we have the degree of every node.

00:03:21.685 --> 00:03:24.100
And then the inverse of D,

00:03:24.100 --> 00:03:26.320
D to the minus 1 is the inverse of

00:03:26.320 --> 00:03:30.460
the diagonal matrix which is just you put 1 over the degree,

00:03:30.460 --> 00:03:33.740
uh, on the- on the diagonal entry for every node.

00:03:33.740 --> 00:03:37.070
So now, the way you can write a summation over the neighbors

00:03:37.070 --> 00:03:41.500
divided by the degree of that node is simply, uh,

00:03:41.500 --> 00:03:43.870
the inverse of the diagonal matrix,

00:03:43.870 --> 00:03:50.360
so one over the degree times the adjacency matrix A times the hidden- uh, uh,

00:03:50.360 --> 00:03:53.030
embeddings matrix H. So this means that now,

00:03:53.030 --> 00:03:54.680
given H at level l,

00:03:54.680 --> 00:03:59.150
if we multiply it by A and multiply it by D to the minus 1,

00:03:59.150 --> 00:04:02.270
we get the matrix of node embeddings at level h plus 1.

00:04:02.270 --> 00:04:05.750
So basically, what is elegant here is that you can rewrite

00:04:05.750 --> 00:04:10.140
this iteration just as a product of three matrices,

00:04:10.140 --> 00:04:11.840
and of course, in the GCN,

00:04:11.840 --> 00:04:14.840
we also have a ReLU non-linearity,

00:04:14.840 --> 00:04:18.370
um, uh, and, uh, transformation here.

00:04:18.370 --> 00:04:21.150
So going back to the GCN,

00:04:21.150 --> 00:04:23.470
here is the node-based, uh, uh,

00:04:23.470 --> 00:04:25.080
formulation of the GCN,

00:04:25.080 --> 00:04:27.230
if you write it in the matrix form,

00:04:27.230 --> 00:04:28.670
you can write it that this is

00:04:28.670 --> 00:04:34.515
a non-linearity times this A tilde that is simply degree times, uh,

00:04:34.515 --> 00:04:40.065
A, um, times the previous layer embeddings times,

00:04:40.065 --> 00:04:43.880
uh, the W matrix which is the transformation matrix.

00:04:43.880 --> 00:04:48.195
So basically, the equation- this equation,

00:04:48.195 --> 00:04:53.029
uh, that is based on the network and the following matrix equation, they are equivalent.

00:04:53.029 --> 00:04:56.525
So if you've- if you've computed this product of these matrices,

00:04:56.525 --> 00:04:58.755
you have just computed that the k plus 1 layer

00:04:58.755 --> 00:05:01.100
embedding for all the nodes of the network, and now,

00:05:01.100 --> 00:05:06.290
you can kind of iterate this capital K times to compute the k layers.

00:05:06.290 --> 00:05:11.810
So, um, this is what a matrix formulation of our GCN looks like.

00:05:11.810 --> 00:05:14.840
So let's now go and simplify the GCN.

00:05:14.840 --> 00:05:20.010
Let's assume and go and remove this ReLU, uh, non-linearity.

00:05:20.010 --> 00:05:21.270
So let's- lets say,

00:05:21.270 --> 00:05:25.610
what would happen if the GCN would- would be governed by the following equation, right?

00:05:25.610 --> 00:05:30.180
So going back, this is the equation with a non-linearity, now,

00:05:30.180 --> 00:05:33.420
we decided- now we decided to drop the non-linearity,

00:05:33.420 --> 00:05:35.250
so here is our, you know,

00:05:35.250 --> 00:05:37.650
simplified the GCN equation.

00:05:37.650 --> 00:05:41.955
So now, let's go and unroll this- uh,

00:05:41.955 --> 00:05:45.720
this iteration, let's- let's unroll this recursion, right?

00:05:45.720 --> 00:05:50.000
So we say, ah, here is the final layer embeddings for the nodes,

00:05:50.000 --> 00:05:53.780
uh, they depend on the layer k minus 1 embeddings of the nodes.

00:05:53.780 --> 00:05:58.280
So let's now take this H_k minus 1 and insert it, uh,

00:05:58.280 --> 00:06:00.290
with the way we compute, uh,

00:06:00.290 --> 00:06:04.160
H_k to the minus- H to the layer k minus 1.

00:06:04.160 --> 00:06:07.375
And you know, I take this part and just I insert it here.

00:06:07.375 --> 00:06:11.100
And now, I notice this depends on H_k minus 2.

00:06:11.100 --> 00:06:15.640
So again, I can go take H_k minus 2 and again expand it.

00:06:15.640 --> 00:06:19.655
And if I do this all the way down to the 0 layer,

00:06:19.655 --> 00:06:22.310
then, um, I- I know what to do, right?

00:06:22.310 --> 00:06:26.165
Like H0 is simply the vector of,

00:06:26.165 --> 00:06:28.765
uh- of, uh, node features X.

00:06:28.765 --> 00:06:32.100
These A tildes just kind of get multiplied together.

00:06:32.100 --> 00:06:37.515
So this is A tilde raised to the power k. And this here at the end, these, uh,

00:06:37.515 --> 00:06:43.295
parameter matrices, it is just a product of parameter matrices that is still a matrix.

00:06:43.295 --> 00:06:45.750
So I can rewrite this, uh,

00:06:45.750 --> 00:06:49.490
recursive equa- equation if I expand it in the following way,

00:06:49.490 --> 00:06:52.880
and then I realized that a product of

00:06:52.880 --> 00:06:56.340
parameter matrices is just a parameter matrix, right?

00:06:56.340 --> 00:06:59.810
So basically, I have just rewritten the K layer

00:06:59.810 --> 00:07:03.830
GCN into this very simple equation that is non-recursive.

00:07:03.830 --> 00:07:06.575
It's A tilde raised to the power k

00:07:06.575 --> 00:07:10.410
times the feature vector times the transformation matrix.

00:07:10.410 --> 00:07:12.570
So what is important?

00:07:12.570 --> 00:07:13.785
What do you need to, uh,

00:07:13.785 --> 00:07:17.285
remember here about A tilde raised to the power k?

00:07:17.285 --> 00:07:19.760
Remember in I think Lecture 1 or 2,

00:07:19.760 --> 00:07:25.400
we talked about what does powering the adjacency matrix to the- to the kth power mean?

00:07:25.400 --> 00:07:27.620
It means we are counting paths,

00:07:27.620 --> 00:07:30.245
it means we are connecting nodes that are neighbors of-

00:07:30.245 --> 00:07:33.110
that are neighbors- neighbors of neighbors and so on.

00:07:33.110 --> 00:07:37.100
So basically, what this means is that this til- A tilde to the k really

00:07:37.100 --> 00:07:40.970
connects the target node to its neighbors, neighbors of neighbors,

00:07:40.970 --> 00:07:42.470
neighbors of neighbors of neighbors,

00:07:42.470 --> 00:07:43.910
and so on, um,

00:07:43.910 --> 00:07:48.285
one-hop farther out in the network as we increase,

00:07:48.285 --> 00:07:50.085
uh, uh, K, um,

00:07:50.085 --> 00:07:51.840
and that's very interesting.

00:07:51.840 --> 00:07:54.885
So now, why did- what can we conclude?

00:07:54.885 --> 00:08:00.210
We can conclude that removing the non-linearity significantly simplifies the GCN.

00:08:00.210 --> 00:08:02.955
Uh, notice also that, um,

00:08:02.955 --> 00:08:05.670
these A tilde to the k times x does not

00:08:05.670 --> 00:08:09.050
include- contain any learnable parameters so we can

00:08:09.050 --> 00:08:15.150
actually pre-compute this on the- on the CPU even before we start, uh, training, right?

00:08:15.150 --> 00:08:19.160
And this can be very efficiently computed because all I have to do is,

00:08:19.160 --> 00:08:22.275
um, multiply the mat- uh,

00:08:22.275 --> 00:08:24.390
A tilde with, uh,

00:08:24.390 --> 00:08:26.940
x, uh, kind of with itself, uh,

00:08:26.940 --> 00:08:31.980
multiple times, and I will be able to get to the A tilde raised to the power K times x.

00:08:31.980 --> 00:08:34.260
So computing this part is very easy,

00:08:34.260 --> 00:08:37.835
it does not depend on any, um,

00:08:37.835 --> 00:08:42.445
model parameters so I can just pre-compute it even before I start learning.

00:08:42.445 --> 00:08:45.615
So now, um, how about,

00:08:45.615 --> 00:08:48.740
uh- so what we have learned is that this, uh,

00:08:48.740 --> 00:08:54.295
A_k times x could be pre-computed and let's call this X tilde.

00:08:54.295 --> 00:08:56.930
So now, uh, the simplified GCN,

00:08:56.930 --> 00:09:01.310
the final embedding layer is simply this X tilde times the parameter matrix,

00:09:01.310 --> 00:09:05.650
and this is just a linear transformation of the pre-computed matrix, right?

00:09:05.650 --> 00:09:07.840
So the way I can- I can think of this,

00:09:07.840 --> 00:09:11.260
this is basically just a pre-computed feature vector for node,

00:09:11.260 --> 00:09:14.355
uh, v, uh, times its- uh,

00:09:14.355 --> 00:09:18.515
times the matrix- learned matrix W. So embedding of node,

00:09:18.515 --> 00:09:22.940
uh, v only depends on its own pre-processed, uh, features,

00:09:22.940 --> 00:09:26.155
right, where I had- I say this X tilde is really

00:09:26.155 --> 00:09:31.935
X tilde computed by A- A tilde raised to the power K times X, right?

00:09:31.935 --> 00:09:36.160
But this is a matrix that has one row for every node,

00:09:36.160 --> 00:09:39.260
so if I say what is the final layer embedding of a given node?

00:09:39.260 --> 00:09:42.150
Is simply the- the corresponding row in

00:09:42.150 --> 00:09:45.600
the matrix times the learned parameter matrix, uh,

00:09:45.600 --> 00:09:51.470
W. But what is important to notice here is that once this X tilde has been computed,

00:09:51.470 --> 00:09:53.805
then learn- then, uh, um,

00:09:53.805 --> 00:09:56.645
the embedding of a given node only depends on

00:09:56.645 --> 00:10:00.065
a given row of the X tilde that is fixed and constant.

00:10:00.065 --> 00:10:06.000
And, uh, the only thing that changes that is learnable is W. So what this means is

00:10:06.000 --> 00:10:12.220
that embe- embeddings of M nodes can be generated in linear ti- in the time linear,

00:10:12.220 --> 00:10:15.755
um, with M because for a given, uh, node,

00:10:15.755 --> 00:10:19.555
its final embedding only depends on its own,

00:10:19.555 --> 00:10:22.485
uh, row in the matrix, uh, X tilde.

00:10:22.485 --> 00:10:25.890
So I can easily sample a mini batch of nodes,

00:10:25.890 --> 00:10:30.490
I sample a set of rows from the matrix X,

00:10:30.490 --> 00:10:35.200
and then I multiply that with W to get the final layer embeddings of those,

00:10:35.200 --> 00:10:37.985
uh, nodes, uh, in the mini-batch.

00:10:37.985 --> 00:10:43.790
So, um, and of course this would be super fast because there is no dependencies between

00:10:43.790 --> 00:10:49.430
the nodes or all the dependencies are already captured in this matrix, uh, X tilde.

00:10:49.430 --> 00:10:54.334
So in summary, uh, Simplified GCN consists of two steps,

00:10:54.334 --> 00:10:57.035
the pre-processing step where- uh,

00:10:57.035 --> 00:11:05.510
where a pre- where we pre-compute this X tilde to be simply the adjacency matrix A,

00:11:05.510 --> 00:11:09.350
um, with, uh, one over the degree of the node on the diagonal.

00:11:09.350 --> 00:11:10.715
We call this A tilde,

00:11:10.715 --> 00:11:14.515
we raise this to the Kth power so we multiply it with itself,

00:11:14.515 --> 00:11:16.230
K step, K times,

00:11:16.230 --> 00:11:21.830
and then we multiply this with the original features of the nodes x.

00:11:21.830 --> 00:11:26.420
And all of this can be done on a CPU even before we start training.

00:11:26.420 --> 00:11:30.920
So now what this means is that we have this matrix, uh, uh, uh,

00:11:30.920 --> 00:11:35.930
X, uh, X tilde that has one row for every node, um,

00:11:35.930 --> 00:11:38.690
and that's all- all we need for

00:11:38.690 --> 00:11:43.070
the training step where basically for each mini-batch we are going to sample uh,

00:11:43.070 --> 00:11:45.065
M nodes at random,

00:11:45.065 --> 00:11:48.905
and then we simply compute their embeddings by taking,

00:11:48.905 --> 00:11:50.225
uh, W and, uh,

00:11:50.225 --> 00:11:52.400
multiplying it with uh, with uh,

00:11:52.400 --> 00:11:55.490
uh, with the corresponding, uh, uh,

00:11:55.490 --> 00:11:58.385
entry in the- in the matrix, uh,

00:11:58.385 --> 00:12:00.560
X tilde that corresponds to that,

00:12:00.560 --> 00:12:03.390
uh, node, uh, um, uh, uh,

00:12:03.390 --> 00:12:08.090
v. And we simply compute the final layer embeddings of all the nodes in the mini-batch,

00:12:08.090 --> 00:12:09.470
um, and then, you know,

00:12:09.470 --> 00:12:11.480
we can use these embeddings to make predictions,

00:12:11.480 --> 00:12:12.785
compute the loss, uh,

00:12:12.785 --> 00:12:16.640
and then update the matrix- the parameter matrix,

00:12:16.640 --> 00:12:19.700
uh, W. So, um,

00:12:19.700 --> 00:12:23.540
the- the good point here is that now the embedding of every node,

00:12:23.540 --> 00:12:27.185
uh, computation of it is independent from the other nodes.

00:12:27.185 --> 00:12:28.550
It is a simple, uh,

00:12:28.550 --> 00:12:31.310
matrix times vector product where

00:12:31.310 --> 00:12:34.760
the matrix W is what we are trying to learn and this can be done,

00:12:34.760 --> 00:12:36.515
uh, super, super fast.

00:12:36.515 --> 00:12:38.900
So, um, now let's,

00:12:38.900 --> 00:12:41.195
uh, uh, summarize and kind of, uh,

00:12:41.195 --> 00:12:44.090
compare, uh, uh, Cluster-GCN with,

00:12:44.090 --> 00:12:47.330
uh, uh, other methods we have learned, uh, today.

00:12:47.330 --> 00:12:52.070
The simplified GCN generates node embeddings much more, uh, efficiently.

00:12:52.070 --> 00:12:55.040
There is no need to create the giant computation graph,

00:12:55.040 --> 00:12:57.064
there is no need to do graph sampling,

00:12:57.064 --> 00:12:59.450
it is all very, very simple, right?

00:12:59.450 --> 00:13:05.240
You just do those matrix products and then the learning step is also very easy.

00:13:05.240 --> 00:13:09.080
So, um, it seems great, but what do we lose?

00:13:09.080 --> 00:13:12.200
Um, right, compared to Cluster-GCN,

00:13:12.200 --> 00:13:17.960
mini-batch of nodes in a simplified GCN can be sampled completely at random from,

00:13:17.960 --> 00:13:19.625
uh, the entire set of nodes.

00:13:19.625 --> 00:13:22.325
As I said, there is no need to do group- uh,

00:13:22.325 --> 00:13:24.260
to do node groups to do the,

00:13:24.260 --> 00:13:25.970
uh, like in Cluster-GCN,

00:13:25.970 --> 00:13:29.120
no need to do the induced subgraph, uh, nothing like this.

00:13:29.120 --> 00:13:30.860
So this me- this means that, uh,

00:13:30.860 --> 00:13:34.250
our- the training is very stable, um,

00:13:34.250 --> 00:13:36.589
and the- the variance of the gradients,

00:13:36.589 --> 00:13:38.615
uh, is much more under control.

00:13:38.615 --> 00:13:41.240
But, right, what is the price?

00:13:41.240 --> 00:13:43.670
The price is that this model is far,

00:13:43.670 --> 00:13:45.930
far, uh, less expressive.

00:13:45.930 --> 00:13:49.450
Meaning, compared to the original graph neural network models,

00:13:49.450 --> 00:13:51.790
simplified GCN is far less

00:13:51.790 --> 00:13:55.900
expressive because it lacked- it doesn't have the non-linearity,

00:13:55.900 --> 00:13:58.465
uh, in generating, uh, node embedding.

00:13:58.465 --> 00:14:01.330
So it means that the theory that we discussed

00:14:01.330 --> 00:14:07.155
about computation graphs, Weisfeiler-Lehman isomorphism test,

00:14:07.155 --> 00:14:10.775
keeping the structure of the underlying subgraphs, uh,

00:14:10.775 --> 00:14:12.650
through that injective mapping,

00:14:12.650 --> 00:14:17.390
all that is out of the window because we don't have the non-linearity anymore.

00:14:17.390 --> 00:14:22.595
So that really makes a huge difference in the expressive power, uh, of the model.

00:14:22.595 --> 00:14:24.740
Right? But, you know,

00:14:24.740 --> 00:14:26.990
in many real-world cases,

00:14:26.990 --> 00:14:29.150
a simplified GCN, uh,

00:14:29.150 --> 00:14:31.205
tends to work well, um,

00:14:31.205 --> 00:14:36.530
and tends to kind of work just slightly worse than the original graph neural networks,

00:14:36.530 --> 00:14:38.510
even though being far less, uh,

00:14:38.510 --> 00:14:40.850
expressive in, uh, theory.

00:14:40.850 --> 00:14:44.345
So the question is, uh, why is that?

00:14:44.345 --> 00:14:48.875
And the reason for that is something that is called graph homophily,

00:14:48.875 --> 00:14:50.540
which basically means, uh,

00:14:50.540 --> 00:14:53.585
this is a concept from social science, uh,

00:14:53.585 --> 00:14:56.420
generally people like to call it, uh,

00:14:56.420 --> 00:14:59.615
birds of feather, uh, sto- stick together.

00:14:59.615 --> 00:15:01.970
So or birds of feather flock together.

00:15:01.970 --> 00:15:07.040
So basically the idea is that similar people tends to connect with each other, right?

00:15:07.040 --> 00:15:09.005
So that basically, you know,

00:15:09.005 --> 00:15:11.285
computer scientists know each other,

00:15:11.285 --> 00:15:13.370
uh, people who study music know each other.

00:15:13.370 --> 00:15:14.810
So essentially, the idea is that you have

00:15:14.810 --> 00:15:18.740
these social communities that are tightly compact where people share

00:15:18.740 --> 00:15:21.545
properties and attributes just because it is easier

00:15:21.545 --> 00:15:24.740
to connect to people who have something in common with you,

00:15:24.740 --> 00:15:27.530
with whom you share some, uh, interest, right?

00:15:27.530 --> 00:15:29.090
So basically what this means in,

00:15:29.090 --> 00:15:32.825
let's say networks, both in social, biological,

00:15:32.825 --> 00:15:36.320
knowledge graphs is that nodes connected by edges tend to

00:15:36.320 --> 00:15:40.655
share the same labels they tend to have similar features.

00:15:40.655 --> 00:15:43.025
Right? In citation networks,

00:15:43.025 --> 00:15:45.935
papers from the same- same area tend to cite each other.

00:15:45.935 --> 00:15:49.490
In movie recommendation, it's like people are interested in

00:15:49.490 --> 00:15:53.480
given genres and you watch multiple movies from the same genre.

00:15:53.480 --> 00:15:56.330
So these movies are kind of similar to each other.

00:15:56.330 --> 00:15:58.370
So, um, and, you know,

00:15:58.370 --> 00:16:01.100
why- why is this important for a simplified GCN?

00:16:01.100 --> 00:16:05.560
Because, um, the t- the three- pre-processing step of

00:16:05.560 --> 00:16:11.830
a simplified GCN is simply feature aggregation over a k-hop, uh, neighborhood, right?

00:16:11.830 --> 00:16:15.160
So the pre-processing features obtained by- is- are

00:16:15.160 --> 00:16:18.640
obtained by iteratively averaging, um, um, uh,

00:16:18.640 --> 00:16:22.835
features of the neighbors and features of the neighbors of neighbors

00:16:22.835 --> 00:16:27.455
without learned transformation and will- without any non-linearity.

00:16:27.455 --> 00:16:28.790
So, as a result,

00:16:28.790 --> 00:16:32.480
nodes connected by edges tend to have similar pre-processing,

00:16:32.480 --> 00:16:35.390
uh, features, and now with labels, um,

00:16:35.390 --> 00:16:41.060
are also clustered kind of across the homophilis parts of the network,

00:16:41.060 --> 00:16:44.210
um, if the labels kind of cluster a- across the network,

00:16:44.210 --> 00:16:47.660
then simplified GCN will work, uh, really well.

00:16:47.660 --> 00:16:52.610
So, uh, basically, the- the- when does the simplified GCN work?

00:16:52.610 --> 00:16:56.810
The premise is that the model uses the pre-process node features to make prediction.

00:16:56.810 --> 00:16:59.900
Nodes connected by edges tend to get

00:16:59.900 --> 00:17:02.450
similar pre-processed features because it's all

00:17:02.450 --> 00:17:05.540
about feature averaging across local neighborhoods.

00:17:05.540 --> 00:17:09.560
So if nodes connected by edges tend to be in the same class,

00:17:09.560 --> 00:17:13.025
tend to have the same label, then simplified GCN,

00:17:13.025 --> 00:17:14.540
uh, is going to, uh,

00:17:14.540 --> 00:17:16.520
make very accurate, uh,

00:17:16.520 --> 00:17:21.020
predictions, uh, so basically if the graph has this kind of homophily structure.

00:17:21.020 --> 00:17:23.945
Now, if the graph doesn't have this homophily structure,

00:17:23.945 --> 00:17:26.855
then the simplified GCN is going to fail,

00:17:26.855 --> 00:17:28.910
uh, quite, uh, quite bad.

00:17:28.910 --> 00:17:30.590
So that's kind of the intuition,

00:17:30.590 --> 00:17:32.000
and of course, ahead of time,

00:17:32.000 --> 00:17:35.690
we generally don't know whether labels are clustered together

00:17:35.690 --> 00:17:39.905
or they're kind of the- kind of sparkled, uh, across the network.

00:17:39.905 --> 00:17:46.309
So to summarize, simplified GCN removes non-linearity in GCN and then reduces,

00:17:46.309 --> 00:17:48.620
uh, the- uh, to simple, uh,

00:17:48.620 --> 00:17:52.205
pre-processing of the node features and graph adjacency matrix.

00:17:52.205 --> 00:17:57.199
When these pre-processed features are obtained on the CPU, a very scalable,

00:17:57.199 --> 00:18:00.860
simple mini-batch- batch gra- stochastic gradient descent

00:18:00.860 --> 00:18:04.595
can be directly applied to optimize the parameters.

00:18:04.595 --> 00:18:10.370
Um, simplified GCN works surprisingly well in- in many benchmarks.

00:18:10.370 --> 00:18:14.315
Uh, the reason for that being is that those benchmarks are easy.

00:18:14.315 --> 00:18:18.230
Um, uh, nodes of similar label tend to link to each other.

00:18:18.230 --> 00:18:19.415
They tend to be part,

00:18:19.415 --> 00:18:20.975
uh, of the same network,

00:18:20.975 --> 00:18:25.385
and meaning that just simple averaging of their features, um, uh,

00:18:25.385 --> 00:18:30.290
without any nonlinearities and without any weight or different weighing of them,

00:18:30.290 --> 00:18:31.700
um, gives you, uh,

00:18:31.700 --> 00:18:34.800
good performance, uh, in practice.


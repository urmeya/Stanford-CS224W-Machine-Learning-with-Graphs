WEBVTT
Kind: captions
Language: en-US

00:00:04.100 --> 00:00:07.860
Thank you for the opportunity and I'm excited to talk about

00:00:07.860 --> 00:00:11.950
my work on pre-training graph neural networks.

00:00:13.280 --> 00:00:17.610
Okay, so, um, here I, uh,

00:00:17.610 --> 00:00:19.245
I want to talk about applying

00:00:19.245 --> 00:00:23.340
graph neural networks to application in scientific domains.

00:00:23.340 --> 00:00:25.710
Uh, for example, uh, in chemistry,

00:00:25.710 --> 00:00:28.230
we have this molecular graphs where each node is

00:00:28.230 --> 00:00:31.800
an atom and each edge represents the chemical bond.

00:00:31.800 --> 00:00:36.645
And here we are interested in predicting the properties of molecules.

00:00:36.645 --> 00:00:39.075
For example, given this kind of molecular graph,

00:00:39.075 --> 00:00:40.350
we want to predict uh,

00:00:40.350 --> 00:00:42.225
whether it's toxic or not.

00:00:42.225 --> 00:00:44.630
In biology, there's also a graph problem.

00:00:44.630 --> 00:00:46.325
For example, uh, we have a

00:00:46.325 --> 00:00:51.380
protein-protein association graph where each node is a protein and the edge represents

00:00:51.380 --> 00:00:55.130
certain association between proteins and the associations are

00:00:55.130 --> 00:00:59.180
very important to determine the functionality of the protein.

00:00:59.180 --> 00:01:02.105
And our goal is to given this, uh,

00:01:02.105 --> 00:01:04.985
subgraph centered around the protein node,

00:01:04.985 --> 00:01:06.590
we want to predict whether

00:01:06.590 --> 00:01:10.835
the center protein node has certain biological activity or not.

00:01:10.835 --> 00:01:15.065
So what I want to say is that in bio- in many scientific domains, this, uh,

00:01:15.065 --> 00:01:21.210
graph problem appears a lot and we want to apply GNNs to solve this problem.

00:01:21.470 --> 00:01:27.995
And just to review how GNNs can be used for graph classification.

00:01:27.995 --> 00:01:32.455
So GNNs obtain an embedding of the entire graph by following two steps.

00:01:32.455 --> 00:01:34.340
First, um, given this,

00:01:34.340 --> 00:01:37.174
the molecular graph, let's say graph, let's say molecule,

00:01:37.174 --> 00:01:42.844
we obtain the embedding of node by either the aggregate neighboring information.

00:01:42.844 --> 00:01:44.450
Once we do that,

00:01:44.450 --> 00:01:46.520
we- once we obtain this node embedding,

00:01:46.520 --> 00:01:52.430
we can pool this node embedding globally to obtain the embedding of the entire graph.

00:01:52.430 --> 00:01:55.680
And once you obtain this embedding of the entire graph, we can use it,

00:01:55.680 --> 00:02:01.205
or we can apply a linear function on top of it to predict whether this entire graph is,

00:02:01.205 --> 00:02:03.600
let's say, toxic or not.

00:02:03.710 --> 00:02:08.350
And just to, [NOISE] uh, say what this means is that, uh,

00:02:08.350 --> 00:02:11.150
here the node embedding is trying to capture

00:02:11.150 --> 00:02:14.600
the local neighborhood structure, uh, around each node.

00:02:14.600 --> 00:02:17.120
If we apply k-hop GNNs,

00:02:17.120 --> 00:02:20.980
we will just know that it should capture k-hop local neighborhood.

00:02:20.980 --> 00:02:25.010
And then the embedding of the entire graph can, uh,

00:02:25.010 --> 00:02:28.025
is basically aggregating this local structure though,

00:02:28.025 --> 00:02:33.000
the node- node embedding that is capturing the local neighborhood structure.

00:02:33.320 --> 00:02:35.595
And, uh, and, uh,

00:02:35.595 --> 00:02:37.200
this is how GNN works,

00:02:37.200 --> 00:02:42.105
and now I am going to talk about challenges of how- applying machine learning to,

00:02:42.105 --> 00:02:43.805
uh, the scientific domains.

00:02:43.805 --> 00:02:46.385
And there are basically two fundamental challenges.

00:02:46.385 --> 00:02:49.190
The first challenge of applying machine learning to

00:02:49.190 --> 00:02:53.195
the scientific domains is that all the scarcity of labeled data.

00:02:53.195 --> 00:02:57.895
Obtaining labels in these scientific domains requires expensive lab experiments.

00:02:57.895 --> 00:03:01.850
For example, to, or say whether a molecule is toxic or not,

00:03:01.850 --> 00:03:04.415
you need to perform wet lab experiments,

00:03:04.415 --> 00:03:05.960
and these are very expensive.

00:03:05.960 --> 00:03:08.330
As a result, we cannot get that,

00:03:08.330 --> 00:03:13.375
a lot of training data and machinery models tend to overfit to this small training data.

00:03:13.375 --> 00:03:18.220
Another important issue is that out-of-distribution prediction, meaning that,

00:03:18.220 --> 00:03:20.675
test examples we want to make a prediction on

00:03:20.675 --> 00:03:23.365
tend to be very different from training examples.

00:03:23.365 --> 00:03:27.770
And this is really the nature of scientific discovery because in scientific discovery,

00:03:27.770 --> 00:03:30.710
we want- we want to discover some new molecule that is

00:03:30.710 --> 00:03:34.460
inherently very different from- from your training molecules.

00:03:34.460 --> 00:03:36.080
And in these domains,

00:03:36.080 --> 00:03:39.930
machine learning models are extrapolate poorly.

00:03:40.900 --> 00:03:46.670
And especially these two challenges are becoming more challenging for deep learning.

00:03:46.670 --> 00:03:50.870
The first, uh, the first point of the label scarcity,

00:03:50.870 --> 00:03:53.870
deep learning models have a lot of parameters to train,

00:03:53.870 --> 00:03:56.329
uh, typically in the order of millions.

00:03:56.329 --> 00:03:58.265
And uh- and in this regime,

00:03:58.265 --> 00:04:02.030
the number of training data is much less than the number of parameters.

00:04:02.030 --> 00:04:04.220
And deep-learning models are extremely prone to

00:04:04.220 --> 00:04:07.490
overfitting on small data- small labeled data.

00:04:07.490 --> 00:04:12.605
Another issue is that deep learning models are known to extrapolate poorly.

00:04:12.605 --> 00:04:16.235
And that's reported that models often make, uh,

00:04:16.235 --> 00:04:19.880
predictions based on spurious correlation in a dataset without

00:04:19.880 --> 00:04:24.475
understanding the true causal mechanism of how to make prediction.

00:04:24.475 --> 00:04:27.830
So let's consider this a toy example of

00:04:27.830 --> 00:04:31.340
a- a image classification between polar bear and brown bear,

00:04:31.340 --> 00:04:33.505
uh, this image is shown here.

00:04:33.505 --> 00:04:36.120
So during training, let's say, uh,

00:04:36.120 --> 00:04:38.675
our training data for- in our training data,

00:04:38.675 --> 00:04:42.020
most polar bears have snow background like this,

00:04:42.020 --> 00:04:45.895
and most brown bears have a grass background like this.

00:04:45.895 --> 00:04:49.370
And as a result, the model can learn to predict,

00:04:49.370 --> 00:04:51.680
make prediction of whether it's, uh,

00:04:51.680 --> 00:04:57.350
the given image is polar bear or brown bear based on the image background rather than

00:04:57.350 --> 00:05:00.380
animal itself because that's sufficient

00:05:00.380 --> 00:05:04.025
to make predictions over the- over the training dataset.

00:05:04.025 --> 00:05:06.050
But what if at the time same,

00:05:06.050 --> 00:05:08.150
if we see polar bear on the grass,

00:05:08.150 --> 00:05:09.635
then the model will, uh,

00:05:09.635 --> 00:05:15.020
because the model is not understanding the- this prediction task,

00:05:15.020 --> 00:05:18.110
the model will perform poorly on the- on

00:05:18.110 --> 00:05:21.410
the data or test data that is different from training data.

00:05:21.410 --> 00:05:23.810
And the model is, uh, just capturing this kind of

00:05:23.810 --> 00:05:28.200
spurious, spurious correlation in the training dataset.

00:05:28.670 --> 00:05:33.465
And our key idea or the goal, uh,

00:05:33.465 --> 00:05:35.880
given these challenges is that we want to improve

00:05:35.880 --> 00:05:39.950
model's out-of-distribution performance even with limited data.

00:05:39.950 --> 00:05:44.345
And the way we do this is to inject domain knowledge

00:05:44.345 --> 00:05:49.315
into a model before we apply them on scarcely-labeled tasks.

00:05:49.315 --> 00:05:52.860
And that, this work- this may work because

00:05:52.860 --> 00:05:56.990
the model already knows the domain knowledge before the model is

00:05:56.990 --> 00:06:01.010
training on- on our downstream data so that the model can generalize

00:06:01.010 --> 00:06:05.190
well without many task-specific labeled data and uh,

00:06:05.190 --> 00:06:08.765
and uh, the model may be able to extract essential,

00:06:08.765 --> 00:06:12.520
non-spurious, uh, pattern in the data, uh,

00:06:12.520 --> 00:06:16.475
which allows the model to better extrapolate to the-

00:06:16.475 --> 00:06:21.880
to the test data that still- that still very different from training data.

00:06:21.880 --> 00:06:25.220
And a very effective solution to inject

00:06:25.220 --> 00:06:28.160
domain knowledge into model is called pre-training.

00:06:28.160 --> 00:06:32.360
And we pre-train a model on relevant tasks that's different

00:06:32.360 --> 00:06:36.755
from a downstream task where data is abundant.

00:06:36.755 --> 00:06:39.320
And after we pre-train the model,

00:06:39.320 --> 00:06:41.870
the model's parameter, uh,

00:06:41.870 --> 00:06:47.330
already contains some domain knowledge and once this is done,

00:06:47.330 --> 00:06:51.980
we can transfer this pre-trained model parameter to the downstream task,

00:06:51.980 --> 00:06:55.990
which is what we care about and which where we have small number of data.

00:06:55.990 --> 00:07:00.555
And we can start from the pre-trained parameter and fine-tune the-

00:07:00.555 --> 00:07:06.000
the parameters on the downstream tasks.

00:07:06.000 --> 00:07:13.430
Um, and just to mention that pre-training has been hugely successful in- in the domain of

00:07:13.430 --> 00:07:16.370
computer vision and natural language processing and it's

00:07:16.370 --> 00:07:20.420
reported that pre-training improves label efficiency.

00:07:20.420 --> 00:07:27.320
Also, pre-training improves out-of-distribution performance.

00:07:27.320 --> 00:07:33.350
And because of this and within

00:07:33.350 --> 00:07:38.780
pre-training can be a very powerful solution to the scientific applications.

00:07:38.780 --> 00:07:43.250
And those two challenges are scarce labels and out-of-distribution prediction.

00:07:43.250 --> 00:07:45.950
So now we motivated

00:07:45.950 --> 00:07:52.115
this pre-training GNNs to solve an important problem in scientific applications.

00:07:52.115 --> 00:07:55.595
So let's consider, you know, actually pre-trained GNNs.

00:07:55.595 --> 00:07:59.570
And our work is about designing

00:07:59.570 --> 00:08:02.120
GNN's pre-training strategies and we want to

00:08:02.120 --> 00:08:06.439
systematically embedded- investigate the following two questions.

00:08:06.439 --> 00:08:09.005
How effective is pre-training GNNs,

00:08:09.005 --> 00:08:12.605
and what is the effective pre-training strategies.

00:08:12.605 --> 00:08:15.350
So as a running example,

00:08:15.350 --> 00:08:17.960
let's think about molecular property prediction task,

00:08:17.960 --> 00:08:20.135
prediction for drug discovery.

00:08:20.135 --> 00:08:25.315
For a given molecule, we want to predict its toxicity or biological activity.

00:08:25.315 --> 00:08:31.700
And the very naive strategy is multi-task supervised pre-training on relevant labels.

00:08:31.700 --> 00:08:34.010
This means that in the, for example,

00:08:34.010 --> 00:08:36.680
chemical database we have, uh, all the,

00:08:36.680 --> 00:08:39.560
uh, experimental measurements of

00:08:39.560 --> 00:08:44.450
tox- various toxicity and biological activities of a lot of molecules.

00:08:44.450 --> 00:08:48.380
And we can first pre-train GNNs to predict those,

00:08:48.380 --> 00:08:52.595
uh, very diverse biological activity of toxicity.

00:08:52.595 --> 00:08:56.570
And then we expect GNNs parameter to capture

00:08:56.570 --> 00:08:59.510
some chemistry domain knowledge which can be tran-

00:08:59.510 --> 00:09:04.710
and then we can transfer that parameter to our downstream task.

00:09:05.600 --> 00:09:09.505
And the setting that we consider is, uh, to,

00:09:09.505 --> 00:09:13.279
to study whether this naive strategy is effective,

00:09:13.279 --> 00:09:15.335
we consider this setting.

00:09:15.335 --> 00:09:18.350
We consider this binary classification of molecules.

00:09:18.350 --> 00:09:22.330
Given molecule, we want to judge whether it's negative or positive.

00:09:22.330 --> 00:09:26.600
And- and we, for the supervised pre-training part, uh,

00:09:26.600 --> 00:09:28.910
we consider- we consider predicting

00:09:28.910 --> 00:09:38.685
more than 1000 diverse binary bioassays annotated over 450,000 molecules.

00:09:38.685 --> 00:09:39.810
So there are a lot of data,

00:09:39.810 --> 00:09:42.065
uh, in this pre-training stage.

00:09:42.065 --> 00:09:46.775
And then we apply transfer the parameter to or downstream task,

00:09:46.775 --> 00:09:49.625
which is eight molecular classification datasets,

00:09:49.625 --> 00:09:51.530
those are relatively small, uh,

00:09:51.530 --> 00:09:54.065
about 1,000 to 100,000 molecules.

00:09:54.065 --> 00:09:56.590
And- and for the data split,

00:09:56.590 --> 00:09:58.730
uh, for the downstream task,

00:09:58.730 --> 00:10:00.755
we consider the scaffold split,

00:10:00.755 --> 00:10:04.860
which makes the test molecules out of distribution.

00:10:04.900 --> 00:10:09.350
And it turns out that the Naive strategy of

00:10:09.350 --> 00:10:14.360
this multi-task supervised pre-training on relevant labels doesn't work very well.

00:10:14.360 --> 00:10:20.325
It gives a limited performance improvement on downstream tasks and even, uh,

00:10:20.325 --> 00:10:22.455
leads to negative transfer,

00:10:22.455 --> 00:10:28.135
which means that the pre-trained model performs worse than randomly initialized model.

00:10:28.135 --> 00:10:30.275
So here's the, uh, table,

00:10:30.275 --> 00:10:31.430
and here is a figure.

00:10:31.430 --> 00:10:33.755
So we have our eight, uh,

00:10:33.755 --> 00:10:37.035
downstream datasets and -and

00:10:37.035 --> 00:10:42.605
the y-axis is the ROCAUC improvement over no pre-training baseline.

00:10:42.605 --> 00:10:45.355
So this purple is a no pre-train baseline.

00:10:45.355 --> 00:10:49.425
And we see that Naive strategy sometimes work well,

00:10:49.425 --> 00:10:52.475
but for these two datasets,

00:10:52.475 --> 00:10:57.730
it's actually performing worse than non pre-trained randomly initialized baseline.

00:10:57.730 --> 00:11:01.545
So this is kind of not desirable,

00:11:01.545 --> 00:11:04.710
we want pre-trained model to perform better.

00:11:06.330 --> 00:11:11.170
So how- what is the- then we move on to our next question,

00:11:11.170 --> 00:11:13.510
what is the effective pre-training strategy?

00:11:13.510 --> 00:11:18.340
And our key idea is to pre-train both node and graph

00:11:18.340 --> 00:11:24.310
embeddings so that GNNs can capture domain-specific knowledge about the graph,

00:11:24.310 --> 00:11:25.840
uh, at the both, uh,

00:11:25.840 --> 00:11:27.595
local and global level.

00:11:27.595 --> 00:11:30.655
So- so in the naive strategy,

00:11:30.655 --> 00:11:33.325
we pre-train GNNs, uh,

00:11:33.325 --> 00:11:35.875
on the- at the level of graph, use this, uh,

00:11:35.875 --> 00:11:39.100
graph embedding to make a various prediction.

00:11:39.100 --> 00:11:40.720
But we think, uh,

00:11:40.720 --> 00:11:43.525
it's important to also pre-training, uh, this, uh,

00:11:43.525 --> 00:11:46.165
node embedding because these node embeddings are

00:11:46.165 --> 00:11:49.375
aggregated to generate the embedding of the entire graph.

00:11:49.375 --> 00:11:52.465
So we- we need to have a high-quality node embedding

00:11:52.465 --> 00:11:56.380
that captures  the local neighborhood structure well.

00:11:56.380 --> 00:12:04.870
So and the- the intuition behind this is that- so here's the figure,

00:12:04.870 --> 00:12:08.005
the- the upper side means the node embedding

00:12:08.005 --> 00:12:11.515
and these node embedding are proved to generate the embedding of the graph,

00:12:11.515 --> 00:12:13.020
and we want the, uh,

00:12:13.020 --> 00:12:18.090
the quality of the node embedding and graph embedding to be both high quality,

00:12:18.090 --> 00:12:23.050
capture the semantics in the- in the- this is,

00:12:23.050 --> 00:12:27.460
uh, different from the naive pre-training strategy where, for example,

00:12:27.460 --> 00:12:30.580
if we- on the node-level pre-training,

00:12:30.580 --> 00:12:33.175
we capture the semantic at the level of nodes,

00:12:33.175 --> 00:12:37.060
but if those node embeddings they are globally aggregated,

00:12:37.060 --> 00:12:39.550
we may not be able to obtain nice, uh,

00:12:39.550 --> 00:12:43.330
graph embedding, um, or if we only pre-training,

00:12:43.330 --> 00:12:45.430
uh, GNNs at the level of graph,

00:12:45.430 --> 00:12:48.220
we can obtain nice graph embedding,

00:12:48.220 --> 00:12:52.900
but the- there's no guarantee that the- before aggregation,

00:12:52.900 --> 00:12:55.299
those node embeddings are of high-quality,

00:12:55.299 --> 00:12:59.170
and this may lead to a negative transfer,

00:12:59.170 --> 00:13:03.350
um, because the node embeddings are kind of not robust.

00:13:04.140 --> 00:13:08.335
So- so in order to realize this, uh,

00:13:08.335 --> 00:13:12.340
strategy to pre-train GNNs at the level of both nodes and graphs,

00:13:12.340 --> 00:13:15.790
we- we designed three concrete methods.

00:13:15.790 --> 00:13:19.090
Uh, two self-supervised method, uh,

00:13:19.090 --> 00:13:23.155
meaning there is no need for external labels for node-level pre-training,

00:13:23.155 --> 00:13:29.110
and one graph-level of prediction of pre-training strategy,

00:13:29.110 --> 00:13:31.660
pre-training method, um, that's,

00:13:31.660 --> 00:13:33.595
uh, that's, uh, supervised.

00:13:33.595 --> 00:13:37.060
So let me go through these three concrete methods.

00:13:37.060 --> 00:13:38.920
[NOISE] Okay.

00:13:38.920 --> 00:13:41.500
So the first method is the attribute masking.

00:13:41.500 --> 00:13:42.970
It's a node-level pre-training method.

00:13:42.970 --> 00:13:46.585
Um, the algorithm is quite simple.

00:13:46.585 --> 00:13:49.930
So given that input graph, we first mask,

00:13:49.930 --> 00:13:53.770
uh, uh, rand- randomly mask our node attributes.

00:13:53.770 --> 00:13:56.470
So let's say we mask these two node attributes.

00:13:56.470 --> 00:13:58.780
In the molecular graphs, this node that we are

00:13:58.780 --> 00:14:01.315
basically masking the identity of the atom.

00:14:01.315 --> 00:14:05.140
Um, and then we can use GNNs to generate, uh,

00:14:05.140 --> 00:14:07.930
node embeddings of these two mask nodes,

00:14:07.930 --> 00:14:13.600
and we use these node embeddings to predict the identity of the mask attributes.

00:14:13.600 --> 00:14:17.605
So here we want to use the node embedding to predict the- that

00:14:17.605 --> 00:14:22.495
this X is the- is the oxygen and this X is the carbon.

00:14:22.495 --> 00:14:25.750
And the intuition behind this is that

00:14:25.750 --> 00:14:28.780
the- through solving the mask attribute prediction task,

00:14:28.780 --> 00:14:31.210
our GNN is forced to learn

00:14:31.210 --> 00:14:36.160
the domain knowledge because it kind of needs to solve this quiz, and, uh, and, uh,

00:14:36.160 --> 00:14:38.395
and- and through this,

00:14:38.395 --> 00:14:40.720
solving this kind of quiz, GNN, uh,

00:14:40.720 --> 00:14:47.120
can learn the- the parameter of the GNN can capture the- the chemistry domain.

00:14:47.700 --> 00:14:53.590
The next self-supervised task that we propose is called the context prediction.

00:14:53.590 --> 00:14:58.225
And, uh, and here the idea- the algorithm is as follows.

00:14:58.225 --> 00:14:59.920
So for each graph,

00:14:59.920 --> 00:15:01.945
uh, we sample one center node.

00:15:01.945 --> 00:15:03.385
Let's say this is a,

00:15:03.385 --> 00:15:06.265
um, um, red nodes,

00:15:06.265 --> 00:15:08.410
and we, for this, uh,

00:15:08.410 --> 00:15:11.440
center node, we extract neighborhood and context graph.

00:15:11.440 --> 00:15:17.320
So neighborhood graph is just the k-hop neighborhood graph, uh, in this case,

00:15:17.320 --> 00:15:19.570
two-hop neighbor graph like this,

00:15:19.570 --> 00:15:21.760
and then the context graph is the- is

00:15:21.760 --> 00:15:25.390
the surrounding graph that's directly attached to this,

00:15:25.390 --> 00:15:27.200
uh, k-hop neighborhood graph.

00:15:27.200 --> 00:15:29.940
Once we do extract this graph,

00:15:29.940 --> 00:15:37.020
we can use two separate GNNs to encode this neighborhood graph into, you know, uh,

00:15:37.020 --> 00:15:41.670
one- use one GNN to encode this neighborhood graph into a vector,

00:15:41.670 --> 00:15:45.440
another GNN is to encode this context graph into a vector.

00:15:45.440 --> 00:15:49.045
And our objective is to maximize, uh,

00:15:49.045 --> 00:15:53.740
or maximize the inner product between the true neighborhood context pair

00:15:53.740 --> 00:15:59.274
while minimizing the inner product between the false or negative,

00:15:59.274 --> 00:16:03.490
uh, uh, false on neighborhood context pair,

00:16:03.490 --> 00:16:06.865
and these false pairs can be obtained by, uh,

00:16:06.865 --> 00:16:11.305
randomly sample context graph from other neighborhoods.

00:16:11.305 --> 00:16:15.790
And here the intuition is that we're using- we are assuming that

00:16:15.790 --> 00:16:21.685
subgraphs that are surrounded by similar contexts are semantically similar.

00:16:21.685 --> 00:16:24.220
So we are basically pushing, uh,

00:16:24.220 --> 00:16:27.730
the embedding of the neighborhood to be

00:16:27.730 --> 00:16:31.285
similar if they are- have similar context graphs.

00:16:31.285 --> 00:16:35.530
And this is the kind of widely used, uh,

00:16:35.530 --> 00:16:38.590
assumption in natural language processing,

00:16:38.590 --> 00:16:40.690
it's called distributional hypothesis.

00:16:40.690 --> 00:16:44.800
So words that are appearing with similar- similar contexts have similar meaning,

00:16:44.800 --> 00:16:48.115
and it's exploited in the famous, uh, Word2vec model.

00:16:48.115 --> 00:16:53.050
Um, so finally, um,

00:16:53.050 --> 00:16:54.730
I'm going to talk about this, uh,

00:16:54.730 --> 00:16:59.710
graph-level pre-training task, which is essentially what I introduced before.

00:16:59.710 --> 00:17:05.755
So multi-task supervised pre-training on many relevant graph level- labels.

00:17:05.755 --> 00:17:08.455
And this is really the direct approach, uh,

00:17:08.455 --> 00:17:11.290
to inject, uh, domain knowledge into the model.

00:17:11.290 --> 00:17:18.070
[NOISE] So to summarize the overall strategy,

00:17:18.070 --> 00:17:21.880
we perform- we first perform node-level pre-training,

00:17:21.880 --> 00:17:25.945
uh, to- to obtain good node representation.

00:17:25.945 --> 00:17:27.595
We- we use that, uh,

00:17:27.595 --> 00:17:31.780
pre-training parameter, uh, to- and then, uh,

00:17:31.780 --> 00:17:34.960
to- to further perform pre-training, uh,

00:17:34.960 --> 00:17:38.605
at the level of graph using supervised graph-level pre-training.

00:17:38.605 --> 00:17:40.765
Once- once we've done,

00:17:40.765 --> 00:17:43.030
uh, this, both steps,

00:17:43.030 --> 00:17:49.345
we fine-tune, uh, the parameters on the downstream task that we care about.

00:17:49.345 --> 00:17:52.030
So this is the overall strategy.

00:17:52.030 --> 00:17:56.515
And it turns out that our strategy works out pretty well.

00:17:56.515 --> 00:17:58.045
Uh, as you can see, these, uh,

00:17:58.045 --> 00:18:00.189
green dots are our strategy,

00:18:00.189 --> 00:18:05.365
and our strategy first avoids the negative transfer in these, uh, two datasets,

00:18:05.365 --> 00:18:10.825
and also consistently perform better than- than- than this,

00:18:10.825 --> 00:18:14.860
uh, orange naive strategy baseline across the datasets.

00:18:14.860 --> 00:18:19.270
Uh, and also, um,

00:18:19.270 --> 00:18:22.135
another interesting side note is that, uh,

00:18:22.135 --> 00:18:27.475
we fix the pre-training strategy and the pre-training different GNNs models,

00:18:27.475 --> 00:18:30.745
uh, use different GNNs models for pre-training.

00:18:30.745 --> 00:18:34.930
And what we found out is that the most expressive GNN model,

00:18:34.930 --> 00:18:37.690
namely GIN, that we've learned in the lecture,

00:18:37.690 --> 00:18:41.620
um, um, benefits most from pre-training.

00:18:41.620 --> 00:18:43.210
Uh, as you can see here,

00:18:43.210 --> 00:18:48.970
the gain of pre-trained model versus non-pre-trained model is the- is the largest,

00:18:48.970 --> 00:18:51.220
um, in terms of accuracy.

00:18:51.220 --> 00:18:53.680
And- and the intuition here is that

00:18:53.680 --> 00:18:56.860
the expressive GNN model can learn to

00:18:56.860 --> 00:19:00.100
capture more domain knowledge than less expressive model,

00:19:00.100 --> 00:19:02.515
especially learned from large amounts of,

00:19:02.515 --> 00:19:05.630
uh, data during pre-training.

00:19:05.700 --> 00:19:10.510
So to summarize of our GNNs, we've, uh,

00:19:10.510 --> 00:19:14.020
said- learned that the GNNs have important application

00:19:14.020 --> 00:19:17.800
in scientific domains like molecular property prediction or,

00:19:17.800 --> 00:19:20.214
um, protein function prediction,

00:19:20.214 --> 00:19:23.650
but, uh, those application domains present the challenges

00:19:23.650 --> 00:19:27.280
of label scarcity and out-of-distribution prediction,

00:19:27.280 --> 00:19:30.655
and we argue that pre-training is a promising,

00:19:30.655 --> 00:19:34.825
uh, framework to tackle both of the challenges.

00:19:34.825 --> 00:19:39.175
However, we found that naive pre-training strategy of this, uh,

00:19:39.175 --> 00:19:41.170
supervised graph level pre-training gives

00:19:41.170 --> 00:19:44.995
sub-optimal performance and even leads to negative transfer.

00:19:44.995 --> 00:19:47.545
And our strategy is to, um,

00:19:47.545 --> 00:19:51.475
effective strategy is to pre-train both node and graph embeddings,

00:19:51.475 --> 00:19:53.320
and we found this strategy leads to

00:19:53.320 --> 00:19:57.730
a significant performance gain on diverse downstream tasks.

00:19:57.730 --> 00:20:01.910
Um, yeah, thank you for listening.


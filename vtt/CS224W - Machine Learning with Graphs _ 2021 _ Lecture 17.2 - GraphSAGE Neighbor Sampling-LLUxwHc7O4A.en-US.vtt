WEBVTT
Kind: captions
Language: en-US

00:00:04.130 --> 00:00:07.500
Let's talk about neighborhood sampling.

00:00:07.500 --> 00:00:08.970
And this is really,

00:00:08.970 --> 00:00:11.730
the key idea of the GraphSAGE,

00:00:11.730 --> 00:00:14.219
uh, paper or the GraphSAGE architecture.

00:00:14.219 --> 00:00:16.410
So what the GraphSAGE, uh,

00:00:16.410 --> 00:00:19.215
what brought to the field of graph neural networks,

00:00:19.215 --> 00:00:22.650
is a way to think about mini-batch implementations.

00:00:22.650 --> 00:00:25.920
Before all the implementations were full batch and

00:00:25.920 --> 00:00:30.030
people would then run- run graph neural networks on you know, 3,

00:00:30.030 --> 00:00:34.700
4, 5,000 node graphs because that's what they could fit into GPU memory.

00:00:34.700 --> 00:00:36.825
And what GraphSAGE , uh,

00:00:36.825 --> 00:00:41.480
changed is they, it changed the way we think of graph neural networks,

00:00:41.480 --> 00:00:44.285
the way we can think of creating minibatches,

00:00:44.285 --> 00:00:47.740
and this also means that we can scale them up to graphs of,

00:00:47.740 --> 00:00:49.850
uh, tens of billions of nodes and edges.

00:00:49.850 --> 00:00:53.180
So, uh, let me now tell you about what do I mean by,

00:00:53.180 --> 00:00:56.565
um, a- a neighborhood sampling.

00:00:56.565 --> 00:00:59.250
So, uh, let's recall,

00:00:59.250 --> 00:01:01.270
let- let's remember how, uh,

00:01:01.270 --> 00:01:05.600
graph neural networks work and they operate through this notion of a computational graph.

00:01:05.600 --> 00:01:09.920
Where GNN generate node embeddings via neighborhood aggregation,

00:01:09.920 --> 00:01:13.400
meaning they aggregate features information from the neighbors,

00:01:13.400 --> 00:01:16.385
and then they create a new message and pass it on.

00:01:16.385 --> 00:01:17.600
So the way you can think of it,

00:01:17.600 --> 00:01:20.685
if you say I want to create an embedding for this node, uh,

00:01:20.685 --> 00:01:24.080
0, I will take neighbors and neighbors of neighbors.

00:01:24.080 --> 00:01:27.230
Here is now my computation graph, um,

00:01:27.230 --> 00:01:31.550
where nodes start with their individual feature vectors at the level 0.

00:01:31.550 --> 00:01:34.474
Uh, these feature vectors get aggregated,

00:01:34.474 --> 00:01:37.760
transformed, passed to the next level, um.

00:01:37.760 --> 00:01:39.800
And then again, the same thing gets aggregated

00:01:39.800 --> 00:01:42.680
transformed all the way to node- node of interest,

00:01:42.680 --> 00:01:44.675
node 0, that makes the prediction.

00:01:44.675 --> 00:01:47.330
Notice here that these nodes, you know, er,

00:01:47.330 --> 00:01:48.935
the node 0 here,

00:01:48.935 --> 00:01:51.815
means this is the feature vector of the node, uh,

00:01:51.815 --> 00:01:54.760
0, um, er, for example,

00:01:54.760 --> 00:01:57.410
here this means that this will be now a layer 1,

00:01:57.410 --> 00:01:59.750
er, representation of nodes,

00:01:59.750 --> 00:02:01.430
er, 1, 2, 3.

00:02:01.430 --> 00:02:05.840
And here I have now a layer 2 representation of node ze- of node 0.

00:02:05.840 --> 00:02:07.760
So what- what do I mean by this is that,

00:02:07.760 --> 00:02:11.820
actually nodes have multiple representations,

00:02:11.820 --> 00:02:14.370
uh, one for each, uh, layer.

00:02:14.370 --> 00:02:17.030
And that's the- that's why when you do full batch,

00:02:17.030 --> 00:02:20.480
you want to take level 0 embeddings to create level

00:02:20.480 --> 00:02:24.245
1 embeddings of everyone, to create level 2 embeddings from everyone.

00:02:24.245 --> 00:02:26.900
But here, because we have this local view,

00:02:26.900 --> 00:02:31.370
we will only need to create level 1 embeddings for nodes 1, 2,

00:02:31.370 --> 00:02:35.000
and 3, because we need those to create a level 2 embedding,

00:02:35.000 --> 00:02:36.995
uh, for node uh, 0.

00:02:36.995 --> 00:02:39.560
So that's, uh, the idea, right?

00:02:39.560 --> 00:02:45.770
So the important observation is that a two layer GNN generates embedding for this node 0,

00:02:45.770 --> 00:02:50.020
using got two-hop neighborhood structure and features around, uh, this graph.

00:02:50.020 --> 00:02:52.490
So if I wanted to compute the layer, uh,

00:02:52.490 --> 00:02:54.785
the embedding of node 0,

00:02:54.785 --> 00:02:57.124
what I need is the graph structure,

00:02:57.124 --> 00:03:02.330
plus features of this two-hop neighborhood around the- the node,

00:03:02.330 --> 00:03:04.715
and I can ignore the rest of the graph.

00:03:04.715 --> 00:03:07.370
I don't need to know the rest of the graph, right?

00:03:07.370 --> 00:03:10.790
The embedding of node 0 will be independent of how

00:03:10.790 --> 00:03:14.600
the graph structure is beyond the two-hop, uh, neighborhood.

00:03:14.600 --> 00:03:16.525
And that's an important insight.

00:03:16.525 --> 00:03:21.650
Because this means if I want to have a K layer GNN to emb- to generate

00:03:21.650 --> 00:03:26.405
an embedding of a node using these K-hop neighborhood structure and features,

00:03:26.405 --> 00:03:28.850
this means I only need to, um,

00:03:28.850 --> 00:03:33.050
know at the time and I'm generating that the embedding that K-hop neighborhood.

00:03:33.050 --> 00:03:35.560
And I can ignore the rest of the network.

00:03:35.560 --> 00:03:38.365
And now, if the level k is, uh,

00:03:38.365 --> 00:03:42.185
a relatively small or the neighborhood size is not too big,

00:03:42.185 --> 00:03:44.780
which means that it is let say constant size,

00:03:44.780 --> 00:03:47.015
but the entire graph can be much, much bigger,

00:03:47.015 --> 00:03:49.730
then it means I need relatively little information,

00:03:49.730 --> 00:03:54.800
or relatively little memory to use to generate or create the embedding,

00:03:54.800 --> 00:03:56.875
uh, of node, uh, 0.

00:03:56.875 --> 00:03:59.120
And, uh, this is the main insight.

00:03:59.120 --> 00:04:04.490
The main insight is that we wanna- that to compute the embedding of a single node,

00:04:04.490 --> 00:04:07.580
all we need is the K-hop neighborhood structure, uh,

00:04:07.580 --> 00:04:11.060
around that node, and we can ignore the rest of the network.

00:04:11.060 --> 00:04:16.875
The rest of the network does not affect the embedding of this node of interest,

00:04:16.875 --> 00:04:21.305
um, all that affects it is the K-hop neighborhood around that node.

00:04:21.305 --> 00:04:26.720
So what this means is that now we can generate minibatches in such a way that we say,

00:04:26.720 --> 00:04:30.020
let's sample M different nodes in a mini-batch,

00:04:30.020 --> 00:04:31.835
but we won't only put,

00:04:31.835 --> 00:04:34.370
we won't put nodes into the mini-batch,

00:04:34.370 --> 00:04:38.390
but we are going to put entire computation graphs into the mini-batch.

00:04:38.390 --> 00:04:40.940
So it means that I would pick a first node and I'll

00:04:40.940 --> 00:04:43.660
take its K-hop neighborhood computation graph,

00:04:43.660 --> 00:04:46.275
and this is one element in the batch.

00:04:46.275 --> 00:04:48.440
And then I'm going to sample the second node,

00:04:48.440 --> 00:04:52.055
create its computation graph and put this into my mini-batch.

00:04:52.055 --> 00:04:53.690
And so on and so forth.

00:04:53.690 --> 00:04:56.540
So now this means perhaps my batches will be smaller

00:04:56.540 --> 00:04:59.780
because batches are not now composed of individual nodes,

00:04:59.780 --> 00:05:02.870
but batches are composed of network neighborhoods,

00:05:02.870 --> 00:05:06.665
batches are composed from computation graphs.

00:05:06.665 --> 00:05:10.250
So we are going to sample M computation graphs, um,

00:05:10.250 --> 00:05:13.550
and then put these M computation graphs into the GPU memory,

00:05:13.550 --> 00:05:18.895
so that we can compute the loss over this mini batch of M computation graphs.

00:05:18.895 --> 00:05:21.260
So to emphasize again,

00:05:21.260 --> 00:05:23.015
you know, what is the key idea?

00:05:23.015 --> 00:05:24.755
The key idea is the following.

00:05:24.755 --> 00:05:29.810
It starts with the insight that in order to compute the embedding of a given node,

00:05:29.810 --> 00:05:34.300
all that we need to know is the K-hop neighborhood of that node.

00:05:34.300 --> 00:05:37.370
So this means if I create a mini

00:05:37.370 --> 00:05:41.810
batching not based on the nodes but based on the K-hop neighborhoods,

00:05:41.810 --> 00:05:44.390
then I will be able to compute- um,

00:05:44.390 --> 00:05:47.320
I will be able to compute the gradient in a reliable way.

00:05:47.320 --> 00:05:50.680
So basically, rather than putting nodes into mini-batches,

00:05:50.680 --> 00:05:51.920
we are putting, uh,

00:05:51.920 --> 00:05:54.695
computational graphs, or in other words,

00:05:54.695 --> 00:05:56.540
K-hop neighborhoods into the-,

00:05:56.540 --> 00:05:59.455
uh, into the mini-batches.

00:05:59.455 --> 00:06:03.540
Now, um, because we have put, um,

00:06:03.540 --> 00:06:05.310
uh, we have put, uh,

00:06:05.310 --> 00:06:07.550
uh, entire computation graphs,

00:06:07.550 --> 00:06:12.275
entire network neighborhoods of K-hops into the, into the mini-batch,

00:06:12.275 --> 00:06:14.540
we can, uh, consider the following, uh,

00:06:14.540 --> 00:06:18.860
stochastic gradient descent strategy to train the model parameters, right?

00:06:18.860 --> 00:06:20.650
We are going to sample,

00:06:20.650 --> 00:06:23.540
um, let say M nodes.

00:06:23.540 --> 00:06:25.730
For each node, we are going now to sample

00:06:25.730 --> 00:06:30.290
the entire K-hop neighborhood to construct the computation graph.

00:06:30.290 --> 00:06:31.850
And the we are, uh,

00:06:31.850 --> 00:06:34.490
assuming that we have enough memory, um,

00:06:34.490 --> 00:06:36.470
that we can fit into the mini-batch

00:06:36.470 --> 00:06:39.965
both the nodes as well as their entire computation graphs.

00:06:39.965 --> 00:06:43.625
Now, we have the complete set of information we need

00:06:43.625 --> 00:06:47.074
to compute an embedding of every- every node in the mini-batch,

00:06:47.074 --> 00:06:50.945
so we can then compute the loss over the- this mini-batch,

00:06:50.945 --> 00:06:54.110
and we can then perform stochastic gradient descent to

00:06:54.110 --> 00:06:57.880
basically update the model parameter with respect to the gradient,

00:06:57.880 --> 00:06:59.250
um, er, to that,

00:06:59.250 --> 00:07:00.890
uh, mini-batch, uh, loss.

00:07:00.890 --> 00:07:03.185
And this is stochastic gradient because,

00:07:03.185 --> 00:07:05.810
um, the batches are randomly created, so uh,

00:07:05.810 --> 00:07:08.930
the- the gradient will have a bit of,

00:07:08.930 --> 00:07:10.955
uh, randomness in it, but that's all fine.

00:07:10.955 --> 00:07:12.800
It just means we can do, uh, great,

00:07:12.800 --> 00:07:14.735
um, updates very, very fast.

00:07:14.735 --> 00:07:17.270
So that's the, that's the idea.

00:07:17.270 --> 00:07:20.345
So, um, if we, uh,

00:07:20.345 --> 00:07:22.175
do it the way I explained it,

00:07:22.175 --> 00:07:25.460
then we have, uh, still an issue with this notion of, uh,

00:07:25.460 --> 00:07:29.045
mini-batches and stochastic training because for each node,

00:07:29.045 --> 00:07:33.020
we need to get the entire K-hop neighborhood and pass it through

00:07:33.020 --> 00:07:37.490
the computation graph and load it into the GPU memory.

00:07:37.490 --> 00:07:43.250
So this means we need to aggregate a lot of information just to compute one- one node,

00:07:43.250 --> 00:07:45.650
uh, e- e- embedding for a single node.

00:07:45.650 --> 00:07:47.974
So computation will be expensive.

00:07:47.974 --> 00:07:50.375
So let me tell you why it will be expensive.

00:07:50.375 --> 00:07:52.340
First, it will be expensive because,

00:07:52.340 --> 00:07:54.710
um, um, uh, the deeper I go,

00:07:54.710 --> 00:07:56.390
the bigger these computation graphs and

00:07:56.390 --> 00:07:59.150
these computation graphs are going to increase- um,

00:07:59.150 --> 00:08:01.860
their size is going to increase exponentially, uh,

00:08:01.860 --> 00:08:04.250
with the- with the depth of

00:08:04.250 --> 00:08:08.480
the computation graph because even if every node has just three children,

00:08:08.480 --> 00:08:10.640
it's going to increase, uh,

00:08:10.640 --> 00:08:13.070
exponentially with the number of layers.

00:08:13.070 --> 00:08:15.260
So that's one issue.

00:08:15.260 --> 00:08:19.370
So the computation graphs are going to get very big if they get

00:08:19.370 --> 00:08:23.165
very deep and then the second thing is that in natural graphs,

00:08:23.165 --> 00:08:25.565
think of the lecture when we talked about

00:08:25.565 --> 00:08:29.149
Microsoft Instant Messenger network when we talked about degree distribution,

00:08:29.149 --> 00:08:31.190
we have these celebrity nodes,

00:08:31.190 --> 00:08:33.110
these high degree nodes, uh,

00:08:33.110 --> 00:08:37.610
that a lot of other people connect to or a lot of other nodes collect- connect to.

00:08:37.610 --> 00:08:39.740
We have such nodes even in knowledge graphs.

00:08:39.740 --> 00:08:41.255
If you think about, you know,

00:08:41.255 --> 00:08:43.310
a node corresponding to a large country,

00:08:43.310 --> 00:08:45.560
let's say like, uh, USA, um,

00:08:45.560 --> 00:08:47.360
it will have a huge degree because there is

00:08:47.360 --> 00:08:49.925
so many other entities related to it and of course,

00:08:49.925 --> 00:08:52.265
a small country will have a much smaller degree

00:08:52.265 --> 00:08:55.115
because there will be a smaller number of entities related to it.

00:08:55.115 --> 00:08:57.440
So the point is, we'll have these hub nodes.

00:08:57.440 --> 00:09:00.530
And now if a hub node has degree 1 million, uh,

00:09:00.530 --> 00:09:02.135
which is nothing out of ordinary,

00:09:02.135 --> 00:09:05.435
then you'll have to aggregate here information from 1 million nodes.

00:09:05.435 --> 00:09:09.290
So this computation graph will get huge very quickly and you are

00:09:09.290 --> 00:09:13.430
most likely going to hit these hub nodes, uh, very often.

00:09:13.430 --> 00:09:14.810
So the point is,

00:09:14.810 --> 00:09:20.540
you cannot take the entire K-hop neighborhood in most of the cases. So what do you do?

00:09:20.540 --> 00:09:23.555
What do you do, is to do- is to

00:09:23.555 --> 00:09:26.810
ap- apply on a project that is called neighborhood sampling.

00:09:26.810 --> 00:09:29.120
Where the key idea is to cra- construct

00:09:29.120 --> 00:09:33.080
a computational graph by sampling at most H neighbors,

00:09:33.080 --> 00:09:34.655
uh, of every node.

00:09:34.655 --> 00:09:38.750
So it means that every- every node- every node in the tree- in

00:09:38.750 --> 00:09:43.205
the computation graph is going to have at most- is going to aggregate from at most,

00:09:43.205 --> 00:09:45.740
uh, H other nodes.

00:09:45.740 --> 00:09:48.155
So in our case, just to give you an example,

00:09:48.155 --> 00:09:50.480
if I say, let's say H equals 3,

00:09:50.480 --> 00:09:54.785
then my original computation graph is now going to be pruned in such a way that

00:09:54.785 --> 00:10:00.020
every- every aggregation is going to aggregate from at most two other nodes.

00:10:00.020 --> 00:10:04.970
So here, this entire branch is going to be cut out and you know,

00:10:04.970 --> 00:10:08.645
some other nodes are going to be cut out as well.

00:10:08.645 --> 00:10:11.570
And what this means is that now our, um,

00:10:11.570 --> 00:10:16.070
computation graph will be much more manageable because we have just,

00:10:16.070 --> 00:10:18.440
um, even if we hit the high degree hub node,

00:10:18.440 --> 00:10:19.835
we are only to take,

00:10:19.835 --> 00:10:22.460
uh, a fixed number of its neighbors,

00:10:22.460 --> 00:10:24.634
uh, in the aggregation.

00:10:24.634 --> 00:10:27.500
So the point is that you can use these print

00:10:27.500 --> 00:10:31.280
computational graphs to more efficiently compute, uh, node embedding.

00:10:31.280 --> 00:10:32.750
Um, so now, uh,

00:10:32.750 --> 00:10:36.110
how do you do this computational graph sampling, right?

00:10:36.110 --> 00:10:39.155
Basically the idea is for every node, for every layer, uh,

00:10:39.155 --> 00:10:41.240
for every internal, um,

00:10:41.240 --> 00:10:43.580
node of the- of the computation graph,

00:10:43.580 --> 00:10:45.920
uh, we are going to, uh,

00:10:45.920 --> 00:10:48.545
first basically compute the K-hop neighborhood, uh,

00:10:48.545 --> 00:10:51.200
from the starting node and then for every, uh,

00:10:51.200 --> 00:10:52.490
node in the computation graph,

00:10:52.490 --> 00:10:55.325
we are going to pick at most, uh,

00:10:55.325 --> 00:10:58.580
K, uh, H random, uh, neighbors.

00:10:58.580 --> 00:11:03.860
Um, and this means that the K-layer GNN will, uh, involve, uh,

00:11:03.860 --> 00:11:06.590
at most, uh, um, uh,

00:11:06.590 --> 00:11:08.645
the product of the- of, uh,

00:11:08.645 --> 00:11:10.850
H leaf nodes in the computation graph.

00:11:10.850 --> 00:11:14.075
So our computation graphs are still gro- gro- going to grow, uh,

00:11:14.075 --> 00:11:16.760
exponentially but the, uh,

00:11:16.760 --> 00:11:18.785
but the point will be that,

00:11:18.785 --> 00:11:20.180
uh, their fan out,

00:11:20.180 --> 00:11:21.740
will be- will be, uh,

00:11:21.740 --> 00:11:25.490
upper bounded by H. So the- the growth won't be uh,

00:11:25.490 --> 00:11:28.280
that bad or that fast and we'll still be able to go,

00:11:28.280 --> 00:11:30.560
uh, quite, uh, deep.

00:11:30.560 --> 00:11:32.840
Now, let me make a few,

00:11:32.840 --> 00:11:34.040
uh, remarks about these.

00:11:34.040 --> 00:11:36.830
Uh, first remark is that there is, uh, the trade-off,

00:11:36.830 --> 00:11:39.860
uh, in- in how many neighbors do we sample, right?

00:11:39.860 --> 00:11:43.880
The smaller, uh, H leads to more efficient neighborhood aggregation

00:11:43.880 --> 00:11:48.065
because computation graphs would be smaller but results in more unstable,

00:11:48.065 --> 00:11:50.420
uh, training because we are ignoring

00:11:50.420 --> 00:11:54.380
entire subparts of the network when we are doing message aggregation.

00:11:54.380 --> 00:11:57.110
So our, uh, um, uh,

00:11:57.110 --> 00:11:59.120
gradient estimates will be more, uh,

00:11:59.120 --> 00:12:01.790
noisy, they will have higher, uh, variance.

00:12:01.790 --> 00:12:05.255
Uh, another thing is that in terms of computation time,

00:12:05.255 --> 00:12:07.085
even the neighborhood sampling,

00:12:07.085 --> 00:12:09.035
the size of the computational graph,

00:12:09.035 --> 00:12:13.220
as I said is still exponential with respect to the number of layers but

00:12:13.220 --> 00:12:17.705
if H is not that large and we don't go too deep in terms of K,

00:12:17.705 --> 00:12:19.230
uh, then it is still man-, uh,

00:12:19.230 --> 00:12:22.235
uh, uh, manageable, right?

00:12:22.235 --> 00:12:25.910
Um, so, you know, adding one more layer to the, uh,

00:12:25.910 --> 00:12:29.390
to the GNN makes the computation H times,

00:12:29.390 --> 00:12:30.935
uh, more expensive and now,

00:12:30.935 --> 00:12:32.150
you know, if, uh,

00:12:32.150 --> 00:12:35.120
H is maybe an order of 5-10,

00:12:35.120 --> 00:12:37.820
and K is also, I don't know, on order of, you know,

00:12:37.820 --> 00:12:40.325
5 plus minus, then you can still,

00:12:40.325 --> 00:12:42.425
uh, keep, uh, doing this.

00:12:42.425 --> 00:12:44.855
And then, uh, the last, uh,

00:12:44.855 --> 00:12:48.500
the last important thing I want to mention is remark number 3 which is

00:12:48.500 --> 00:12:52.820
this- this approach gives you a lot of freedom how you select the nodes,

00:12:52.820 --> 00:12:57.680
uh, to sample and so far I don't- I call it a random sampling, right?

00:12:57.680 --> 00:13:01.340
Just uniformly at random pick e- at H neighbors,

00:13:01.340 --> 00:13:03.350
uh, of a given node.

00:13:03.350 --> 00:13:07.175
Uh, but the- the issue is with this approach is that,

00:13:07.175 --> 00:13:10.580
uh, real-world networks have these highly skewed degree distributions.

00:13:10.580 --> 00:13:16.250
So there is a lot of kind of single- single nodes or low degree nodes in the network.

00:13:16.250 --> 00:13:18.935
And if you are taking an, uh,

00:13:18.935 --> 00:13:20.090
a given node of interest,

00:13:20.090 --> 00:13:21.980
and sample H of its neighbors,

00:13:21.980 --> 00:13:25.280
you are most likely going to sample this like degree 1, uh,

00:13:25.280 --> 00:13:27.680
nodes that are not perhaps the most important nodes in

00:13:27.680 --> 00:13:31.055
the network and perhaps very noisy, not the most informative.

00:13:31.055 --> 00:13:33.605
This could be users that are not very engaged,

00:13:33.605 --> 00:13:35.540
this could be pieces of content that are

00:13:35.540 --> 00:13:38.645
not very important and you don't have a good signal on.

00:13:38.645 --> 00:13:40.130
So what you can do,

00:13:40.130 --> 00:13:41.840
and this works much better in practice,

00:13:41.840 --> 00:13:46.940
is that you'll do a random walk with restart from the node of interest,

00:13:46.940 --> 00:13:48.320
here, the green node.

00:13:48.320 --> 00:13:51.935
And then your sampling strategy is to take, uh, uh,

00:13:51.935 --> 00:13:56.000
H neighbors but not at- let's say not at random,

00:13:56.000 --> 00:13:57.365
but based on, uh,

00:13:57.365 --> 00:13:59.945
their random walk with restart scores.

00:13:59.945 --> 00:14:05.359
So it means that at every layer you are going to take H most important neighbors,

00:14:05.359 --> 00:14:06.920
uh, of a given node.

00:14:06.920 --> 00:14:10.400
And, uh this means that the graph

00:14:10.400 --> 00:14:13.820
you are going to select will be kind of much more representative,

00:14:13.820 --> 00:14:15.830
much better connected, um,

00:14:15.830 --> 00:14:17.810
and it will have- it will be based on

00:14:17.810 --> 00:14:22.279
these more important nodes which have better and more reliable feature information,

00:14:22.279 --> 00:14:23.735
these are more active users,

00:14:23.735 --> 00:14:26.780
so they kind of provide you more information for predictions.

00:14:26.780 --> 00:14:30.980
So in practice, this strategy of sampling the computation graph,

00:14:30.980 --> 00:14:33.455
um, works much better,

00:14:33.455 --> 00:14:36.020
um, and, you know, there is- I think here to say,

00:14:36.020 --> 00:14:39.545
there is room to do a proper investigation about

00:14:39.545 --> 00:14:43.910
how would you define what are different strategies to sample,

00:14:43.910 --> 00:14:45.590
to define the computation graphs,

00:14:45.590 --> 00:14:48.125
to sample the computation graphs and ha-

00:14:48.125 --> 00:14:51.050
what are their strategies in which kind of cases?

00:14:51.050 --> 00:14:52.700
I think this still hasn't been, uh,

00:14:52.700 --> 00:14:56.990
systematically investigated but such a study would be very important,

00:14:56.990 --> 00:14:58.460
uh, for the field of,

00:14:58.460 --> 00:15:00.860
uh, graph machine learning.

00:15:00.860 --> 00:15:06.080
So, uh, to summarize the pro- the neighborhood sampling approach,

00:15:06.080 --> 00:15:10.340
the idea is that the computational graph is constructed for each node, um,

00:15:10.340 --> 00:15:14.540
and the computational graphs are put into the mini- mini-batch because

00:15:14.540 --> 00:15:16.970
computational graphs can become very big very

00:15:16.970 --> 00:15:20.180
quickly by hitting a high degree hub node, um,

00:15:20.180 --> 00:15:22.910
neighborhoods- we then proposed neighborhood sampling

00:15:22.910 --> 00:15:25.730
which is where the computational graph is created

00:15:25.730 --> 00:15:31.750
stochastically or is pruned sub-sampled to increase computational efficiency.

00:15:31.750 --> 00:15:36.620
It also increases the model robustness because now the GNN architecture is,

00:15:36.620 --> 00:15:38.270
uh, stochastic by it's self,

00:15:38.270 --> 00:15:42.020
so it's almost like a form of dropout if you want to think of it that way, uh,

00:15:42.020 --> 00:15:47.315
and the computa- pruned computational graph is used to generate node embeddings.

00:15:47.315 --> 00:15:51.860
Um, here, uh, caveat is that if- if your network,

00:15:51.860 --> 00:15:55.040
uh, GNN- number of GNN layers is very deep,

00:15:55.040 --> 00:15:58.040
these computation graphs may still become large which means

00:15:58.040 --> 00:16:01.010
your batch sizes will have to be smaller, um,

00:16:01.010 --> 00:16:03.890
which means, uh, your gradient will be, um,

00:16:03.890 --> 00:16:07.445
uh, uh, kind of more, uh, less reliable.

00:16:07.445 --> 00:16:10.375
So if the batch size is small,

00:16:10.375 --> 00:16:14.065
the, uh, the gradient is less reliable,

00:16:14.065 --> 00:16:16.660
and if the pruning is too much,

00:16:16.660 --> 00:16:19.880
then again the, uh,

00:16:19.880 --> 00:16:22.460
gradient, uh, gradients are not too reliable.

00:16:22.460 --> 00:16:25.870
So it's important to find a good balance between the batch size and, uh,

00:16:25.870 --> 00:16:30.505
the pruning factor or sampling factor for the computation graphs.

00:16:30.505 --> 00:16:33.295
But that's essentially the idea and this is really, I would say,

00:16:33.295 --> 00:16:37.850
what most of the large-scale industrial implementations of graph neural networks use,

00:16:37.850 --> 00:16:39.335
uh, to achieve, uh,

00:16:39.335 --> 00:16:41.660
scaling gap to industrial size graphs.

00:16:41.660 --> 00:16:43.480
For example, this is what is used at Pinterest,

00:16:43.480 --> 00:16:45.920
at Alibaba and so on.


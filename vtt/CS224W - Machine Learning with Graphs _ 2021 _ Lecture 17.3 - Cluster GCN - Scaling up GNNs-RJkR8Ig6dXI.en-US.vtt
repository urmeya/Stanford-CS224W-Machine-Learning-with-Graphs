WEBVTT
Kind: captions
Language: en-US

00:00:04.070 --> 00:00:10.260
Now we are going to talk about the second method in this idea of neighborhood, uh,

00:00:10.260 --> 00:00:14.100
sampling and trying to limit, uh, the- the,

00:00:14.100 --> 00:00:15.960
uh, batch size, uh,

00:00:15.960 --> 00:00:18.600
issues, uh, in graph neural networks.

00:00:18.600 --> 00:00:21.060
Um, and the way we are going to do this is,

00:00:21.060 --> 00:00:23.115
uh, to start with the following observation, right?

00:00:23.115 --> 00:00:25.935
That the size of computational graphs, um,

00:00:25.935 --> 00:00:27.685
can still become very big, uh,

00:00:27.685 --> 00:00:31.355
or increases exponentially with the number of, uh, GNN layers.

00:00:31.355 --> 00:00:35.020
And what we can also observe and notice is that, um,

00:00:35.020 --> 00:00:38.300
computation in these, uh- computational graphs

00:00:38.300 --> 00:00:41.900
can be very much redundant because of shared neighbors.

00:00:41.900 --> 00:00:43.895
Like if you think for example, two nodes,

00:00:43.895 --> 00:00:47.210
A and B, and you think about the computation graphs,

00:00:47.210 --> 00:00:51.645
then they have, uh- because they share- they have a lot of friends in common,

00:00:51.645 --> 00:00:54.590
these computation graphs are heavily redundant, right?

00:00:54.590 --> 00:00:55.985
Like the- the, uh,

00:00:55.985 --> 00:00:58.745
both to compute A and compute B,

00:00:58.745 --> 00:01:01.760
the subgraphs the comp- the parts of the computation graphs

00:01:01.760 --> 00:01:04.820
between- below node C and D are identical.

00:01:04.820 --> 00:01:09.430
So this would be the duplicate- this will be duplicated, uh, um, computation.

00:01:09.430 --> 00:01:12.870
And, uh, there are two approaches to do this.

00:01:12.870 --> 00:01:15.610
One is to, uh, realize that this, uh,

00:01:15.610 --> 00:01:19.360
computation is duplicated then computed only once.

00:01:19.360 --> 00:01:23.285
Uh, there is a very nice paper about this at last year KDD called,

00:01:23.285 --> 00:01:25.790
uh, uh, HAGs, hierarchical, um,

00:01:25.790 --> 00:01:29.499
aggregation, uh, graphs that basically prevent,

00:01:29.499 --> 00:01:33.455
um, uh, multiple computations, so redundant computations.

00:01:33.455 --> 00:01:37.895
Um, and the- the second case will be about Cluster-GCN,

00:01:37.895 --> 00:01:40.555
which is what I'm going to talk about today.

00:01:40.555 --> 00:01:46.770
And the way we are, uh, motivating Cluster-GCN is to remember what is a full ba- batch,

00:01:46.770 --> 00:01:48.210
uh, graph neural network, right?

00:01:48.210 --> 00:01:51.980
A full batch graph neural network is that all the embeddings

00:01:51.980 --> 00:01:55.850
for all the nodes are embedded together in a- in a single pass.

00:01:55.850 --> 00:02:00.065
So basically, um, embeddings of all the nodes are computed for layer 1,

00:02:00.065 --> 00:02:03.200
embeddings for all the nodes are then computed for layer 2,

00:02:03.200 --> 00:02:04.400
uh, and so on, right?

00:02:04.400 --> 00:02:07.040
The point is you need to have the embeddings of

00:02:07.040 --> 00:02:10.610
all the nodes at lab- at layer L minus 1 to be

00:02:10.610 --> 00:02:13.940
able to compute the embeddings for all the other nodes- for

00:02:13.940 --> 00:02:17.579
all the nodes at layer L. So it means that,

00:02:17.579 --> 00:02:18.750
uh, in each layer,

00:02:18.750 --> 00:02:20.595
2 times number of edges,

00:02:20.595 --> 00:02:22.180
uh, messages need to be, uh,

00:02:22.180 --> 00:02:24.685
exchanged because every node needs to connect-

00:02:24.685 --> 00:02:27.825
collect the- the message is from its neighbors,

00:02:27.825 --> 00:02:32.030
so there are two messages along each undirected edge, you know,

00:02:32.030 --> 00:02:34.375
for both, uh, endpoints,

00:02:34.375 --> 00:02:36.975
and, uh, what is the observation?

00:02:36.975 --> 00:02:38.305
The observation is that, uh,

00:02:38.305 --> 00:02:42.590
the amount of computation we need to do in a single layer of, uh, let's say,

00:02:42.590 --> 00:02:43.820
uh, full batch, uh,

00:02:43.820 --> 00:02:46.910
implementation is linear in the size of the graph.

00:02:46.910 --> 00:02:48.635
It's linear in the number of edges.

00:02:48.635 --> 00:02:50.640
So it means it is very fast.

00:02:50.640 --> 00:02:53.330
But the problem is that the graphs are too big.

00:02:53.330 --> 00:02:56.880
Uh, so we cannot do this kind of at once,

00:02:56.880 --> 00:02:58.410
uh, in the GPU.

00:02:58.410 --> 00:03:01.940
So the insight from full-batch GNN is that

00:03:01.940 --> 00:03:07.855
layer-wise node embedding updates allow us to reuse embeddings from the previous layer.

00:03:07.855 --> 00:03:12.440
Um, and this significantly reduces the computational redundancy of,

00:03:12.440 --> 00:03:13.880
uh, neighborhood sampling, right?

00:03:13.880 --> 00:03:17.150
Uh, this means that this layer-wise update to generate

00:03:17.150 --> 00:03:21.590
embeddings from one layer to the next is very cheap, right?

00:03:21.590 --> 00:03:23.760
All I need to do is previous layer embeddings.

00:03:23.760 --> 00:03:25.160
I need to aggregate them,

00:03:25.160 --> 00:03:27.920
combine them with the previous layer embedding of a given node

00:03:27.920 --> 00:03:30.650
and I have a new, uh- a new embedding.

00:03:30.650 --> 00:03:31.880
So what this means is that,

00:03:31.880 --> 00:03:33.680
uh, this can be done very fast.

00:03:33.680 --> 00:03:38.720
It's only linear time operation in the size of the graph because it's aggregation,

00:03:38.720 --> 00:03:41.330
uh, uh, basically takes time linear because

00:03:41.330 --> 00:03:44.275
it's linear number of edges over which we aggregate.

00:03:44.275 --> 00:03:48.700
So this sig- means that the computations are very fast.

00:03:48.700 --> 00:03:52.100
But of course the problem is that layer-wise update is not feasible

00:03:52.100 --> 00:03:55.970
for the entire graph at once because of the GPU memory.

00:03:55.970 --> 00:04:02.990
So now, what the idea is in the Cluster-GCN is that can we sample

00:04:02.990 --> 00:04:06.920
the entire graph into small sub-parts and then

00:04:06.920 --> 00:04:11.900
perform full batch implementation on those subgraphs.

00:04:11.900 --> 00:04:14.465
All right. So the idea is the following.

00:04:14.465 --> 00:04:15.860
I'll take the large graph.

00:04:15.860 --> 00:04:18.000
I'm going to sample a subgraph of it.

00:04:18.000 --> 00:04:20.634
I don't know maybe I'll just say I'll sample the subgraph,

00:04:20.634 --> 00:04:24.740
and then I'm going, uh, in this subgraph I'm able to fit it on a GPU.

00:04:24.740 --> 00:04:28.505
So now I can apply a full batch implementation,

00:04:28.505 --> 00:04:31.825
uh, in the GPU on that sampled subgraph.

00:04:31.825 --> 00:04:34.295
All right? So that's essentially the idea.

00:04:34.295 --> 00:04:37.040
So the difference between neighborhood sampling is that

00:04:37.040 --> 00:04:41.080
there we compute the computation graph for each individual node.

00:04:41.080 --> 00:04:46.790
Um, here, we are going to sample an entire subgraph of the original graph and

00:04:46.790 --> 00:04:49.400
then pretend that this is the graph of interest and

00:04:49.400 --> 00:04:52.990
just perform GNN on that, uh, subgraph.

00:04:52.990 --> 00:04:55.710
Right? So then the question is,

00:04:55.710 --> 00:04:57.600
how should I sample these subgraphs?

00:04:57.600 --> 00:05:00.375
What are good subgraphs for training GNNs?

00:05:00.375 --> 00:05:02.250
Um, and here's the reasoning.

00:05:02.250 --> 00:05:05.990
The important point to remember is that GNN performs

00:05:05.990 --> 00:05:10.070
embedding by passing messages via the edges of the network, right?

00:05:10.070 --> 00:05:12.740
Every node computes its embedding by collecting

00:05:12.740 --> 00:05:15.530
information from the neighbors, uh, right?

00:05:15.530 --> 00:05:18.130
So the idea is now if I wanna have a good subgraph,

00:05:18.130 --> 00:05:23.600
the good subgraph have- have- has to retain as many of the edges of the original graph as

00:05:23.600 --> 00:05:26.720
possible so that my computations mimic

00:05:26.720 --> 00:05:30.260
the computations on the big graph as well as possible.

00:05:30.260 --> 00:05:32.600
So our subgraphs should retain

00:05:32.600 --> 00:05:36.535
edge connectivity structure of the ne- original graph as much as possible.

00:05:36.535 --> 00:05:41.520
Which means that the GNN over the subgraph generates embeddings that are close,

00:05:41.520 --> 00:05:45.120
uh, to the- to the GNN over the original graph.

00:05:45.120 --> 00:05:47.810
So to give you- to give you an idea,

00:05:47.810 --> 00:05:51.500
if I have an original graph and let's say I sample, uh,

00:05:51.500 --> 00:05:55.650
I- I sample a subgraph on four nodes like I show here on the left

00:05:55.650 --> 00:05:59.945
or I sample another subgraph on four nodes like I show on the right,

00:05:59.945 --> 00:06:04.220
then it's clear that the left subgraph is kind of much better for training.

00:06:04.220 --> 00:06:05.645
Because if I now say,

00:06:05.645 --> 00:06:08.930
let's compute the embedding of this corner node up here,

00:06:08.930 --> 00:06:12.740
here, all the three neighbors of it are in the- in the subgraph.

00:06:12.740 --> 00:06:17.240
So I'll be able to get a quite good estimate of the embedding of this,

00:06:17.240 --> 00:06:20.575
uh, node, um, corner node up here.

00:06:20.575 --> 00:06:22.750
So right, it's- all of its, um, uh,

00:06:22.750 --> 00:06:26.065
uh, neighbors are- are part of my subgraph.

00:06:26.065 --> 00:06:28.555
While if I take this other subgraph here,

00:06:28.555 --> 00:06:30.280
we- denoted by green nodes.

00:06:30.280 --> 00:06:34.420
And I wanna predict or generate an embedding for this green node as well,

00:06:34.420 --> 00:06:38.935
then- then I- all I'm able to do is aggregate message from one of its neighbors,

00:06:38.935 --> 00:06:43.320
because only one of its neighbors is part of my, uh, subgraph, right?

00:06:43.320 --> 00:06:47.100
So the right subgraph drops many- many edges,

00:06:47.100 --> 00:06:49.480
um, and leading to isolated nodes,

00:06:49.480 --> 00:06:53.320
which means my computation graphs over this subgraph won't

00:06:53.320 --> 00:06:57.340
be representative of the computation graphs in the original graph.

00:06:57.340 --> 00:06:59.980
So left choice is far better,

00:06:59.980 --> 00:07:02.075
uh, than the right choice.

00:07:02.075 --> 00:07:04.750
And this now brings us back to

00:07:04.750 --> 00:07:10.015
the network community structure because real world graphs exhibit community structure.

00:07:10.015 --> 00:07:11.675
They exhibit clustering structure.

00:07:11.675 --> 00:07:15.820
So large graphs can be decomposed into many small, uh, communities.

00:07:15.820 --> 00:07:20.170
And the key insight would be that we can sample these subgraphs based on

00:07:20.170 --> 00:07:23.110
the community structure so that each subgraph

00:07:23.110 --> 00:07:27.470
retains most of the edges and only few edges, uh, are dropped.

00:07:27.470 --> 00:07:30.720
So a Cluster-GCN, the way it works,

00:07:30.720 --> 00:07:33.340
it has, uh, uh, a two-step process.

00:07:33.340 --> 00:07:34.930
In the preprocessing step,

00:07:34.930 --> 00:07:37.555
we take the original graph and split it,

00:07:37.555 --> 00:07:39.205
cut it into many small,

00:07:39.205 --> 00:07:41.425
uh- small, uh, subgraphs.

00:07:41.425 --> 00:07:45.430
Uh, and then, uh, mini-batch training means that we sample one node,

00:07:45.430 --> 00:07:47.565
group 1 subgraph, uh, and, uh,

00:07:47.565 --> 00:07:51.890
perform the- the full batch message passing over that entire subgraph,

00:07:51.890 --> 00:07:55.805
compute the node embe- uh- node embeddings of all the nodes in the subgraph,

00:07:55.805 --> 00:07:58.910
evaluate the loss, compute the gradient,

00:07:58.910 --> 00:08:00.500
and update the parameters, right?

00:08:00.500 --> 00:08:02.120
So the idea is input graph,

00:08:02.120 --> 00:08:04.445
split into many subgraphs.

00:08:04.445 --> 00:08:06.335
We take one subgraph,

00:08:06.335 --> 00:08:08.345
fit it into the GPU memory,

00:08:08.345 --> 00:08:11.840
do the full computation on the subgraph, compute the loss,

00:08:11.840 --> 00:08:15.705
update model parameters, load in another subgraph,

00:08:15.705 --> 00:08:18.510
do the, uh, a full- full batch,

00:08:18.510 --> 00:08:21.355
uh, layer embeddings of all the nodes in the subgraph.

00:08:21.355 --> 00:08:23.750
Compute the loss, update the gradient,

00:08:23.750 --> 00:08:25.700
load, uh, the next subgraphs.

00:08:25.700 --> 00:08:30.720
So that's how vanilla Cluster-GCN, uh, would work.

00:08:30.780 --> 00:08:34.059
So to give you more- more details,

00:08:34.059 --> 00:08:36.490
the idea is that given a large, uh, graph,

00:08:36.490 --> 00:08:38.140
we're going to partition it into,

00:08:38.140 --> 00:08:40.105
let's say, capital C groups.

00:08:40.105 --> 00:08:44.800
Um, we can use some scalable community detection method like Louvain, uh,

00:08:44.800 --> 00:08:47.110
like METIS method, um,

00:08:47.110 --> 00:08:48.655
or we can use, uh,

00:08:48.655 --> 00:08:50.560
even BIGCLAM is fine,

00:08:50.560 --> 00:08:52.375
um, and then basically this,

00:08:52.375 --> 00:08:54.370
um, set of, uh,

00:08:54.370 --> 00:08:56.500
node groups V. Uh,

00:08:56.500 --> 00:09:00.715
we are going to take an induced subgraph on the subset of nodes,

00:09:00.715 --> 00:09:02.920
um, and basically these induced subgraphs will

00:09:02.920 --> 00:09:05.310
include all the edges between the members, ah,

00:09:05.310 --> 00:09:07.720
of the- of the group, and of course,

00:09:07.720 --> 00:09:10.614
these edges that go across the subgraphs,

00:09:10.614 --> 00:09:11.920
those will be dropped.

00:09:11.920 --> 00:09:16.150
[NOISE] So between group edges are going to be dropped in these,

00:09:16.150 --> 00:09:21.100
uh, uh, graphs, G1 to GC which are these subgraphs we sampled.

00:09:21.100 --> 00:09:24.040
So now, for each, uh, mini-batch,

00:09:24.040 --> 00:09:28.210
we are going to sample one such node group, one such subgraph,

00:09:28.210 --> 00:09:32.275
create an induced subgraph over that set of nodes,

00:09:32.275 --> 00:09:35.830
and this is now our graph that we are going to use,

00:09:35.830 --> 00:09:38.575
um, as a computation in the GPU.

00:09:38.575 --> 00:09:42.940
So the point is that this induced subgraph has to fit into the GPU memory.

00:09:42.940 --> 00:09:45.760
So now, we do layer-wise node update

00:09:45.760 --> 00:09:48.745
basically the same thing as what we do in the full batch, uh,

00:09:48.745 --> 00:09:54.565
GNN to compute now the embeddings of all the nodes in the subgraph, recompute, uh,

00:09:54.565 --> 00:09:57.025
the loss, uh, and then do the, uh,

00:09:57.025 --> 00:10:01.420
compute the gradient with respect to the loss and then do the gradient update.

00:10:01.420 --> 00:10:07.300
So the point here is that when we were doing GraphSAGE neighborhood sampling, our nodes,

00:10:07.300 --> 00:10:10.870
we were able to kind of sample them all- all- all across the graphs.

00:10:10.870 --> 00:10:12.760
But in this case, um,

00:10:12.760 --> 00:10:14.500
we are going to sample a subgraph of

00:10:14.500 --> 00:10:17.470
the original graph and now only compute on those nodes,

00:10:17.470 --> 00:10:20.860
uh, we have sampled, and that's the big difference between,

00:10:20.860 --> 00:10:23.125
ah, the neighborhood sampling that can be kind of,

00:10:23.125 --> 00:10:27.175
those neighborhoods can be distributed across the entire network while here,

00:10:27.175 --> 00:10:31.360
we are going to create those neighborhoods basically just being limited to

00:10:31.360 --> 00:10:35.785
the subgraph we sample and we feed that subgraph into the GPU memory.

00:10:35.785 --> 00:10:39.190
So what are some issues and how do we fix them?

00:10:39.190 --> 00:10:45.205
The issue with Cluster-GCN is that these induced subgraphs are removed in- in,

00:10:45.205 --> 00:10:48.370
uh, between the group links, between subgraph links.

00:10:48.370 --> 00:10:51.115
And as a result, messages from other groups

00:10:51.115 --> 00:10:54.535
will be lost during message-passing which will hur- which can hurt,

00:10:54.535 --> 00:10:56.770
uh, GNN performance, right?

00:10:56.770 --> 00:11:03.250
So for example, if I sample this red graph here and I just zoom in into the node- uh,

00:11:03.250 --> 00:11:05.050
into it, what I see is that,

00:11:05.050 --> 00:11:06.385
for example, whenever, ah,

00:11:06.385 --> 00:11:10.375
this particular node aggregates information,

00:11:10.375 --> 00:11:14.185
it will never aggregate information from these two edges because these two- uh,

00:11:14.185 --> 00:11:17.710
the node endpoints are part of the se- the different subgraphs.

00:11:17.710 --> 00:11:21.160
So all the message aggregation will happen only between these four nodes,

00:11:21.160 --> 00:11:22.300
uh, and the five,

00:11:22.300 --> 00:11:23.905
uh, edges between them,

00:11:23.905 --> 00:11:28.000
so there will be in some sense, uh, lost messages.

00:11:28.000 --> 00:11:31.225
So, uh, the issue will be because the-

00:11:31.225 --> 00:11:35.785
the graph community detection put similar nodes together into the same group,

00:11:35.785 --> 00:11:38.005
uh, sample node groups tend to cover

00:11:38.005 --> 00:11:41.890
only a small concentrated portion of the entire graph, right?

00:11:41.890 --> 00:11:46.420
So we are going to learn only on a small concentrated portions of the graph while

00:11:46.420 --> 00:11:51.475
neighborhood sampling allows us to kind of learn on small but very diverse,

00:11:51.475 --> 00:11:53.590
uh, set of, uh, neighborhoods.

00:11:53.590 --> 00:11:56.470
Um, and this means that because this sample node,

00:11:56.470 --> 00:12:01.285
sample subgraph vor- will be- will be our concentrated part of the entire graph,

00:12:01.285 --> 00:12:04.270
it won't be kind of diverse or a representative to

00:12:04.270 --> 00:12:07.330
represent the entire, uh, training graphs.

00:12:07.330 --> 00:12:11.110
So what this means is that when we compute the gradient and the loss, uh,

00:12:11.110 --> 00:12:14.080
this is going to fluctuate a lot from one node group to

00:12:14.080 --> 00:12:17.350
another because- because of the social communities,

00:12:17.350 --> 00:12:19.015
uh, if you think of it that way.

00:12:19.015 --> 00:12:22.120
Each subgroup will be very concentrated in one given

00:12:22.120 --> 00:12:25.660
aspect and the- the model will have hard time to learn, right?

00:12:25.660 --> 00:12:26.965
You will- I know- you know,

00:12:26.965 --> 00:12:29.500
you have a cluster of computer scientists,

00:12:29.500 --> 00:12:31.720
so you will learn how to make- you will compute

00:12:31.720 --> 00:12:34.150
the gradient with respect to computer scientists,

00:12:34.150 --> 00:12:36.730
then you have a cluster of, let's say, uh,

00:12:36.730 --> 00:12:39.430
music students and now your gradient that

00:12:39.430 --> 00:12:42.130
you have just- your models that were just computed over

00:12:42.130 --> 00:12:45.160
the computer scientists in your social network are now going to

00:12:45.160 --> 00:12:48.370
be computed over music people which are very, very different.

00:12:48.370 --> 00:12:50.740
And then I don't know, you have mathematicians who are

00:12:50.740 --> 00:12:54.100
different- community of mathematicians who are different from the two.

00:12:54.100 --> 00:12:57.085
So the gradients are going to fluctuate quite a lot,

00:12:57.085 --> 00:13:03.250
and training is quite unstable which in practice leads to slow convergence of,

00:13:03.250 --> 00:13:06.070
uh, gradient descent or stochastic gradient descent.

00:13:06.070 --> 00:13:12.235
So, uh, a way to improve this is to- is what is called Advanced Cluster-GCN.

00:13:12.235 --> 00:13:13.810
And the idea is that here,

00:13:13.810 --> 00:13:17.290
we wanna aggregate multiple node groups per mini-batch, right?

00:13:17.290 --> 00:13:20.830
So the idea is to- to make the graphs we sampled even

00:13:20.830 --> 00:13:25.510
smaller but then we are going to sample multiple of these,

00:13:25.510 --> 00:13:28.510
uh, subgraphs, uh, into the mini-batch.

00:13:28.510 --> 00:13:30.610
Um, and we are going to create, uh,

00:13:30.610 --> 00:13:37.000
induced subgraph on the aggregated node group that is now composed of many small, uh,

00:13:37.000 --> 00:13:41.410
node groups, and the rest will be the same is- I- as in the Cluster-GCN,

00:13:41.410 --> 00:13:43.740
which is take now the induced subgraph,

00:13:43.740 --> 00:13:45.330
put it into the GPU memory,

00:13:45.330 --> 00:13:46.755
and create the update.

00:13:46.755 --> 00:13:48.900
But the point is now that the subgroups can be more

00:13:48.900 --> 00:13:52.245
diverse so the induced subgraph will be more diverse,

00:13:52.245 --> 00:13:56.255
so your gradients will be more stable and more, uh, representative.

00:13:56.255 --> 00:13:57.820
So here is the picture,

00:13:57.820 --> 00:14:01.510
the picture is that now my node groups are smaller than what I had before,

00:14:01.510 --> 00:14:03.265
but I can sample more of them.

00:14:03.265 --> 00:14:06.220
So I sample here two node groups and now

00:14:06.220 --> 00:14:12.685
the sample subgraph is the induced subgraph on the nodes belonging to the two groups.

00:14:12.685 --> 00:14:15.625
So I will also include, uh, this,

00:14:15.625 --> 00:14:19.300
uh, uh, blue, uh, edge that kind of connects the two groups.

00:14:19.300 --> 00:14:20.770
So it means that, um, uh,

00:14:20.770 --> 00:14:24.820
this makes the sample nodes more representative, more diverse.

00:14:24.820 --> 00:14:28.195
They are more representative of the entire node population,

00:14:28.195 --> 00:14:30.955
which will lead to better gradient estimate,

00:14:30.955 --> 00:14:32.844
less variance in gradients,

00:14:32.844 --> 00:14:36.265
and faster training, uh, and convergence.

00:14:36.265 --> 00:14:39.580
So, uh, just to say more, right?

00:14:39.580 --> 00:14:41.545
This is again a two-step approach.

00:14:41.545 --> 00:14:43.660
I take the nodes, um,

00:14:43.660 --> 00:14:47.095
I- I separate them into very small, uh,

00:14:47.095 --> 00:14:50.440
subgroups, much smaller than the original version, but, uh,

00:14:50.440 --> 00:14:52.285
when I'm doing mini-batch training,

00:14:52.285 --> 00:14:55.930
I will sample multiple groups of nodes, um,

00:14:55.930 --> 00:14:59.635
and then I will aggregate these groups of nodes into one super group,

00:14:59.635 --> 00:15:04.210
and then I'm going to create an induced subgraph on the entire, uh, super group.

00:15:04.210 --> 00:15:09.280
And then, um, I'm going to include the edges between all the members of the super group,

00:15:09.280 --> 00:15:12.085
which means it will be edges between the- the small, uh,

00:15:12.085 --> 00:15:16.825
subgroups as well as the edges between the small, uh, subgroups.

00:15:16.825 --> 00:15:19.540
Um, and then I will perfo- perform, uh,

00:15:19.540 --> 00:15:23.875
Cluster-GCN, uh, the same way as we did before.

00:15:23.875 --> 00:15:29.124
So now, um, if I compare the Cluster-GCN approach,

00:15:29.124 --> 00:15:31.285
with the neighborhood sampling approach,

00:15:31.285 --> 00:15:32.920
here is how the two compare.

00:15:32.920 --> 00:15:35.125
The neighborhood sampling says,

00:15:35.125 --> 00:15:36.655
uh, sample H, uh,

00:15:36.655 --> 00:15:38.230
nodes per layer, um,

00:15:38.230 --> 00:15:40.000
and let's do this for,

00:15:40.000 --> 00:15:41.620
uh, M nodes in the network.

00:15:41.620 --> 00:15:44.875
So the total size of the mini-batch will be

00:15:44.875 --> 00:15:49.690
M times H raised to the power of K. M is the number of computation graph,

00:15:49.690 --> 00:15:53.140
uh, H is the fan out of computation graphs,

00:15:53.140 --> 00:15:56.455
and K is the number of layers of the GNN.

00:15:56.455 --> 00:15:57.940
So for M, uh,

00:15:57.940 --> 00:15:59.965
nodes in the computation graph,

00:15:59.965 --> 00:16:05.665
our cost in terms of memory as well as computation will be M times, uh, H_K.

00:16:05.665 --> 00:16:10.000
In the Cluster-GCN, um, uh, the- uh,

00:16:10.000 --> 00:16:15.025
the- we are performing message passing over a subgraph induced by M nodes,

00:16:15.025 --> 00:16:18.490
and the subgraph over M nodes is going to

00:16:18.490 --> 00:16:22.615
include M times average degree number of edges, right?

00:16:22.615 --> 00:16:27.280
Each node in the subgraph has an average degree of, uh, D average,

00:16:27.280 --> 00:16:30.250
so in total, we- we'll have M times,

00:16:30.250 --> 00:16:31.870
uh, average degree, uh,

00:16:31.870 --> 00:16:33.595
number of edges in the graph.

00:16:33.595 --> 00:16:37.615
And because we are doing K hop, uh, message-passing,

00:16:37.615 --> 00:16:40.510
the computational cost of this approach will be

00:16:40.510 --> 00:16:45.895
K times M size of the subgraph times the average degree.

00:16:45.895 --> 00:16:49.090
So if you compare the two, in summary,

00:16:49.090 --> 00:16:55.735
the cost to generate embeddings from M nodes in a K layer GNN is M to the- M times H_K.

00:16:55.735 --> 00:16:58.675
In Cluster-GCN, it is, um,

00:16:58.675 --> 00:17:01.450
K times M times average degree,

00:17:01.450 --> 00:17:04.375
um, and, um, if you assume, let's say, uh,

00:17:04.375 --> 00:17:07.170
H to be half of the average degree,

00:17:07.170 --> 00:17:09.380
uh, then it would mean that, ah,

00:17:09.380 --> 00:17:12.465
Cluster-GCN is much more computationally efficient,

00:17:12.465 --> 00:17:14.410
um, than neighborhood sampling.

00:17:14.410 --> 00:17:17.000
So, um, it is linear ins- instead of

00:17:17.000 --> 00:17:20.465
exponential with respect to the- to the- to the depth.

00:17:20.465 --> 00:17:22.350
So what do we do in practice?

00:17:22.350 --> 00:17:24.680
It depends a bit on the- on the dataset.

00:17:24.680 --> 00:17:29.150
Usually, we set H to be bigger than the half average degree,

00:17:29.150 --> 00:17:30.980
uh, perhaps, you know, two times,

00:17:30.980 --> 00:17:34.205
three times, ah, average degree, um, and, uh,

00:17:34.205 --> 00:17:37.610
and because number of layers K is not- uh,

00:17:37.610 --> 00:17:39.430
is not that deep, um,

00:17:39.430 --> 00:17:43.460
the- the neighborhood sampling approach tends to be kind of more,

00:17:43.460 --> 00:17:45.945
uh, used, uh, in practice.

00:17:45.945 --> 00:17:51.159
So to summarize, Cluster-GCN first partitions the entire,

00:17:51.159 --> 00:17:54.260
um, set of nodes in the graph into small node groups.

00:17:54.260 --> 00:17:58.700
In each mini-batch, multiple node groups are sampled, um, and their,

00:17:58.700 --> 00:18:01.865
uh, and their nodes are kind of,

00:18:01.865 --> 00:18:03.980
um, uh, aggregated together.

00:18:03.980 --> 00:18:08.260
Then an induced subgraph on this, uh, uh, um,

00:18:08.260 --> 00:18:11.170
node- nodes, uh, from the union of

00:18:11.170 --> 00:18:14.755
the- of the groups that we have sampled, uh, is created.

00:18:14.755 --> 00:18:16.525
And then the GNN performs

00:18:16.525 --> 00:18:21.265
layer-wise node embeddings update over this induced, uh, subgraph.

00:18:21.265 --> 00:18:24.860
Generally, Cluster-GCN is more computationally efficient,

00:18:24.860 --> 00:18:30.110
than neighborhood sampling, especially when the number of layers is large, um,

00:18:30.110 --> 00:18:33.785
but Cluster-GCN leads to systematically biased gradients

00:18:33.785 --> 00:18:37.970
because of missing cross-community edges and also because,

00:18:37.970 --> 00:18:40.790
uh, if, uh, number of layers is deep,

00:18:40.790 --> 00:18:42.780
then in the original graph,

00:18:42.780 --> 00:18:44.390
um, if you do neighborhood sampling,

00:18:44.390 --> 00:18:45.675
you can really go deep.

00:18:45.675 --> 00:18:47.885
But in the Cluster-GCN,

00:18:47.885 --> 00:18:51.980
you are only going to go to the edge of the original of the sampled graph,

00:18:51.980 --> 00:18:53.390
and then you'll kind of bounce back.

00:18:53.390 --> 00:18:55.940
And even though you have a lot of depth,

00:18:55.940 --> 00:18:59.135
this depth would be kind of oscillating over the subgraph

00:18:59.135 --> 00:19:02.480
and won't even ec- really explore the real depth,

00:19:02.480 --> 00:19:05.315
uh, of the underlying original graph.

00:19:05.315 --> 00:19:07.670
So, uh, overall, um,

00:19:07.670 --> 00:19:13.500
I would say neighborhood sampling is used more because of additional, uh, flexibility.


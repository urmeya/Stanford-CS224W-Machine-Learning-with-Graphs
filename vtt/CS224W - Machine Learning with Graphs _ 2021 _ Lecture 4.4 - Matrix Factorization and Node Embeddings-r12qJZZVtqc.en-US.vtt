WEBVTT
Kind: captions
Language: en-US

00:00:03.980 --> 00:00:06.900
So this now concludes,

00:00:06.900 --> 00:00:08.640
uh, the first two parts of the lecture.

00:00:08.640 --> 00:00:10.305
And what I wanna do in the, uh,

00:00:10.305 --> 00:00:12.615
remaining, uh, few minutes,

00:00:12.615 --> 00:00:14.940
10 minutes or so, I wanna talk about

00:00:14.940 --> 00:00:18.495
connection to know the embeddings and matrix factorization.

00:00:18.495 --> 00:00:22.320
So, uh, let me refresh you what we talked,

00:00:22.320 --> 00:00:24.735
uh, what we talked, uh, on Tuesday.

00:00:24.735 --> 00:00:27.690
So we talked about embeddings and we talked

00:00:27.690 --> 00:00:30.930
about how we have this embedding matrix z that, you know,

00:00:30.930 --> 00:00:34.439
the number of rows is the e- embedding dimension,

00:00:34.439 --> 00:00:37.205
and the number of columns is the number of nodes,

00:00:37.205 --> 00:00:38.465
uh, in the graph.

00:00:38.465 --> 00:00:41.735
And this means that every column of this matrix z

00:00:41.735 --> 00:00:45.080
will store an embedding for that given node.

00:00:45.080 --> 00:00:50.615
And our objective was in this encoder-decoder framework is that we wanna maximize,

00:00:50.615 --> 00:00:54.310
um, the- the- the dot product between pairs of nodes,

00:00:54.310 --> 00:00:55.460
uh, that are similar.

00:00:55.460 --> 00:00:57.410
So if two nodes are similar, uh,

00:00:57.410 --> 00:01:00.260
then their dot product of their embeddings of

00:01:00.260 --> 00:01:03.835
their columns in this matrix z, uh, has to be high.

00:01:03.835 --> 00:01:06.315
And that was the- that was the idea.

00:01:06.315 --> 00:01:09.140
Then of course, how did you define this notion of similarity?

00:01:09.140 --> 00:01:14.495
We said two nodes are similar if they co-appear in the same random walk,

00:01:14.495 --> 00:01:16.600
uh, starting at the given node.

00:01:16.600 --> 00:01:18.930
So now, you know,

00:01:18.930 --> 00:01:20.910
how- how can we,

00:01:20.910 --> 00:01:22.985
uh, think about this more broadly, right?

00:01:22.985 --> 00:01:25.130
You could say we define the notion of

00:01:25.130 --> 00:01:27.860
similarity through random walks in the previous lecture.

00:01:27.860 --> 00:01:31.460
What if we define an even simpler notion of, uh, similarity?

00:01:31.460 --> 00:01:35.440
What if we say, two nodes are similar if they are connected by an edge.

00:01:35.440 --> 00:01:37.720
Then what this means is we say, "Oh,

00:01:37.720 --> 00:01:39.950
I want to approximate this matrix,

00:01:39.950 --> 00:01:42.920
say like entry in the mai- UV in the matrix,

00:01:42.920 --> 00:01:46.155
say this is either 0 or 1 by this dot product?"

00:01:46.155 --> 00:01:49.830
Like I'm saying, if two nodes are connect- u and v are connected,

00:01:49.830 --> 00:01:51.980
then I want their dot product to be 1.

00:01:51.980 --> 00:01:53.525
And if they are not connected,

00:01:53.525 --> 00:01:56.940
I want their dot product to be 0, right?

00:01:56.940 --> 00:02:01.760
Um, of course, like if I write it this way at the level of the single entry,

00:02:01.760 --> 00:02:04.715
if I write it in the- in the matrix form,

00:02:04.715 --> 00:02:06.200
this- I'm writing basically saying,

00:02:06.200 --> 00:02:10.025
this is Z trans- Z transposed times Z equals A.

00:02:10.025 --> 00:02:17.375
So this is now caused matrix factorization because I take my matrix A and I factorize it,

00:02:17.375 --> 00:02:20.380
I represent it as a product of,

00:02:20.380 --> 00:02:22.760
um- of, uh, two matrices.

00:02:22.760 --> 00:02:25.335
Um, Z- Z transposed and Z.

00:02:25.335 --> 00:02:28.655
So essentially I'm saying I take my adjacency matrix, uh, A.

00:02:28.655 --> 00:02:30.710
Here is, for example, one entry of it.

00:02:30.710 --> 00:02:33.140
I want- because there is an edge, I want, you know,

00:02:33.140 --> 00:02:37.880
the- the- the dot product of this row and that column to be value 1.

00:02:37.880 --> 00:02:40.280
For example, for, uh, for this entry,

00:02:40.280 --> 00:02:43.235
I would want the product of this,

00:02:43.235 --> 00:02:46.565
uh, row and this particular column to be 0.

00:02:46.565 --> 00:02:53.940
So this means that we take matrix A and factorize it as a product of Z transpose times Z.

00:02:53.950 --> 00:02:59.520
Of course, because embedding dimension D number of rows in Z,

00:02:59.520 --> 00:03:03.470
Z is much, much smaller than the number of, uh- uh, nodes, right?

00:03:03.470 --> 00:03:05.240
So the matrix is very wide,

00:03:05.240 --> 00:03:06.995
but not too- too deep.

00:03:06.995 --> 00:03:10.400
This means that this exact approximation saying

00:03:10.400 --> 00:03:15.270
A equals Z transpose times Z is generally not possible, right?

00:03:15.270 --> 00:03:17.265
We don't have enough representation power

00:03:17.265 --> 00:03:19.740
to really capture each edge but perfectly.

00:03:19.740 --> 00:03:23.645
So we can learn this martix Z approximately.

00:03:23.645 --> 00:03:25.235
So what we can see is,

00:03:25.235 --> 00:03:28.995
let's find the matrix Z such that, you know, the, uh,

00:03:28.995 --> 00:03:31.320
Z transpose times Z, uh,

00:03:31.320 --> 00:03:35.520
the values of it are as similar to the values of A as possible.

00:03:35.520 --> 00:03:38.600
How do I- how do I measure similarity now between

00:03:38.600 --> 00:03:41.735
values is to use what is called Frobenius norm,

00:03:41.735 --> 00:03:44.485
which is simply take an entry in A,

00:03:44.485 --> 00:03:46.680
subtract an entry in this,

00:03:46.680 --> 00:03:49.710
uh- in this matrix Z transpose times Z,

00:03:49.710 --> 00:03:52.245
um, and take the square value of it and sum it up.

00:03:52.245 --> 00:03:55.350
So Frobenius norm is simply, uh,

00:03:55.350 --> 00:03:57.405
a sum of the square differences, uh,

00:03:57.405 --> 00:04:01.935
between corresponding entries into, uh- into matrices.

00:04:01.935 --> 00:04:05.540
Right? So this is now very similar to what we were

00:04:05.540 --> 00:04:09.080
also doing in the previous, uh, lecture, right?

00:04:09.080 --> 00:04:10.350
In the node embeddings lecture.

00:04:10.350 --> 00:04:11.960
In the node embeddings lecture,

00:04:11.960 --> 00:04:15.710
we were not using the L2 norm to- to define

00:04:15.710 --> 00:04:20.105
the discrepancy between A and its factorization,

00:04:20.105 --> 00:04:23.180
we used the softmax,

00:04:23.180 --> 00:04:26.075
uh, uh, function instead of the L2 norm.

00:04:26.075 --> 00:04:31.710
But the goal of approximating A with Z transpose times Z was the same.

00:04:31.710 --> 00:04:37.730
So the conclusion is that the inner product decoder with note similarity defined by

00:04:37.730 --> 00:04:40.850
edge- edge connectivity is equivalent to

00:04:40.850 --> 00:04:44.570
the factorization of adjacency matrix, uh, A, right?

00:04:44.570 --> 00:04:47.510
So we say we wanna directly approximate A by

00:04:47.510 --> 00:04:50.780
the embeddings of the nodes such that if two nodes are linked,

00:04:50.780 --> 00:04:53.825
then I want their dot product to be equal to 1.

00:04:53.825 --> 00:04:55.100
And if they are not linked,

00:04:55.100 --> 00:04:57.775
I want their dot product to be equal to 0.

00:04:57.775 --> 00:05:02.095
So that's the, uh- the simplest way to define node similarity.

00:05:02.095 --> 00:05:05.240
Now, in random walk-based similarity,

00:05:05.240 --> 00:05:08.380
it turns out that when we have this more,

00:05:08.380 --> 00:05:11.485
um, nuanced, more complex definition of, uh,

00:05:11.485 --> 00:05:17.300
similarity, then, um, it is still- the entire process is still equivalent to

00:05:17.300 --> 00:05:24.060
matrix factorization of just- of a more complex or transformed, uh, adjacency matrix.

00:05:24.060 --> 00:05:26.130
So, um, here's the equation,

00:05:26.130 --> 00:05:29.235
let me explain it, uh, on the next slide, what does it mean.

00:05:29.235 --> 00:05:32.360
So it means that what we are really trying to do is

00:05:32.360 --> 00:05:35.660
we are trying to factorize this particular,

00:05:35.660 --> 00:05:39.495
um, transformed graph adjacency matrix.

00:05:39.495 --> 00:05:41.775
So how is the the- uh,

00:05:41.775 --> 00:05:43.440
what- what is the transformation?

00:05:43.440 --> 00:05:48.044
The transformation is- here is the, um, adjacency matrix.

00:05:48.044 --> 00:05:51.720
Here is the, um, one over the- the diagonal matrix,

00:05:51.720 --> 00:05:53.685
these are the node degrees.

00:05:53.685 --> 00:05:55.350
Um, this is now, uh,

00:05:55.350 --> 00:05:57.120
raised to the power r,

00:05:57.120 --> 00:06:00.585
where this- r goes between 1 and T where, uh,

00:06:00.585 --> 00:06:07.010
capital T is the context window length is actually the length of the random walks that,

00:06:07.010 --> 00:06:08.960
uh, we are simulating in,

00:06:08.960 --> 00:06:11.740
uh- in DeepWalk or, uh, node2vec.

00:06:11.740 --> 00:06:16.280
Um, volume of G is simply the sum of the entries of the adjacency matrix,

00:06:16.280 --> 00:06:18.790
we see- which is twice the number of edges.

00:06:18.790 --> 00:06:22.040
And the log b is a factor that corresponds to

00:06:22.040 --> 00:06:26.195
the number of negative samples we are using in the optimization problem.

00:06:26.195 --> 00:06:29.585
So basically, what this means is that you can compute,

00:06:29.585 --> 00:06:31.920
um- in this case,

00:06:31.920 --> 00:06:36.860
deep walk either by what we talked last time by simulating random walks

00:06:36.860 --> 00:06:42.720
and defining the gradients and doing gradient descent or taking the adjacency matrix,

00:06:42.720 --> 00:06:44.180
A, of your graph,

00:06:44.180 --> 00:06:47.585
transforming it according to this equation where

00:06:47.585 --> 00:06:50.930
we both take into account the lands of the random walks,

00:06:50.930 --> 00:06:53.515
as well as the number of negative samples we use.

00:06:53.515 --> 00:06:55.815
And if we factorize this, uh,

00:06:55.815 --> 00:06:59.900
transform matrix, what I mean by this is if we go and, uh,

00:06:59.900 --> 00:07:04.295
replace this A here with the transform matrix and then we try to solve,

00:07:04.295 --> 00:07:06.054
uh, this- this equation,

00:07:06.054 --> 00:07:07.875
uh, then the, uh, the solution to it,

00:07:07.875 --> 00:07:11.955
this matrix Z, will be exactly the same as what,

00:07:11.955 --> 00:07:13.485
uh- what we- what, uh,

00:07:13.485 --> 00:07:17.400
our approach that we discussed in the previous lecture arrived to.

00:07:17.400 --> 00:07:21.030
So, um, basically that is a very- very nice paper.

00:07:21.030 --> 00:07:24.450
Um, if you wanna know more about this called network embedding as

00:07:24.450 --> 00:07:28.410
matrix factorization that basically unifies DeepWalk LINE,

00:07:28.410 --> 00:07:32.405
um, and a lot of other algorithms including node2vec in this,

00:07:32.405 --> 00:07:35.675
uh, one mathematical, uh, framework.

00:07:35.675 --> 00:07:38.330
So, um, as I said,

00:07:38.330 --> 00:07:44.150
this random walk-based similarity can also be thought as, um, matrix factorization.

00:07:44.150 --> 00:07:46.490
The equation I show here is for DeepWalk.

00:07:46.490 --> 00:07:53.035
Um, you can derive similar type of matrix transformation for, uh, node2vec.

00:07:53.035 --> 00:07:58.325
Just the matrix is even more complex because the random walk process of node2vec is,

00:07:58.325 --> 00:07:59.675
uh- is more complex,

00:07:59.675 --> 00:08:02.255
is more, uh- more nuanced.

00:08:02.255 --> 00:08:05.045
So to conclude the lecture today,

00:08:05.045 --> 00:08:07.790
I wanna talk a bit about the limitations and kind of

00:08:07.790 --> 00:08:11.160
motivate what are we going to talk about next week.

00:08:11.160 --> 00:08:16.490
So the limitations of node embeddings via this matrix factorization or this random walks,

00:08:16.490 --> 00:08:21.030
uh, like- like I discussed in terms of, um,

00:08:21.030 --> 00:08:24.370
node2vec, DeepWalk for multidimensional embeddings,

00:08:24.370 --> 00:08:29.060
you can even think of PageRank as a single-dimensional embedding is that,

00:08:29.060 --> 00:08:33.020
um, we cannot obtain embeddings for nodes not in the training set.

00:08:33.020 --> 00:08:35.180
So this means if our graph is evolving,

00:08:35.180 --> 00:08:38.115
if new nodes are appearing over time, then, uh,

00:08:38.115 --> 00:08:40.770
the nodes that are not in the graph, uh,

00:08:40.770 --> 00:08:44.260
at the time when we are computing embeddings won't have an embedding.

00:08:44.260 --> 00:08:46.100
So if a newly added node 5,

00:08:46.100 --> 00:08:48.755
for example, arrives, let's say, at the test time,

00:08:48.755 --> 00:08:51.290
lets say- or is a new user in the social network,

00:08:51.290 --> 00:08:53.390
we can- we cannot compute its embedding,

00:08:53.390 --> 00:08:56.090
we have to redo everything from scratch.

00:08:56.090 --> 00:08:59.570
We have to recompute all embeddings of all nodes in the network.

00:08:59.570 --> 00:09:02.065
So this is very limiting.

00:09:02.065 --> 00:09:04.890
The second important thing is that this, uh,

00:09:04.890 --> 00:09:09.450
embeddings cannot capture structural similarity.

00:09:09.450 --> 00:09:11.390
Uh, the reason being is, for example,

00:09:11.390 --> 00:09:14.795
if I have the graph like I show here and I consider nodes,

00:09:14.795 --> 00:09:16.300
uh, 1 and 11.

00:09:16.300 --> 00:09:19.260
Even though they are in very different parts of the graph,

00:09:19.260 --> 00:09:21.405
their local network structure, uh,

00:09:21.405 --> 00:09:24.210
looks- looks quite similar.

00:09:24.210 --> 00:09:27.410
Um, and DeepWalk and node2vec will come up with

00:09:27.410 --> 00:09:31.895
very different embeddings for 11 and- and 1 because,

00:09:31.895 --> 00:09:34.555
you know, 11 is neighbor with 12 and 13,

00:09:34.555 --> 00:09:37.265
while 1 is neighbor with 2 and 3.

00:09:37.265 --> 00:09:41.540
So this means that they- these types of embeddings

00:09:41.540 --> 00:09:45.455
won't be able to capture this notion of local structural similarity,

00:09:45.455 --> 00:09:50.570
but it will- more be able to capture who- who- what are the identities of the neighbors,

00:09:50.570 --> 00:09:53.525
uh, next, uh- next to a given starting node.

00:09:53.525 --> 00:09:57.520
Of course, if you were to define these over anonymous walks,

00:09:57.520 --> 00:10:01.890
then, um, that would cap- allow us to capture the structure because, uh,

00:10:01.890 --> 00:10:06.605
2 and 3, um, would- would- basically the identities of the nodes,

00:10:06.605 --> 00:10:09.305
um, uh, are forgotten, and then we would be able to,

00:10:09.305 --> 00:10:11.920
uh, solve this, uh, problem.

00:10:11.920 --> 00:10:16.265
And then the last limitation I wanna talk about is- is that

00:10:16.265 --> 00:10:20.465
this approaches cannot utilize node edge in graph level features,

00:10:20.465 --> 00:10:24.370
meaning, you know, feature vectors attached to nodes,

00:10:24.370 --> 00:10:28.810
uh, edges and graphs cannot naturally be incorporated in this framework.

00:10:28.810 --> 00:10:31.475
Right? We basically create node embedding separately

00:10:31.475 --> 00:10:34.440
from the features that these nodes might have.

00:10:34.440 --> 00:10:35.870
So for example, you know, I use it in

00:10:35.870 --> 00:10:38.495
a social network may have a set of properties, features,

00:10:38.495 --> 00:10:41.690
attributes where are protein has a set of properties,

00:10:41.690 --> 00:10:45.600
features that you would want to use them in creating, uh, embeddings.

00:10:45.600 --> 00:10:48.480
And really, what are we going to talk next is,

00:10:48.480 --> 00:10:51.695
um, you know, what are the solutions to these limitations?

00:10:51.695 --> 00:10:56.330
The solution for- to these limitations is deep representation learning and,

00:10:56.330 --> 00:10:57.710
uh, graph neural networks.

00:10:57.710 --> 00:10:58.940
And in the next week,

00:10:58.940 --> 00:11:00.330
we are going to move, uh,

00:11:00.330 --> 00:11:01.815
to the topic of, uh,

00:11:01.815 --> 00:11:04.430
graph neural networks that will allow us to

00:11:04.430 --> 00:11:07.530
resolve these limitations that I have just, uh,

00:11:07.530 --> 00:11:10.330
discussed, and will allow us to fuse the feature

00:11:10.330 --> 00:11:13.880
information together with the structured information.

00:11:13.880 --> 00:11:16.295
So to summarize today's lecture,

00:11:16.295 --> 00:11:20.425
we talked about PageRank that measures important soft nodes in a graph.

00:11:20.425 --> 00:11:23.720
Uh, we talked about it in three different ways,

00:11:23.720 --> 00:11:27.545
we talked about it in terms of flow formulation,

00:11:27.545 --> 00:11:29.330
in terms of links, and nodes.

00:11:29.330 --> 00:11:31.355
We talked about it in terms of

00:11:31.355 --> 00:11:35.080
random walk and stationary distribution of a random walk process.

00:11:35.080 --> 00:11:38.495
And we also talked about it from the linear algebra point of view, uh,

00:11:38.495 --> 00:11:41.690
by basically computing the eigenvector, uh,

00:11:41.690 --> 00:11:43.880
to the particularly transformed,

00:11:43.880 --> 00:11:45.590
uh, graph adjacency matrix.

00:11:45.590 --> 00:11:48.180
Then we talked about, um,

00:11:48.180 --> 00:11:51.290
extensions of PageRank particular random walk

00:11:51.290 --> 00:11:53.450
with restarts and personalized PageRank where

00:11:53.450 --> 00:11:59.790
basically the difference is in terms of changing the teleportation set.

00:11:59.790 --> 00:12:02.645
Um, and then the last part of the lecture,

00:12:02.645 --> 00:12:06.200
we talked about node embeddings based on random walks and

00:12:06.200 --> 00:12:10.210
how they can be rep- expressed as a form of matrix factorization.

00:12:10.210 --> 00:12:15.740
So this means that viewing graphs as matrices is a-plays

00:12:15.740 --> 00:12:18.695
a key role in all of the above algorithms

00:12:18.695 --> 00:12:21.890
where we can think of this in many different ways.

00:12:21.890 --> 00:12:23.270
But mathematically at the end,

00:12:23.270 --> 00:12:27.920
it's all about mat- matrix representation of the graph, factorizing this matrix,

00:12:27.920 --> 00:12:32.300
computing eigenvectors, eigenvalues, and extracting connectivity information,

00:12:32.300 --> 00:12:36.850
uh, out of it with the tools of, uh, linear algebra.

00:12:36.850 --> 00:12:39.540
So, um, thank you very much,

00:12:39.540 --> 00:12:43.089
everyone, for the- for the lecture.


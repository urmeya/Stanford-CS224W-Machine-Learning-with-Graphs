WEBVTT
Kind: captions
Language: en-US

00:00:04.070 --> 00:00:07.950
So we are going to talk about random walk approaches,

00:00:07.950 --> 00:00:09.390
uh, to node embeddings.

00:00:09.390 --> 00:00:12.615
Um, and the idea here will be the following.

00:00:12.615 --> 00:00:16.410
Uh, we are going to learn a vector z for every node

00:00:16.410 --> 00:00:20.565
and this would be an embedding of the node and this is what we aim to find.

00:00:20.565 --> 00:00:22.770
We are then going to, um,

00:00:22.770 --> 00:00:26.745
also define a probability that, uh, basically will- uh,

00:00:26.745 --> 00:00:31.740
will be the pre- predicted probability of how similar a given node u,

00:00:31.740 --> 00:00:34.155
um, is uh, to some node, uh,

00:00:34.155 --> 00:00:38.520
v. And given that we are going to use random walks to define this similarity,

00:00:38.520 --> 00:00:40.410
this would be the probab- proba- uh,

00:00:40.410 --> 00:00:42.910
predicted probability of visiting node v,

00:00:42.910 --> 00:00:46.030
one random walks starting from node u.

00:00:46.030 --> 00:00:49.980
Then we are also goin- need to, um,

00:00:49.980 --> 00:00:53.580
nonlinear functions, uh, that will be used to,

00:00:53.580 --> 00:00:55.940
uh, define or to produce these probabilities.

00:00:55.940 --> 00:01:00.070
First, I'm going to define the notion of a softmax function,

00:01:00.070 --> 00:01:01.620
which- which is- uh,

00:01:01.620 --> 00:01:05.430
which returns a vector of k real values,

00:01:05.430 --> 00:01:09.320
uh, um, and the- and these values sum to one.

00:01:09.320 --> 00:01:12.380
So essentially, given a set of numbers z,

00:01:12.380 --> 00:01:15.230
the- the softmax of that vector will be

00:01:15.230 --> 00:01:20.035
a probability distribution over those values and the more likely that,

00:01:20.035 --> 00:01:23.015
um, that- number is the maximum in the vector,

00:01:23.015 --> 00:01:24.620
the higher the probability,

00:01:24.620 --> 00:01:25.745
uh, it will have.

00:01:25.745 --> 00:01:28.445
And essentially the way you can think of it, I take this, uh,

00:01:28.445 --> 00:01:33.650
value z and I exponentiate them and then I normalize everything to sum to one.

00:01:33.650 --> 00:01:36.810
So the idea is if the largest value, um,

00:01:36.810 --> 00:01:38.100
in- in this vector z,

00:01:38.100 --> 00:01:39.870
when I expo- exponentiate it,

00:01:39.870 --> 00:01:42.105
it will be even larger than everything else.

00:01:42.105 --> 00:01:43.950
So most probability mass,

00:01:43.950 --> 00:01:46.785
um, will be concentrated on that value.

00:01:46.785 --> 00:01:49.070
This is why this is called softmax because it's

00:01:49.070 --> 00:01:52.040
a kind of a soft version of a maximum function.

00:01:52.040 --> 00:01:56.000
Um, and then we are also going to define this notion of a sigmoid which is an

00:01:56.000 --> 00:01:59.960
S shaped function that turns real values into,

00:01:59.960 --> 00:02:02.195
uh, a range of- of 01,

00:02:02.195 --> 00:02:08.410
um, and softmax is defined as 1 over 1 plus e to the minus x. Um,

00:02:08.410 --> 00:02:11.535
and this is a nice way how to take something that lives on, uh,

00:02:11.535 --> 00:02:14.750
minus infinity to plus infinity and kind of squish it,

00:02:14.750 --> 00:02:17.170
uh, to, uh, value 01.

00:02:17.170 --> 00:02:19.750
So that's- those are two important functions,

00:02:19.750 --> 00:02:21.500
uh, I will- we will use.

00:02:21.500 --> 00:02:24.005
Now let me define the notion of a random walk.

00:02:24.005 --> 00:02:26.120
So a random walk is simply,

00:02:26.120 --> 00:02:28.790
um, a process on top of the graph,

00:02:28.790 --> 00:02:31.700
where we sa- let say start at some node and then out

00:02:31.700 --> 00:02:34.550
of the outgoing neighbors of that node,

00:02:34.550 --> 00:02:36.460
in this case it will be 1, 3, and 5,

00:02:36.460 --> 00:02:41.870
we pick one at random and we move to it and this is one step of random- random walk.

00:02:41.870 --> 00:02:43.115
Now we are in this,

00:02:43.115 --> 00:02:44.390
uh, node 5, again,

00:02:44.390 --> 00:02:46.970
we have four different ways in which we can go.

00:02:46.970 --> 00:02:48.290
We can return back to four.

00:02:48.290 --> 00:02:53.170
We can go to 8, 6 or 7 and we pick one of them at random and move there.

00:02:53.170 --> 00:02:55.515
Um, and this process continues,

00:02:55.515 --> 00:02:58.330
let say for a- for a- in this case for a fixed,

00:02:58.330 --> 00:02:59.940
uh, number of steps.

00:02:59.940 --> 00:03:03.530
So the way you can think of this is that we basically simulated this, uh,

00:03:03.530 --> 00:03:06.320
random walk over- over this graph and let's say,

00:03:06.320 --> 00:03:08.000
um, over our fixed, uh,

00:03:08.000 --> 00:03:11.840
number of steps where the random walk can traverse the same edge multiple times,

00:03:11.840 --> 00:03:14.560
can return, can go back and forth,

00:03:14.560 --> 00:03:16.770
do, uh, whatever the random walk,

00:03:16.770 --> 00:03:18.345
uh, wants to do.

00:03:18.345 --> 00:03:23.480
All right. And this random walk is a seq- sequence of nodes visited this way,

00:03:23.480 --> 00:03:27.430
uh, on a graph across the- across the edges.

00:03:27.430 --> 00:03:30.890
So now how are we going to- to define

00:03:30.890 --> 00:03:34.000
this notion of similarity and these probabilities that we talked about?

00:03:34.000 --> 00:03:36.140
What we are going to do is to say we want to learn these

00:03:36.140 --> 00:03:39.170
coordinate z such that the product of two,

00:03:39.170 --> 00:03:41.225
uh, nodes u and v,

00:03:41.225 --> 00:03:44.075
um, is similar or equals is, uh,

00:03:44.075 --> 00:03:49.465
approximates the probability that u and v co-occur on a random walk, uh, over the graph.

00:03:49.465 --> 00:03:51.765
So here is- here is the idea, right?

00:03:51.765 --> 00:03:54.260
First we will need to estimate the probability of visiting

00:03:54.260 --> 00:03:56.790
node v on a random walk starting,

00:03:56.790 --> 00:03:59.220
uh, at some node u using some, let's say,

00:03:59.220 --> 00:04:01.550
a random walk strategy R. I'm going to define

00:04:01.550 --> 00:04:04.220
this notion of random walks strategy, uh, later,

00:04:04.220 --> 00:04:07.700
but for now just think it's a simple random walk where we pick one of the, uh,

00:04:07.700 --> 00:04:11.710
uh, neighbors uniformly at random and we move to it.

00:04:11.710 --> 00:04:14.670
And then we wanna optimize the embeddings,

00:04:14.670 --> 00:04:17.980
uh, in such a way to encode this random walk statistics.

00:04:17.980 --> 00:04:22.970
Basically we want the- the cosine of the angle between the two vectors,

00:04:22.970 --> 00:04:27.950
this is the dot product to be proportional or similar to the probability that,

00:04:27.950 --> 00:04:30.590
uh, u and v are visited, uh,

00:04:30.590 --> 00:04:34.355
uh, on the same random, uh, walk.

00:04:34.355 --> 00:04:36.240
So why random walks?

00:04:36.240 --> 00:04:40.655
We want to use random walks because they are expressive, they are flexible.

00:04:40.655 --> 00:04:44.570
It gives us a flexible stochastic definition of node similarity that

00:04:44.570 --> 00:04:49.500
incorporates both kind of local as well as higher order neighbor with information, right?

00:04:49.500 --> 00:04:51.680
And the idea is that if a random walk, uh,

00:04:51.680 --> 00:04:54.195
starting from node u visits v, um,

00:04:54.195 --> 00:04:57.690
with high probability that u and v are similar, uh, uh,

00:04:57.690 --> 00:05:01.530
they have kind of similar network neighborhood they are close together with each other,

00:05:01.530 --> 00:05:04.410
there might be multiple paths between them and so on.

00:05:04.410 --> 00:05:08.930
Um, and what is interesting is that this is in some sense also

00:05:08.930 --> 00:05:13.280
efficient because we do not need to consider all the node pairs when- when training.

00:05:13.280 --> 00:05:17.570
We only need to consider pairs that co-occur in random walks.

00:05:17.570 --> 00:05:20.404
[BACKGROUND] So the supervised,

00:05:20.404 --> 00:05:21.890
uh, feature learning, uh,

00:05:21.890 --> 00:05:23.000
will work the following.

00:05:23.000 --> 00:05:25.325
The intuition is that we find embedding of nodes in

00:05:25.325 --> 00:05:28.375
d-dimensional space that preserves similarity.

00:05:28.375 --> 00:05:32.520
Uh, the idea is that we want to learn node embedding such that nearby nodes in

00:05:32.520 --> 00:05:36.980
the network are clo- are- are embedded close together in the embedding space.

00:05:36.980 --> 00:05:39.800
Um, and given a node u the question is,

00:05:39.800 --> 00:05:41.605
how do we define nearby?

00:05:41.605 --> 00:05:45.210
And we are going to have these definition and,

00:05:45.210 --> 00:05:47.355
uh, sub r of u, where, uh,

00:05:47.355 --> 00:05:50.820
basically this is n labels the neighborhood,

00:05:50.820 --> 00:05:54.750
uh, of u obtained by some random walk strategy or r, right?

00:05:54.750 --> 00:05:56.280
So for a given node uh,

00:05:56.280 --> 00:05:58.830
u we need to define what is the neighborhood?

00:05:58.830 --> 00:06:03.080
And in our case, neighborhood will simply be a sequence of nodes that this,

00:06:03.080 --> 00:06:08.055
um, uh, that the random walk starting at u has visited.

00:06:08.055 --> 00:06:12.005
So now how are we setting this up as an optimization problem?

00:06:12.005 --> 00:06:13.385
Given the graph, uh,

00:06:13.385 --> 00:06:16.810
on nodes, uh, V and an edge set E,

00:06:16.810 --> 00:06:20.450
our goal is to learn a mapping from the nodes, uh,

00:06:20.450 --> 00:06:24.170
to their embeddings and we are going to maximize the following,

00:06:24.170 --> 00:06:26.195
uh, maximum likelihood objective, right?

00:06:26.195 --> 00:06:28.460
Our goal will be to find this function,

00:06:28.460 --> 00:06:32.730
this mapping so basically find the coordinates z of the nodes such that,

00:06:32.730 --> 00:06:35.305
uh, the summation over all the nodes, uh,

00:06:35.305 --> 00:06:39.925
of log probabilities that given the node u, um, that, uh,

00:06:39.925 --> 00:06:44.450
maximizes log probabilities of the nodes that appear in its- uh,

00:06:44.450 --> 00:06:48.210
in its local uh, random-walk neighborhood, right?

00:06:48.210 --> 00:06:51.375
So we want to basically to sum out- to maximize the sum,

00:06:51.375 --> 00:06:54.560
which means we want to make nodes that are, um,

00:06:54.560 --> 00:06:58.399
that are visited in the same random walk to be kind of embedded,

00:06:58.399 --> 00:06:59.910
uh, close together, right?

00:06:59.910 --> 00:07:02.330
So we wanna learn feature representations that are

00:07:02.330 --> 00:07:05.225
predictive of the nodes in each, uh, uh, uh,

00:07:05.225 --> 00:07:07.490
of the nodes that appear in it's, uh,

00:07:07.490 --> 00:07:09.365
random walk neighborhood uh, uh,

00:07:09.365 --> 00:07:12.295
N. That's the idea.

00:07:12.295 --> 00:07:14.730
So how are we going to do this?

00:07:14.730 --> 00:07:17.560
First, we are going to run short fixed length

00:07:17.560 --> 00:07:21.500
random walk starting from each node u in the graph using,

00:07:21.500 --> 00:07:23.600
uh, some random walks strategy R. Uh,

00:07:23.600 --> 00:07:25.025
for each node u,

00:07:25.025 --> 00:07:26.570
we are going to collect, uh,

00:07:26.570 --> 00:07:29.300
N of u which is a multi set of nodes

00:07:29.300 --> 00:07:32.510
visited in random walk starting from node u. Multi-set

00:07:32.510 --> 00:07:35.030
meaning that a same node can appear

00:07:35.030 --> 00:07:38.620
multiple times in the neighborhood because it may be visited multiple times.

00:07:38.620 --> 00:07:41.450
Um, and then we are going to optimize- define

00:07:41.450 --> 00:07:45.350
an optimization problem and optimize the embedding, so that, uh,

00:07:45.350 --> 00:07:48.350
given node u we wanna be able to predict who are

00:07:48.350 --> 00:07:52.280
the nodes that are in its neighborhood and defined again by the random walk.

00:07:52.280 --> 00:07:55.460
So we are going to maximize, uh, uh, this,

00:07:55.460 --> 00:07:56.930
uh, objective here, uh,

00:07:56.930 --> 00:07:59.155
this maximum likelihood objective.

00:07:59.155 --> 00:08:01.190
So how can we write this out?

00:08:01.190 --> 00:08:02.480
We can write this out the following.

00:08:02.480 --> 00:08:06.470
We- we- we write it as sum over all the starting nodes u,

00:08:06.470 --> 00:08:10.050
sum over all the nodes that are in the neighborhood of u.

00:08:10.050 --> 00:08:14.360
Let's call this nodes v and then we wanna maximize the log probability,

00:08:14.360 --> 00:08:15.950
uh, that predicts, uh,

00:08:15.950 --> 00:08:17.540
that node v, um,

00:08:17.540 --> 00:08:20.640
is in the neighborhood of node U. Um,

00:08:20.640 --> 00:08:24.230
and as I said opt- intuition is that we want to optimize embeddings to

00:08:24.230 --> 00:08:27.890
maximize the likelihood of a random walk, uh, co-occurrence.

00:08:27.890 --> 00:08:30.335
Um, how are we going to do this?

00:08:30.335 --> 00:08:33.315
Um, we still need to define this, uh,

00:08:33.315 --> 00:08:37.370
probability p and the way we are going to define it is we are going to use

00:08:37.370 --> 00:08:41.915
the notion of softmax function that I have introduced a couple of slides ago.

00:08:41.915 --> 00:08:46.025
So the idea here will be that what we wanna do is we wanna maximize

00:08:46.025 --> 00:08:52.250
the dot product between the node u and node v. So node u is the starting node,

00:08:52.250 --> 00:08:53.630
node v is the node,

00:08:53.630 --> 00:08:55.525
um, in the neighborhood.

00:08:55.525 --> 00:08:57.510
Random walk neighborhood of node, uh,

00:08:57.510 --> 00:09:01.890
u we wanna maxima- we wanna to apply softf- softmax.

00:09:01.890 --> 00:09:06.920
So this is the exponentiated value of the dot product of the node that is in

00:09:06.920 --> 00:09:09.170
the neighborhood divided by sum of

00:09:09.170 --> 00:09:13.520
the exponential dot product with all the other nodes in the network, right?

00:09:13.520 --> 00:09:18.180
So the idea here is that we wanna assign as much probability mass to- uh,

00:09:18.180 --> 00:09:20.550
to these dot product um,

00:09:20.550 --> 00:09:24.690
and as little to all other, uh, dot products.

00:09:24.690 --> 00:09:27.460
So now to write- to put it all together,

00:09:27.460 --> 00:09:31.660
the way we can think of this is- this is- we are trying to optimize this function,

00:09:31.660 --> 00:09:34.795
which is a sum over all the nodes for every node,

00:09:34.795 --> 00:09:38.410
sum over all the nodes v that are seen on

00:09:38.410 --> 00:09:42.745
random walks starting from this node u and then we wanna, uh, uh, uh,

00:09:42.745 --> 00:09:48.145
optimize for a minus log probability of these softmax which says,

00:09:48.145 --> 00:09:49.410
I want to, uh,

00:09:49.410 --> 00:09:53.830
maximize the dot product between the- the starting node u and

00:09:53.830 --> 00:09:58.360
the node v that is in the neighborhood and we- and we normalize this over all the nodes,

00:09:58.360 --> 00:09:59.680
uh, in the network.

00:09:59.680 --> 00:10:04.140
So now, um, you know what does it mean optimizing random walk embeddings,

00:10:04.140 --> 00:10:07.070
it means finding this coordinates z, uh,

00:10:07.070 --> 00:10:11.595
used here such that this likelihood function is, uh, minimized.

00:10:11.595 --> 00:10:14.035
Now, uh, the question is,

00:10:14.035 --> 00:10:16.735
how do we do this in practice?

00:10:16.735 --> 00:10:20.725
And the problem is that this is very expensive because if you look at this,

00:10:20.725 --> 00:10:23.530
we actually have to sum- two nested summations,

00:10:23.530 --> 00:10:25.450
uh, over all nodes of the network.

00:10:25.450 --> 00:10:27.880
We sum over all nodes in the- of- of the network

00:10:27.880 --> 00:10:30.625
here for starting nodes of the random walks.

00:10:30.625 --> 00:10:33.355
And here when we are normalizing softmax,

00:10:33.355 --> 00:10:37.120
we are normalizing it over all of the nodes of the network again.

00:10:37.120 --> 00:10:39.205
So this is a double summation,

00:10:39.205 --> 00:10:43.030
which means that it will have complexity order, um, V squared.

00:10:43.030 --> 00:10:46.060
So it will be, uh, quadratic in the number of nodes in the network,

00:10:46.060 --> 00:10:47.965
and that's prohibitively expensive.

00:10:47.965 --> 00:10:49.720
So let me tell you what, uh,

00:10:49.720 --> 00:10:52.585
we do to make this, uh, practical.

00:10:52.585 --> 00:10:58.330
And the- the issue here is that there is this problem with the softmax that, um,

00:10:58.330 --> 00:11:01.360
we need to sum over all the nodes to basically

00:11:01.360 --> 00:11:04.795
normalize it back to- to a distribution over the nodes.

00:11:04.795 --> 00:11:07.660
So, um, can we approximate this theorem?

00:11:07.660 --> 00:11:09.310
And the answer is yes.

00:11:09.310 --> 00:11:12.355
And the solution to this is called negative sampling.

00:11:12.355 --> 00:11:17.740
And intuitively, the idea is that rather than summing here over all the nodes,

00:11:17.740 --> 00:11:22.630
uh, in the network, we are only going to sum over a subset of the nodes.

00:11:22.630 --> 00:11:25.420
So essentially, we are going to sample a couple of

00:11:25.420 --> 00:11:28.660
negative examples and sum, uh, over them.

00:11:28.660 --> 00:11:33.510
So the way the approximation works out is that, um, we, um,

00:11:33.510 --> 00:11:37.135
we can- we can view this as an approximation to the, um,

00:11:37.135 --> 00:11:40.630
to the- to the softmax function, where we can,

00:11:40.630 --> 00:11:42.340
uh, approximate it using,

00:11:42.340 --> 00:11:44.560
uh, the following, uh, expression.

00:11:44.560 --> 00:11:50.020
We are going to take log sigmoid function of the dot product between u and v. Uh,

00:11:50.020 --> 00:11:52.480
this is for the, uh, for the theorem here.

00:11:52.480 --> 00:11:57.400
And then we are going to say minus sum of i going from one to k,

00:11:57.400 --> 00:12:01.120
so this is our k negative examples, logarithm, again,

00:12:01.120 --> 00:12:03.235
of the sigmoid function between the, uh,

00:12:03.235 --> 00:12:05.095
st- starting node u,

00:12:05.095 --> 00:12:07.390
and the negative, um,

00:12:07.390 --> 00:12:09.160
negative sample, uh, i,

00:12:09.160 --> 00:12:10.690
where this negative samples,

00:12:10.690 --> 00:12:12.895
this negative nodes will be, uh,

00:12:12.895 --> 00:12:17.470
sampled at random, but not at ra- at uniform random,

00:12:17.470 --> 00:12:19.330
but random in a biased way.

00:12:19.330 --> 00:12:21.110
So the idea here is that,

00:12:21.110 --> 00:12:25.125
instead of normalizing with respect to all nodes in the network,

00:12:25.125 --> 00:12:30.840
we are going to normalize softmax against k random negative samples,

00:12:30.840 --> 00:12:33.195
so negative nodes, uh, from the network.

00:12:33.195 --> 00:12:36.795
And this negative samples will be carefully chosen.

00:12:36.795 --> 00:12:40.395
So how do we choose negative samples?

00:12:40.395 --> 00:12:43.260
We- we sample k negative nodes,

00:12:43.260 --> 00:12:46.680
each with probabil- probability proportional to its degrees.

00:12:46.680 --> 00:12:49.400
So it means that nodes that have higher degree,

00:12:49.400 --> 00:12:52.960
uh, will be more likely to be chosen as a negative sample.

00:12:52.960 --> 00:12:57.400
Um, there are two considerations for picking k in practice,

00:12:57.400 --> 00:12:59.260
which means number of negative samples.

00:12:59.260 --> 00:13:02.965
Higher values of k will give me more robust estimate.

00:13:02.965 --> 00:13:06.850
Uh, but higher values of K also correspond, uh, to,

00:13:06.850 --> 00:13:08.305
uh, to more, uh,

00:13:08.305 --> 00:13:11.380
to more sampling again, to higher bias on negative events.

00:13:11.380 --> 00:13:15.790
So what people tend to choose in practice is k between 5 and 20.

00:13:15.790 --> 00:13:16.990
And if you think about it,

00:13:16.990 --> 00:13:18.820
this is a very small number.

00:13:18.820 --> 00:13:22.330
If you think network of a million nodes or 100,000 nodes,

00:13:22.330 --> 00:13:24.730
rather than summing over 100,000,

00:13:24.730 --> 00:13:27.010
uh, nodes, uh, every time here,

00:13:27.010 --> 00:13:28.960
you are only summing over, you know,

00:13:28.960 --> 00:13:30.880
5-20 nodes in this case.

00:13:30.880 --> 00:13:34.030
And this way, your method and your estimation will be far,

00:13:34.030 --> 00:13:37.315
um, much, much, much more, uh, efficient.

00:13:37.315 --> 00:13:41.530
So, um, now how do we solve this optimization problem?

00:13:41.530 --> 00:13:43.930
Uh, I won't go into too much detail,

00:13:43.930 --> 00:13:46.585
but these things are today solved with,

00:13:46.585 --> 00:13:48.415
uh, stochastic gradient descent.

00:13:48.415 --> 00:13:53.275
And I just want to give you a quick introduction to stochastic gradient descent, uh,

00:13:53.275 --> 00:13:56.800
two slides that are great lectures, uh,

00:13:56.800 --> 00:14:00.790
a lot of really good tutorials on what is stochastic gradient descent,

00:14:00.790 --> 00:14:03.910
how does it work and all the theoretical analysis of it.

00:14:03.910 --> 00:14:07.195
But essentially, the idea is that if you have a smooth function,

00:14:07.195 --> 00:14:11.260
then you can optimizing- optimize it by doing gradient descent,

00:14:11.260 --> 00:14:15.880
by basically computing the gradient at- at a given point and then moving, um, uh, for,

00:14:15.880 --> 00:14:17.890
uh, as a small step, um,

00:14:17.890 --> 00:14:20.800
in- in the direction opposite of the gradient, right?

00:14:20.800 --> 00:14:23.290
So this is the- this is the idea here, right?

00:14:23.290 --> 00:14:25.030
Your start at some random point.

00:14:25.030 --> 00:14:26.620
Um, in our case,

00:14:26.620 --> 00:14:29.050
we can initialize embeddings of nodes,

00:14:29.050 --> 00:14:31.555
uh, at- with random numbers,

00:14:31.555 --> 00:14:33.595
and then we iterate until we converge.

00:14:33.595 --> 00:14:35.440
We computed the derivative of

00:14:35.440 --> 00:14:39.520
the likelihood function with respect to the embedding of a single node,

00:14:39.520 --> 00:14:43.180
and now we find the direction of the derivative of the gradient.

00:14:43.180 --> 00:14:44.470
And then we make, uh,

00:14:44.470 --> 00:14:47.890
a step in the opposite direction of that gradient,

00:14:47.890 --> 00:14:50.950
where, um, uh, this is the learning rate,

00:14:50.950 --> 00:14:52.900
which means how big step do we take.

00:14:52.900 --> 00:14:55.240
And we can actually even tune the step as we make,

00:14:55.240 --> 00:14:56.890
uh, as we go and solve.

00:14:56.890 --> 00:15:00.505
Uh, but essentially, this is what gradient descent is.

00:15:00.505 --> 00:15:02.740
And in stochastic gradient descent,

00:15:02.740 --> 00:15:05.770
we are approximating the gradient in a stochastic way.

00:15:05.770 --> 00:15:10.570
So rather than evaluating the gradient over all the examples, we just do it, um,

00:15:10.570 --> 00:15:15.640
uh, over, uh, a small batch of examples or over an individual examples.

00:15:15.640 --> 00:15:19.780
So what does it mean? Is that rather than, um, uh, evaluating,

00:15:19.780 --> 00:15:24.280
uh, the gradient over all the nodes- all the negative nodes, um,

00:15:24.280 --> 00:15:28.450
and- or all the neighbors in the neighborhood of a given node,

00:15:28.450 --> 00:15:30.205
and then make a- make a step,

00:15:30.205 --> 00:15:32.335
we are going to do this only for a, uh,

00:15:32.335 --> 00:15:33.670
for a given, uh,

00:15:33.670 --> 00:15:35.290
for a given node in the neighborhood.

00:15:35.290 --> 00:15:36.970
So basically, the idea is that, you know,

00:15:36.970 --> 00:15:38.275
we'll sample node i,

00:15:38.275 --> 00:15:39.655
and then for all, uh,

00:15:39.655 --> 00:15:41.740
js that in the- in the,

00:15:41.740 --> 00:15:42.880
uh, in the neighborhood,

00:15:42.880 --> 00:15:45.295
we are going to compute the gradient, uh,

00:15:45.295 --> 00:15:48.355
and then make a step and keep iterating this.

00:15:48.355 --> 00:15:49.630
Uh, we- of course, we'll get

00:15:49.630 --> 00:15:52.840
a stochastic estimates or kind of a random estimate of the gradient.

00:15:52.840 --> 00:15:55.735
But we'll be able to up- to make updates much, much faster,

00:15:55.735 --> 00:15:57.070
which in practice tends,

00:15:57.070 --> 00:15:59.305
uh, to be much, uh, better.

00:15:59.305 --> 00:16:01.585
So let me summarize.

00:16:01.585 --> 00:16:03.190
We are going to run th-

00:16:03.190 --> 00:16:07.210
a short fixed-length random walks starting from each node on the graph.

00:16:07.210 --> 00:16:09.025
Uh, for each node u,

00:16:09.025 --> 00:16:11.245
we are going to collect, um,

00:16:11.245 --> 00:16:13.960
its neighborhood and as a multiset of

00:16:13.960 --> 00:16:16.540
nodes visited on the random walks starting from node u.

00:16:16.540 --> 00:16:18.940
And then we are going to optimize this embeddings

00:16:18.940 --> 00:16:22.030
used- using stochastic gradient descent, which means, uh,

00:16:22.030 --> 00:16:23.575
we are going to, uh, uh,

00:16:23.575 --> 00:16:26.380
find the coordinate Z that maximize,

00:16:26.380 --> 00:16:28.360
uh, this particular expression.

00:16:28.360 --> 00:16:31.510
And we are going to efficiently approximate this expression,

00:16:31.510 --> 00:16:33.580
uh, using negative sampling,

00:16:33.580 --> 00:16:39.010
where we, um, sample negative nodes of each probability proportional to their degree.

00:16:39.010 --> 00:16:43.195
And in practice, we sample about 5-20 negative examples,

00:16:43.195 --> 00:16:46.555
uh, for, uh, for every node, for every step.

00:16:46.555 --> 00:16:49.330
So, um, now, um,

00:16:49.330 --> 00:16:52.495
the question that I wanna also talk is,

00:16:52.495 --> 00:16:54.790
um, you know, how should we do this random walk?

00:16:54.790 --> 00:16:57.130
Right? So far, I only described, uh,

00:16:57.130 --> 00:16:58.914
how to optimize the embeddings,

00:16:58.914 --> 00:17:01.435
uh, for a given the random walk, um, uh,

00:17:01.435 --> 00:17:04.300
R. And we talked about this uniform random walk,

00:17:04.300 --> 00:17:09.820
where basically we run fixed-length unbiased random walks starting at each node.

00:17:09.820 --> 00:17:12.880
And the idea here is that, um,

00:17:12.880 --> 00:17:14.530
there is the issue of this, uh,

00:17:14.530 --> 00:17:16.915
type of similarity because in many cases,

00:17:16.915 --> 00:17:18.295
it might be to constraint.

00:17:18.295 --> 00:17:22.510
So the question is, can we use richer, um, random walks?

00:17:22.510 --> 00:17:25.150
Can be made the- can be make random walks more

00:17:25.150 --> 00:17:29.035
expressive so that we can tune these embeddings more?

00:17:29.035 --> 00:17:33.280
And this is the idea of a method called, uh, node2vec,

00:17:33.280 --> 00:17:35.440
where the idea is that we wanna, again,

00:17:35.440 --> 00:17:37.870
embed nodes with similar network neighborhoods,

00:17:37.870 --> 00:17:39.565
uh, closing the feature space.

00:17:39.565 --> 00:17:40.825
Uh, we are going to frame.

00:17:40.825 --> 00:17:42.670
The goal is again as, um,

00:17:42.670 --> 00:17:44.950
maximum likelihood optimization problem,

00:17:44.950 --> 00:17:47.680
uh, independent of the downstream prediction task.

00:17:47.680 --> 00:17:50.230
Uh, and the key observation here is that

00:17:50.230 --> 00:17:53.635
we have a flexible notion of network neighborhood,

00:17:53.635 --> 00:17:57.115
um, which leads to much richer, uh, node embeddings.

00:17:57.115 --> 00:18:00.370
And the extension of this simple random walk, uh,

00:18:00.370 --> 00:18:03.910
here is that we are going to develop a second-order random

00:18:03.910 --> 00:18:07.900
walk R to generate- to generate the network neighborhood,

00:18:07.900 --> 00:18:11.890
uh, N, and then we are going to apply the same optimization problem.

00:18:11.890 --> 00:18:16.210
So the only difference between deep walk and node2vec is how these, uh,

00:18:16.210 --> 00:18:19.135
set of neighboring nodes, um,

00:18:19.135 --> 00:18:22.315
is defined and how the random walk is defined.

00:18:22.315 --> 00:18:26.170
So the idea is to use flexible, um,

00:18:26.170 --> 00:18:29.770
biased random walks that can trade off between the local and global views,

00:18:29.770 --> 00:18:31.495
uh, in the network.

00:18:31.495 --> 00:18:33.865
Um, and what I mean by local and global,

00:18:33.865 --> 00:18:35.350
when you are doing the random walk,

00:18:35.350 --> 00:18:37.360
you can think of, uh, for example,

00:18:37.360 --> 00:18:41.380
depth-first search, uh, as a way to explore as much of the network

00:18:41.380 --> 00:18:45.775
as possible given a given budget of steps starting at node u.

00:18:45.775 --> 00:18:48.130
But if you really want to get a good understanding how

00:18:48.130 --> 00:18:50.755
the network looks like very locally around node u,

00:18:50.755 --> 00:18:53.950
then perhaps you'd want to explore the network more in,

00:18:53.950 --> 00:18:57.040
uh, um, breadth-first search, uh, fashion.

00:18:57.040 --> 00:18:58.600
So this is really,

00:18:58.600 --> 00:19:00.220
um, what this will allow us to do.

00:19:00.220 --> 00:19:05.050
It will allow us to trade off or kind of extrapolate between breadth-first search, um,

00:19:05.050 --> 00:19:07.240
and depth-first search, uh, type,

00:19:07.240 --> 00:19:10.390
um, network exploration, right?

00:19:10.390 --> 00:19:12.805
Um, uh, as I said, right, like, um,

00:19:12.805 --> 00:19:16.780
in terms of strategies to explore the network neighborhood,

00:19:16.780 --> 00:19:19.030
uh, and define the notion o- of, uh,

00:19:19.030 --> 00:19:21.235
N from a given starting node,

00:19:21.235 --> 00:19:23.830
you- you could imagine you wanna explore very

00:19:23.830 --> 00:19:27.160
locally and would give you a very local view of the network,

00:19:27.160 --> 00:19:29.890
and this will be just kind of breadth-first search exploration.

00:19:29.890 --> 00:19:33.010
What you'd wanna look, perhaps a depth-first search explanation, right?

00:19:33.010 --> 00:19:34.885
You wanna have these kind of global, uh,

00:19:34.885 --> 00:19:37.405
macroscopic view of the network because you

00:19:37.405 --> 00:19:41.035
capture much longer and larger, uh, distances.

00:19:41.035 --> 00:19:45.400
All right. And that's essentially the intuition behind node2vec is

00:19:45.400 --> 00:19:49.450
that you can- you can explore the network in different ways and you will get,

00:19:49.450 --> 00:19:52.000
um, better resolution, uh, you know,

00:19:52.000 --> 00:19:54.460
at more microscopic view versus more,

00:19:54.460 --> 00:19:57.445
uh, macroscopic view, uh, of the network.

00:19:57.445 --> 00:20:01.360
So how are we going to now do this in practice?

00:20:01.360 --> 00:20:03.355
How are we going to define this random walk?

00:20:03.355 --> 00:20:08.725
We are going to do biased fixed-length random walks are that,

00:20:08.725 --> 00:20:12.025
that- so that a given node u generates its neighborhood,

00:20:12.025 --> 00:20:14.020
uh, N, uh, of u.

00:20:14.020 --> 00:20:16.930
And we are going to have two hyperparameters.

00:20:16.930 --> 00:20:19.150
We'll have the return parameter p,

00:20:19.150 --> 00:20:23.050
that will say how likely is the random walk maker step back,

00:20:23.050 --> 00:20:25.435
backtrack to this- to the previous node.

00:20:25.435 --> 00:20:28.900
And then we are going to have another parameter q that

00:20:28.900 --> 00:20:32.470
are- we are going to call, uh, in-out parameter,

00:20:32.470 --> 00:20:35.920
and it will allow us to trade off between moving outward,

00:20:35.920 --> 00:20:37.765
kind of doing breadth-first search,

00:20:37.765 --> 00:20:41.860
versus staying inwards, staying close to the starting node in this way,

00:20:41.860 --> 00:20:43.975
mimicking, uh, breadth-first search.

00:20:43.975 --> 00:20:46.720
And intuitively, we can think of q as

00:20:46.720 --> 00:20:50.080
the ratio between the breadth-first and depth-first,

00:20:50.080 --> 00:20:52.150
uh, exploration of the network.

00:20:52.150 --> 00:20:54.849
To make this a bit more, uh, precise,

00:20:54.849 --> 00:21:00.835
this is called a second-order random walk because it remembers where it came from.

00:21:00.835 --> 00:21:04.720
Um, and then imagine for example in this- in this case that

00:21:04.720 --> 00:21:09.685
the random walk just came from node S_1 to the node W. And now at W,

00:21:09.685 --> 00:21:13.240
random walk needs to decide what to do, and there are- you know,

00:21:13.240 --> 00:21:14.485
it needs to pick a node,

00:21:14.485 --> 00:21:18.265
and there are actually three things that th- the- that the no- walker can do.

00:21:18.265 --> 00:21:20.560
It can return back where it came from.

00:21:20.560 --> 00:21:23.905
It can stay at the same distance,

00:21:23.905 --> 00:21:25.390
um, uh, from, uh,

00:21:25.390 --> 00:21:27.250
from where it came as it was before,

00:21:27.250 --> 00:21:28.600
so you know, it's one hop,

00:21:28.600 --> 00:21:31.000
our W is one hop from S_1,

00:21:31.000 --> 00:21:33.580
so S_2 is also one hop from S_1.

00:21:33.580 --> 00:21:37.780
So this means you stay at the same distance as from S_1 as you were,

00:21:37.780 --> 00:21:40.135
or you can navigate farther out,

00:21:40.135 --> 00:21:46.390
meaning navigate to someone- somebody who is at a distance 2 from the previous node S_1.

00:21:46.390 --> 00:21:49.960
All right? So because we know where the random walker came,

00:21:49.960 --> 00:21:51.430
the random walker needs to decide,

00:21:51.430 --> 00:21:54.310
go back, stay at the same orbit,

00:21:54.310 --> 00:21:57.760
at the same level, Or move one step further?

00:21:57.760 --> 00:22:02.830
And the way we are going to parameterize this is using parameters p and q.

00:22:02.830 --> 00:22:07.480
So we- di- if you think of this in terms of an unnormalized probabilities,

00:22:07.480 --> 00:22:08.995
then we can think that, you know,

00:22:08.995 --> 00:22:10.600
staying at the same distance,

00:22:10.600 --> 00:22:14.155
we take this with probability proportional to some constant,

00:22:14.155 --> 00:22:16.930
we return with probability 1 over p1,

00:22:16.930 --> 00:22:20.605
and then we move farther away with probability one over

00:22:20.605 --> 00:22:24.580
q one or proportional with- to- to 1 over q1, all right?

00:22:24.580 --> 00:22:27.355
So here p is the return parameter,

00:22:27.355 --> 00:22:31.165
and q is walk away type parameter.

00:22:31.165 --> 00:22:35.844
So how are we going now to do this in- in practice is essentially,

00:22:35.844 --> 00:22:37.120
as the random walker,

00:22:37.120 --> 00:22:39.310
let's say goes from S_1 to the W,

00:22:39.310 --> 00:22:41.275
now it needs to decide where to go.

00:22:41.275 --> 00:22:45.940
We are going to have this unnormalized probability distribution of transitions,

00:22:45.940 --> 00:22:49.120
which neighbor of W to navigate to.

00:22:49.120 --> 00:22:52.360
We are going to normalize these to sum to one and then

00:22:52.360 --> 00:22:55.585
flip a biased coin that will- that will navigate,

00:22:55.585 --> 00:22:58.435
that will pick one of these four possible options, right?

00:22:58.435 --> 00:23:02.980
Returning back, staying at the same distance, or navigating further.

00:23:02.980 --> 00:23:04.450
And now, for example,

00:23:04.450 --> 00:23:07.255
if I set a low value of p,

00:23:07.255 --> 00:23:12.220
then this first term will be very high and the random walk will most likely return.

00:23:12.220 --> 00:23:14.215
Uh, if we, uh,

00:23:14.215 --> 00:23:16.255
want to navigate farther away,

00:23:16.255 --> 00:23:18.325
we set a low value of q,

00:23:18.325 --> 00:23:20.365
which means that S_3 and S_4,

00:23:20.365 --> 00:23:23.530
will- will get a lot of, uh, probabilityness.

00:23:23.530 --> 00:23:26.575
Um, and that's, uh, that's basically the idea.

00:23:26.575 --> 00:23:29.440
And then again, the- the set n will be defined

00:23:29.440 --> 00:23:32.890
by the nodes visited by this biased random walk

00:23:32.890 --> 00:23:36.130
that is trading off the exploration of farther

00:23:36.130 --> 00:23:39.430
out in the neigh- neighborhood versus exploration close,

00:23:39.430 --> 00:23:41.935
uh, to the- to the- to the starting node,

00:23:41.935 --> 00:23:43.570
um, S_1 in this case.

00:23:43.570 --> 00:23:44.905
So that is, um,

00:23:44.905 --> 00:23:47.095
that is the- that is the idea.

00:23:47.095 --> 00:23:49.825
Um, so how does the algorithm work?

00:23:49.825 --> 00:23:53.275
We are going to compute the random walk probabilities first.

00:23:53.275 --> 00:23:56.695
Then we are going to simulate, the r, um,

00:23:56.695 --> 00:24:01.825
biased random walks of some fixed length l starting from each node u.

00:24:01.825 --> 00:24:03.580
And then we are going to, uh,

00:24:03.580 --> 00:24:06.715
optimize the, uh, objective function, uh,

00:24:06.715 --> 00:24:09.040
the same negative sampling objective function

00:24:09.040 --> 00:24:11.410
that I- that I already discussed in DeepWalk,

00:24:11.410 --> 00:24:13.855
uh, using stochastic gradient descent.

00:24:13.855 --> 00:24:15.955
Um, the beauty in this,

00:24:15.955 --> 00:24:19.825
is that there is linear time complexity in the optimization.

00:24:19.825 --> 00:24:21.220
Because for every node,

00:24:21.220 --> 00:24:23.185
we have a fixed set of random walks.

00:24:23.185 --> 00:24:24.865
So it's linear, uh,

00:24:24.865 --> 00:24:26.950
in the size o- of the graph.

00:24:26.950 --> 00:24:30.895
And the- all these different three steps are also parallelizable.

00:24:30.895 --> 00:24:33.310
So can- you can run them- uh, in parallel.

00:24:33.310 --> 00:24:34.930
The drawback of this, uh,

00:24:34.930 --> 00:24:36.880
no demanding approaches, uh,

00:24:36.880 --> 00:24:40.000
is that we need to learn a separate,

00:24:40.000 --> 00:24:43.255
uh, embedding, uh, for every node, uh, individually.

00:24:43.255 --> 00:24:45.370
So with a bigger networks,

00:24:45.370 --> 00:24:46.795
we need to learn, uh,

00:24:46.795 --> 00:24:50.805
bigger, uh, embeddings, or more embeddings.

00:24:50.805 --> 00:24:54.705
Um, of course, there's- there has been a lot of work, um,

00:24:54.705 --> 00:24:57.270
after these, uh, these initial,

00:24:57.270 --> 00:24:59.775
uh, papers that have proposed these ideas,

00:24:59.775 --> 00:25:01.440
there are different kinds, uh,

00:25:01.440 --> 00:25:03.855
of random walks that people kept proposed,

00:25:03.855 --> 00:25:06.960
that our alternative optimization schemes, um,

00:25:06.960 --> 00:25:10.290
and also different network pre-processing techniques, uh,

00:25:10.290 --> 00:25:14.310
that allow us to define different notions, uh, of similarity.

00:25:14.310 --> 00:25:15.930
Here are some papers that I linked,

00:25:15.930 --> 00:25:17.760
uh, you know, if you are interested,

00:25:17.760 --> 00:25:19.945
curious to learn more, um,

00:25:19.945 --> 00:25:21.535
please, uh, please read them,

00:25:21.535 --> 00:25:23.320
it will be a very good read.

00:25:23.320 --> 00:25:27.775
So let me summarize what we have learned so far.

00:25:27.775 --> 00:25:30.100
So the core idea was to embed nodes.

00:25:30.100 --> 00:25:32.170
So the distances in the embedding space

00:25:32.170 --> 00:25:35.275
reflect node similarities in the original network.

00:25:35.275 --> 00:25:39.250
Um, and we talked about two different notions of node similarity.

00:25:39.250 --> 00:25:41.920
Uh, first one was naive similarity where,

00:25:41.920 --> 00:25:42.940
uh, if two node-,

00:25:42.940 --> 00:25:44.440
if we could- for example,

00:25:44.440 --> 00:25:45.670
do, uh, connect, uh,

00:25:45.670 --> 00:25:49.135
make notes, uh, close together if they are simply connected by an edge.

00:25:49.135 --> 00:25:50.410
We could, uh, do,

00:25:50.410 --> 00:25:53.110
a neighborhood similarity, um,

00:25:53.110 --> 00:25:56.095
and today we talked about random walk approaches, uh,

00:25:56.095 --> 00:25:58.525
to, uh, node similarity where we said,

00:25:58.525 --> 00:26:02.080
all the nodes visited on a random walk from a starting node,

00:26:02.080 --> 00:26:04.300
those are, uh, similar to it.

00:26:04.300 --> 00:26:06.640
So that's, uh, essentially the idea,

00:26:06.640 --> 00:26:09.130
uh, for- uh, for today.

00:26:09.130 --> 00:26:11.830
So, uh, now of course the question is,

00:26:11.830 --> 00:26:13.630
which method should you use?

00:26:13.630 --> 00:26:16.585
Um, and no method wins all cases.

00:26:16.585 --> 00:26:20.815
So for example, node2vec performs better on a node classification,

00:26:20.815 --> 00:26:22.930
while for example, a link prediction,

00:26:22.930 --> 00:26:25.240
some alternative methods may perform better.

00:26:25.240 --> 00:26:27.310
Uh, there is a very nice survey,

00:26:27.310 --> 00:26:30.520
um, three years ago by Goyal and Ferrara.

00:26:30.520 --> 00:26:36.310
That, um, surveyed many of these methods and compare them on a number of different tasks.

00:26:36.310 --> 00:26:39.340
Um, er, and generally, you know,

00:26:39.340 --> 00:26:41.305
random walk approaches are, uh,

00:26:41.305 --> 00:26:43.705
quite efficient because you can simulate,

00:26:43.705 --> 00:26:45.670
uh, a limited number of random walks.

00:26:45.670 --> 00:26:48.910
They don't necessarily scale to the super big networks,

00:26:48.910 --> 00:26:53.020
but they scale to lar- to rel- let say medium-size network.

00:26:53.020 --> 00:26:55.750
Um , and, uh, in general, right?

00:26:55.750 --> 00:26:58.225
You must choose the definition of node similarity

00:26:58.225 --> 00:27:02.239
that best matches, uh, your application.


WEBVTT
Kind: captions
Language: en-US

00:00:05.358 --> 00:00:09.720
Welcome to CS224W, Machine Learning with Graphs.

00:00:09.720 --> 00:00:11.270
My name is Jure Leskovec.

00:00:11.270 --> 00:00:13.570
I'm Associate Professor of Computer Science at

00:00:13.570 --> 00:00:16.825
Stanford University and I will be your instructor.

00:00:16.825 --> 00:00:22.240
What I'm going to do in the first lecture is to motivate and get you excited about graph,

00:00:22.240 --> 00:00:28.030
uh, structured data and how can we apply novel machine learning methods to it?

00:00:28.030 --> 00:00:30.020
So why graphs?

00:00:30.020 --> 00:00:33.520
Graphs are a general language for describing and an-

00:00:33.520 --> 00:00:37.375
analyzing entities with the relations in interactions.

00:00:37.375 --> 00:00:40.570
This means that rather than thinking of the world

00:00:40.570 --> 00:00:44.230
or a given domain as a set of isolated datapoints,

00:00:44.230 --> 00:00:50.090
we really think of it in terms of networks and relations between these entities.

00:00:50.090 --> 00:00:51.800
This means that there is

00:00:51.800 --> 00:00:56.270
the underla- ler- underlying graph of relations between the entities,

00:00:56.270 --> 00:00:58.820
and these entities are related, uh,

00:00:58.820 --> 00:01:00.080
to each other, uh,

00:01:00.080 --> 00:01:02.800
according to these connections or the structure of the graph.

00:01:02.800 --> 00:01:06.710
And there are many types of data that can naturally be

00:01:06.710 --> 00:01:10.670
represented as graphs and modeling these graphical relations,

00:01:10.670 --> 00:01:13.535
these relational structure of the underlying domain,

00:01:13.535 --> 00:01:15.065
uh, allows us to, uh,

00:01:15.065 --> 00:01:16.340
build much more faithful,

00:01:16.340 --> 00:01:17.600
much more accurate, uh,

00:01:17.600 --> 00:01:19.100
models of the underlying,

00:01:19.100 --> 00:01:21.095
uh, phenomena underlying data.

00:01:21.095 --> 00:01:26.730
So for example, we can think of a computer networks, disease pathways, uh,

00:01:26.730 --> 00:01:29.460
networks of particles in physics, uh,

00:01:29.460 --> 00:01:32.030
networks of organisms in food webs,

00:01:32.030 --> 00:01:37.250
infrastructure, as well as events can all be represented as a graphs.

00:01:37.250 --> 00:01:40.400
Similarly, we can think of social networks,

00:01:40.400 --> 00:01:43.760
uh, economic networks, communication networks,

00:01:43.760 --> 00:01:46.370
say patients between different papers,

00:01:46.370 --> 00:01:49.115
Internet as a giant communication network,

00:01:49.115 --> 00:01:53.150
as well as ways on how neurons in our brain are connected.

00:01:53.150 --> 00:01:58.025
Again, all these domains are inherently network or graphs.

00:01:58.025 --> 00:02:00.710
And that representation allows us to capture

00:02:00.710 --> 00:02:03.934
the relationships between different objects or entities,

00:02:03.934 --> 00:02:06.545
uh, in these different, uh, domains.

00:02:06.545 --> 00:02:09.515
And last, we can take knowledge and

00:02:09.515 --> 00:02:13.610
represent facts as relationships between different entities.

00:02:13.610 --> 00:02:17.914
We can describe the regulatory mechanisms in our cells,

00:02:17.914 --> 00:02:22.580
um, as processes governed by the connections between different entities.

00:02:22.580 --> 00:02:27.455
We can even take scenes from real world and presented them

00:02:27.455 --> 00:02:32.600
as graphs of relationships between the objects in the scene.

00:02:32.600 --> 00:02:34.160
These are called scene graphs.

00:02:34.160 --> 00:02:39.560
We can take computer code software and represent it as a graph of, let's say,

00:02:39.560 --> 00:02:42.350
calls between different functions or as

00:02:42.350 --> 00:02:45.800
a structure of the code captures by the abstract syntax tree.

00:02:45.800 --> 00:02:50.600
We can also naturally take molecules which are composed of nodes, uh,

00:02:50.600 --> 00:02:55.370
of atoms and bonds as a set of graphs, um,

00:02:55.370 --> 00:03:00.410
where we represent atoms as nodes and their bonds as edges between them.

00:03:00.410 --> 00:03:02.150
And of course, in computer graphics,

00:03:02.150 --> 00:03:07.330
we can take three-dimensional shapes and- and represent them, um, as a graphs.

00:03:07.330 --> 00:03:09.230
So in all these domains,

00:03:09.230 --> 00:03:11.810
graphical structure is the- is

00:03:11.810 --> 00:03:15.620
the important part that allows us to model the under- underlying domain,

00:03:15.620 --> 00:03:18.530
underlying phenomena in a fateful way.

00:03:18.530 --> 00:03:21.455
So the way we are going to think about graph

00:03:21.455 --> 00:03:25.040
relational data in this class is that there are essentially two big,

00:03:25.040 --> 00:03:29.145
uh, parts, uh, of data that can be represented as graphs.

00:03:29.145 --> 00:03:32.810
First are what is called natural graphs or networks,

00:03:32.810 --> 00:03:36.780
where underlying domains can naturally be represented as graphs.

00:03:36.780 --> 00:03:38.990
For example, social networks,

00:03:38.990 --> 00:03:43.355
societies are collection of seven billion individuals and connections between them,

00:03:43.355 --> 00:03:47.630
communications and transactions between electronic devices, phone calls,

00:03:47.630 --> 00:03:51.930
financial transactions, all naturally form, uh, graphs.

00:03:51.930 --> 00:03:54.110
In biomedicine we have genes,

00:03:54.110 --> 00:03:57.109
proteins regulating biological processes,

00:03:57.109 --> 00:03:59.510
and we can represent interactions between

00:03:59.510 --> 00:04:03.585
these different biological entities with a graph.

00:04:03.585 --> 00:04:05.215
And- and as I mentioned,

00:04:05.215 --> 00:04:08.485
connections between neurons in our brains are,

00:04:08.485 --> 00:04:11.860
um, essentially a network of, uh, connections.

00:04:11.860 --> 00:04:13.735
And if we want to model these domains,

00:04:13.735 --> 00:04:16.180
really present them as networks.

00:04:16.180 --> 00:04:21.114
A second example of domains that also have relational structure,

00:04:21.114 --> 00:04:26.010
um, where- and we can use graphs to represent that relational structure.

00:04:26.010 --> 00:04:30.895
So for example, information and knowledge is many times organized and linked.

00:04:30.895 --> 00:04:33.585
Software can be represented as a graph.

00:04:33.585 --> 00:04:35.445
We can many times take, uh,

00:04:35.445 --> 00:04:38.580
datapoints and connect similar data points.

00:04:38.580 --> 00:04:40.200
And this will create our graph,

00:04:40.200 --> 00:04:41.910
uh, a similarity network.

00:04:41.910 --> 00:04:44.100
And we can take other, um, uh,

00:04:44.100 --> 00:04:48.065
domains that have natural relational structure like molecules,

00:04:48.065 --> 00:04:51.185
scene graphs, 3D shapes, as well as,

00:04:51.185 --> 00:04:52.475
you know, in physics,

00:04:52.475 --> 00:04:56.065
we can take particle-based simulation to simulate how,

00:04:56.065 --> 00:04:58.910
uh, particles are related to each other through,

00:04:58.910 --> 00:05:01.355
uh, and they represent this with the graph.

00:05:01.355 --> 00:05:05.480
So this means that there are many different domains, either, uh,

00:05:05.480 --> 00:05:08.600
as natural graphs or natural networks,

00:05:08.600 --> 00:05:11.720
as well as other domains that can naturally be

00:05:11.720 --> 00:05:16.375
modeled as graphs to capture the relational structure.

00:05:16.375 --> 00:05:19.460
And the main question for this class that we are

00:05:19.460 --> 00:05:22.070
going to address is to talk about how do we take

00:05:22.070 --> 00:05:28.075
advantage of this relational structure to be- to make better, more accurate predictions.

00:05:28.075 --> 00:05:30.950
And this is especially important because

00:05:30.950 --> 00:05:34.130
couplings domains have reached a relational structure,

00:05:34.130 --> 00:05:36.980
uh, which can be presented, uh, with a graph.

00:05:36.980 --> 00:05:40.130
And by explicitly modeling these relationships,

00:05:40.130 --> 00:05:41.930
we will be able to achieve, uh,

00:05:41.930 --> 00:05:44.450
better performance, build more, uh,

00:05:44.450 --> 00:05:48.535
accurate, uh, models, make more accurate predictions.

00:05:48.535 --> 00:05:53.920
And this is especially interesting and important in the age of deep learning,

00:05:53.920 --> 00:06:00.280
where the- today's deep learning modern toolbox is specialized for simple data types.

00:06:00.280 --> 00:06:03.880
It is specialized for simple sequences, uh, and grids.

00:06:03.880 --> 00:06:06.205
A sequence is a, uh,

00:06:06.205 --> 00:06:10.830
like text or speech has this linear structure and there

00:06:10.830 --> 00:06:15.475
has- there are been amazing tools developed to analyze this type of structure.

00:06:15.475 --> 00:06:20.170
Images can all be resized and have this spatial locality so- so

00:06:20.170 --> 00:06:24.804
they can be represented as fixed size grids or fixed size standards.

00:06:24.804 --> 00:06:29.090
And again, deep learning methodology has been very good at processing this type of,

00:06:29.090 --> 00:06:30.915
uh, fixed size images.

00:06:30.915 --> 00:06:38.075
However, um, graphs, networks are much harder to process because they are more complex.

00:06:38.075 --> 00:06:42.665
First, they have arbitrary size and arb- and complex topology.

00:06:42.665 --> 00:06:47.990
Um, and there is also no spatial locality as in grids or as in text.

00:06:47.990 --> 00:06:50.360
In text we know left and right,

00:06:50.360 --> 00:06:53.600
in grids we have up and down, uh, left and right.

00:06:53.600 --> 00:06:56.165
But in networks, there is no reference point,

00:06:56.165 --> 00:06:57.815
there is no notion of,

00:06:57.815 --> 00:07:00.355
uh, uh, spatial locality.

00:07:00.355 --> 00:07:03.240
The second important thing is there is no reference point,

00:07:03.240 --> 00:07:07.710
there is no fixed node ordering that would allow us,

00:07:07.710 --> 00:07:09.075
uh, uh, to do, uh,

00:07:09.075 --> 00:07:10.500
to do deep learning.

00:07:10.500 --> 00:07:15.290
And often, these networks are dynamic and have multi-model features.

00:07:15.290 --> 00:07:17.105
So in this course,

00:07:17.105 --> 00:07:19.070
we are really going to, uh,

00:07:19.070 --> 00:07:24.200
talk about how do we develop neural networks that are much more broadly applicable?

00:07:24.200 --> 00:07:29.855
How do we develop neural networks that are applicable to complex data types like graphs?

00:07:29.855 --> 00:07:34.685
And really, it is relational data graphs that are the- the new frontier,

00:07:34.685 --> 00:07:39.085
uh, of deep learning and representation learning, uh, research.

00:07:39.085 --> 00:07:43.475
So intuitively, what we would like to do is we would like to do,

00:07:43.475 --> 00:07:45.515
uh, build neural networks,

00:07:45.515 --> 00:07:48.155
but on the input we'll take, uh, uh,

00:07:48.155 --> 00:07:50.075
our graph and on the output,

00:07:50.075 --> 00:07:51.920
they will be able to make predictions.

00:07:51.920 --> 00:07:55.310
And, uh, these predictions can be at the level of individual nodes,

00:07:55.310 --> 00:07:58.220
can be at the level of pairs of nodes or links,

00:07:58.220 --> 00:08:03.860
or it can be something much more complex like a brand new generated graph or, uh,

00:08:03.860 --> 00:08:08.455
prediction of a property of a given molecule that can be represented,

00:08:08.455 --> 00:08:10.505
um, as a graph on the input.

00:08:10.505 --> 00:08:12.125
And the question is,

00:08:12.125 --> 00:08:14.690
how do we design this neural network architecture

00:08:14.690 --> 00:08:17.270
that will allow us to do this end to end,

00:08:17.270 --> 00:08:21.650
meaning there will be no human feature engineering, uh, needed?

00:08:21.650 --> 00:08:24.215
So what I mean by that is that, um,

00:08:24.215 --> 00:08:26.884
in traditional, uh, machine learning approaches,

00:08:26.884 --> 00:08:31.595
a lot of effort goes into des- designing proper features,

00:08:31.595 --> 00:08:36.680
proper ways to capture the structure of the data so that machine learning models can,

00:08:36.680 --> 00:08:38.520
uh, take advantage of it.

00:08:38.520 --> 00:08:41.120
So what we would like to do in this class,

00:08:41.120 --> 00:08:44.210
we will talk mostly about representation learning

00:08:44.210 --> 00:08:47.615
where these feature engineering step is taken away.

00:08:47.615 --> 00:08:50.165
And basically, as soon as we have our graph,

00:08:50.165 --> 00:08:51.905
uh, uh, repr- graph data,

00:08:51.905 --> 00:08:58.010
we can automatically learn a good representation of the graph so that it can be used for,

00:08:58.010 --> 00:09:00.740
um, downstream machine learning algorithm.

00:09:00.740 --> 00:09:05.900
So that a presentation learning is about automatically extracting or learning features,

00:09:05.900 --> 00:09:07.435
uh, in the graph.

00:09:07.435 --> 00:09:11.120
The way we can think of representation learning is to map

00:09:11.120 --> 00:09:14.765
nodes of our graph to a d-dimensional embedding,

00:09:14.765 --> 00:09:16.820
to the d-dimensional vectors,

00:09:16.820 --> 00:09:19.280
such that seeming that are nodes in the network are

00:09:19.280 --> 00:09:22.115
embedded close together in the embedding space.

00:09:22.115 --> 00:09:25.100
So the goal is to learn this function f that will take

00:09:25.100 --> 00:09:28.160
the nodes and map them into these d-dimensional,

00:09:28.160 --> 00:09:30.200
um, real valued vectors,

00:09:30.200 --> 00:09:34.280
where this vector will call this, uh, representation, uh,

00:09:34.280 --> 00:09:38.165
or a feature representation or an embedding of a given node,

00:09:38.165 --> 00:09:40.310
an embedding of an entire graph,

00:09:40.310 --> 00:09:42.110
an embedding of a given link,

00:09:42.110 --> 00:09:43.565
um, and so on.

00:09:43.565 --> 00:09:46.775
So a big part of our class we'll be, uh,

00:09:46.775 --> 00:09:51.005
investigating and learning about latest presentation learning,

00:09:51.005 --> 00:09:53.750
deep learning approaches that can be applied,

00:09:53.750 --> 00:09:56.765
uh, to graph, uh, structured data.

00:09:56.765 --> 00:09:59.940
And we are going to, uh, uh,

00:09:59.940 --> 00:10:02.600
talk about many different topics in

00:10:02.600 --> 00:10:06.230
machine learning and the representation learning for graph structure data.

00:10:06.230 --> 00:10:09.725
So first, we're going to talk about traditional methods

00:10:09.725 --> 00:10:13.565
for machine learning and graphs like graphlets and graph kernels.

00:10:13.565 --> 00:10:17.480
We are then going to talk about methods to generate, um,

00:10:17.480 --> 00:10:22.130
generic node embeddings, methods like DeepWalk and Node2Vec.

00:10:22.130 --> 00:10:25.010
We are going to spend quite a bit of time talking about

00:10:25.010 --> 00:10:30.010
graph neural networks and popular graph neural network architectures like graph,

00:10:30.010 --> 00:10:31.940
uh, convolutional neural network,

00:10:31.940 --> 00:10:36.720
the GraphSage architecture or Graph Attention Network, uh, architecture.

00:10:36.720 --> 00:10:41.450
We are also going to study the expressive power of graph neural networks,

00:10:41.450 --> 00:10:44.045
um, the theory behind them,

00:10:44.045 --> 00:10:47.705
and how do we scale them up to very large graphs.

00:10:47.705 --> 00:10:50.465
Um, and then in the second part of this course,

00:10:50.465 --> 00:10:53.585
we are also going to talk about heterogeneous graphs,

00:10:53.585 --> 00:10:55.940
knowledge graphs, and applications,

00:10:55.940 --> 00:10:57.395
uh, to logical reasoning.

00:10:57.395 --> 00:11:00.790
We're learning about methods like TransE and BetaE.

00:11:00.790 --> 00:11:05.690
We are also going to talk about how do we build deep generative models for

00:11:05.690 --> 00:11:08.270
graphs where we can think of the prediction of the model to

00:11:08.270 --> 00:11:11.060
be an entire newly generated graph.

00:11:11.060 --> 00:11:15.890
And we are also going to discuss applications to biomedicine, um,

00:11:15.890 --> 00:11:18.890
various scientific applications, as well

00:11:18.890 --> 00:11:22.220
as applications to industry in terms of recommender systems,

00:11:22.220 --> 00:11:24.770
fraud detection, and so on.

00:11:24.770 --> 00:11:27.695
So here is the outline of this course.

00:11:27.695 --> 00:11:30.860
Week by week, 10 weeks, starting, uh,

00:11:30.860 --> 00:11:35.030
starting today and all the way to the middle of March,

00:11:35.030 --> 00:11:37.805
um, where, uh, the- the course will finish.

00:11:37.805 --> 00:11:41.795
We will have 20 lectures and we will cover all the topics,

00:11:41.795 --> 00:11:43.190
uh, that I have discussed,

00:11:43.190 --> 00:11:44.990
and in particular focus on

00:11:44.990 --> 00:11:49.650
graph neural networks and the representation learning in graphs.


WEBVTT
Kind: captions
Language: en-US

00:00:04.100 --> 00:00:10.200
So now I wanna talk about the second method about, uh, community structure,

00:00:10.200 --> 00:00:12.284
and we'll call this, uh, BigCLAM,

00:00:12.284 --> 00:00:14.655
is the name of the method and this is for detecting

00:00:14.655 --> 00:00:17.195
overlapping communities in networks, right?

00:00:17.195 --> 00:00:21.840
So so far we made assumption about this type of structure of the network.

00:00:21.840 --> 00:00:23.145
So like we have these clusters,

00:00:23.145 --> 00:00:26.910
each one has a lot of connections and then a few connections across.

00:00:26.910 --> 00:00:29.940
But in many cases, you know,

00:00:29.940 --> 00:00:33.830
people- humans, we belong to multiple social communities at once.

00:00:33.830 --> 00:00:35.750
So you can have these social groups,

00:00:35.750 --> 00:00:37.550
uh, that overlap, right?

00:00:37.550 --> 00:00:41.420
So how could we extract this type of overlapping community structure?

00:00:41.420 --> 00:00:42.905
To give you an example,

00:00:42.905 --> 00:00:46.265
this is actually a Facebook network of one of the students,

00:00:46.265 --> 00:00:48.025
uh, uh, in my group.

00:00:48.025 --> 00:00:52.080
These are his friends and these are collection- connections between his friends.

00:00:52.080 --> 00:00:53.510
Um, and you can ask, you know,

00:00:53.510 --> 00:00:55.550
what is the community structure of this network?

00:00:55.550 --> 00:00:58.640
What kind of communities does this PhD student of- uh,

00:00:58.640 --> 00:01:01.105
uh, from my group, uh, belong to?

00:01:01.105 --> 00:01:03.470
Um, and you know, if you look at this, perhaps you say,

00:01:03.470 --> 00:01:05.660
there is- I see one big group here,

00:01:05.660 --> 00:01:07.280
maybe I see another one here,

00:01:07.280 --> 00:01:09.310
maybe there is something down here.

00:01:09.310 --> 00:01:15.140
What is actually interesting if you look at it and you ask the student to- to, uh,

00:01:15.140 --> 00:01:17.490
put his friends into different social groups,

00:01:17.490 --> 00:01:20.495
uh, here are the social groups he came up with.

00:01:20.495 --> 00:01:25.320
He basically said, I- I- I have friends from four different social groups,

00:01:25.320 --> 00:01:27.120
uh, and here are the social groups.

00:01:27.120 --> 00:01:30.190
And of course, some friends belong to multiple of these social groups.

00:01:30.190 --> 00:01:34.050
Um, and then you can ask the- the student to label these groups.

00:01:34.050 --> 00:01:37.205
And he said, "Oh, these are my friends from high school.

00:01:37.205 --> 00:01:41.840
These are the friends from an internship I did in a given company.

00:01:41.840 --> 00:01:44.660
These are the Stanford friends I play basketball with,

00:01:44.660 --> 00:01:47.905
and these are these other Stanford friends I play, uh, squash with."

00:01:47.905 --> 00:01:50.570
Uh, and of course there are some people here, right?

00:01:50.570 --> 00:01:53.910
That are both- that went to the student to the same high school,

00:01:53.910 --> 00:01:58.310
they are now at Stanford and they play both basketball and squash with him.

00:01:58.310 --> 00:02:00.875
And, you know, that have some from the same high school

00:02:00.875 --> 00:02:03.860
who only- who did the internship with him at this company,

00:02:03.860 --> 00:02:05.720
but are not currently at Stanford, right?

00:02:05.720 --> 00:02:09.714
So we see how these, um, communities overlap differently.

00:02:09.714 --> 00:02:13.085
Another thing that I wanna notice you in this picture is that

00:02:13.085 --> 00:02:17.015
nodes are either solid or they are kind of gray.

00:02:17.015 --> 00:02:22.340
Um, and the point here is what I'm trying to show is that for- for a given- uh,

00:02:22.340 --> 00:02:24.439
for a given network,

00:02:24.439 --> 00:02:27.310
uh, we can- for this given input,

00:02:27.310 --> 00:02:29.455
we can ask our community detection method,

00:02:29.455 --> 00:02:32.575
our BigCLAM to identify the clusters.

00:02:32.575 --> 00:02:36.239
And the big- BigCLAM identifies these four clusters,

00:02:36.239 --> 00:02:38.415
and then we, uh, color the nodes.

00:02:38.415 --> 00:02:41.995
The node is full if it was assigned to the correct cluster,

00:02:41.995 --> 00:02:47.020
and it is gray if it- if it was not assigned to the great- to the correct cluster.

00:02:47.020 --> 00:02:52.480
Uh, and what is amazing here is basically that only on the network structure alone,

00:02:52.480 --> 00:02:55.690
only on this unlabeled network structure,

00:02:55.690 --> 00:02:58.900
we can assign nodes to the correct clusters,

00:02:58.900 --> 00:03:02.890
to the correct social communities without knowing anything about them.

00:03:02.890 --> 00:03:05.350
Which is remarkable and super impressive, right?

00:03:05.350 --> 00:03:06.920
It's a super hard task.

00:03:06.920 --> 00:03:09.210
So, uh, this was one example,

00:03:09.210 --> 00:03:12.540
another example I wanna show you is in, uh, biological networks.

00:03:12.540 --> 00:03:14.750
This is a protein-protein interaction network,

00:03:14.750 --> 00:03:17.150
and if you identify functional modules here,

00:03:17.150 --> 00:03:19.055
you see how these functional modules,

00:03:19.055 --> 00:03:20.810
uh, overlap with each other.

00:03:20.810 --> 00:03:23.270
So this is, um, very interesting to think

00:03:23.270 --> 00:03:26.515
about overlapping community structure in networks.

00:03:26.515 --> 00:03:30.680
So just to contrast what we have been talking about so far,

00:03:30.680 --> 00:03:32.720
we talked about networks having

00:03:32.720 --> 00:03:36.995
this kind of clustering structure where dense clusters, few connections across.

00:03:36.995 --> 00:03:40.400
If we think about this in terms of the graph adjacency matrix,

00:03:40.400 --> 00:03:42.485
then the adjacency matrix would look like this,

00:03:42.485 --> 00:03:44.900
where basically you have one cluster, you know,

00:03:44.900 --> 00:03:46.970
the orange one, you have the green cluster,

00:03:46.970 --> 00:03:48.140
a lot of connections.

00:03:48.140 --> 00:03:51.560
And then these areas of the adjacency matrix means, you know,

00:03:51.560 --> 00:03:55.100
an orange node linking to a green node and there is, uh,

00:03:55.100 --> 00:03:59.375
less connections here because we assume there is less connections across the two clusters.

00:03:59.375 --> 00:04:01.115
In the overlapping case,

00:04:01.115 --> 00:04:03.770
it actually- the situation gets much more tricky.

00:04:03.770 --> 00:04:06.200
If you have two overlapping clusters,

00:04:06.200 --> 00:04:08.480
then these red nodes here are in the overlap.

00:04:08.480 --> 00:04:09.950
And if you think of it from the, um,

00:04:09.950 --> 00:04:13.940
um, adjacency matrix point of view, this is the picture, right?

00:04:13.940 --> 00:04:15.650
You have one cluster,

00:04:15.650 --> 00:04:17.555
you have the other cluster.

00:04:17.555 --> 00:04:19.025
And here in the overlap,

00:04:19.025 --> 00:04:22.990
you have even more connections because these connections kind of,

00:04:22.990 --> 00:04:25.440
uh, come either from one cluster or,

00:04:25.440 --> 00:04:26.540
uh, the other cluster.

00:04:26.540 --> 00:04:28.550
And the point here wouldn't be to say, oh,

00:04:28.550 --> 00:04:32.605
there is this set of people that are super strongly connected with each other.

00:04:32.605 --> 00:04:34.290
The point is to say, oh,

00:04:34.290 --> 00:04:38.460
there are two clusters that overlap in the set in the middle.

00:04:38.460 --> 00:04:41.680
So now, how are we going to detect,

00:04:41.680 --> 00:04:44.600
uh, this type of structure, uh, in the network?

00:04:44.600 --> 00:04:47.035
How we are going to extract communities.

00:04:47.035 --> 00:04:49.875
Um, our plan of action is the following.

00:04:49.875 --> 00:04:51.420
We are going in step 1,

00:04:51.420 --> 00:04:55.990
define a generative model for graphs that is based on node community affiliations,

00:04:55.990 --> 00:04:58.765
and we'll call this model Affiliation Graph Model.

00:04:58.765 --> 00:05:03.800
And then we are going to define an optimization problem that, uh, will, uh,

00:05:03.800 --> 00:05:06.950
optimize the log-likelihood of,

00:05:06.950 --> 00:05:10.230
uh, the model to generate, uh, the graph.

00:05:10.230 --> 00:05:14.935
So what we are going to learn today is an instance of, uh,

00:05:14.935 --> 00:05:20.690
generative modeling of networks and how to feed a generative model to a given graph.

00:05:20.690 --> 00:05:22.355
So this is, uh,

00:05:22.355 --> 00:05:25.505
kind of very useful and very fundamental.

00:05:25.505 --> 00:05:28.355
So first, let me define the model.

00:05:28.355 --> 00:05:29.840
The model will be the following.

00:05:29.840 --> 00:05:33.860
Uh, we will have a set of nodes at the bottom,

00:05:33.860 --> 00:05:36.020
and a set of communities at the top.

00:05:36.020 --> 00:05:40.820
And a node will be connected to a community if a given node belongs to a community.

00:05:40.820 --> 00:05:42.045
So in this case, you know,

00:05:42.045 --> 00:05:45.500
blue nodes belong to A alone, green nodes, uh,

00:05:45.500 --> 00:05:47.990
belo- uh, belong to B alone,

00:05:47.990 --> 00:05:50.255
and then the red nodes belong to both.

00:05:50.255 --> 00:05:52.895
So they are connected both to A and B, um,

00:05:52.895 --> 00:05:54.860
and this is how we are going to de- describe

00:05:54.860 --> 00:05:57.605
the community structure of the- of the network.

00:05:57.605 --> 00:06:02.925
Now, we wanna build a generative model which says, you- I'm given,

00:06:02.925 --> 00:06:06.015
uh, uh, an assignment of nodes to communities,

00:06:06.015 --> 00:06:08.840
I want to generate the edges of the network.

00:06:08.840 --> 00:06:11.125
How do I generate the edges?

00:06:11.125 --> 00:06:15.170
And we are going now to describe the generative process,

00:06:15.170 --> 00:06:16.760
how to turn the, uh,

00:06:16.760 --> 00:06:19.960
affiliations of nodes to communities into edges of the network.

00:06:19.960 --> 00:06:22.095
Our model will have a set of nodes,

00:06:22.095 --> 00:06:23.339
a set of communities,

00:06:23.339 --> 00:06:27.630
and a set of memberships of nodes to communities, right? Like this.

00:06:27.630 --> 00:06:30.080
And then we are going to assume that each community,

00:06:30.080 --> 00:06:32.584
C up here, has a single parameter,

00:06:32.584 --> 00:06:37.620
P sub C. And this parameter will have the following, uh, role, right?

00:06:37.620 --> 00:06:40.940
So basically we'll say given the- given our- our model,

00:06:40.940 --> 00:06:43.220
given affiliations in this community parameters,

00:06:43.220 --> 00:06:47.165
we wanna generate the edges of the underlying network.

00:06:47.165 --> 00:06:50.300
And we are going to say that nodes in community

00:06:50.300 --> 00:06:53.720
C will connect to each other by flipping a coin with, uh,

00:06:53.720 --> 00:06:57.050
bare ground with a probabili- that is going to create

00:06:57.050 --> 00:07:00.360
an edge with probability C. So the way to think of this is,

00:07:00.360 --> 00:07:01.910
you know, this pair of nodes says, oh,

00:07:01.910 --> 00:07:03.875
we belong to the same community A,

00:07:03.875 --> 00:07:06.890
let's flip this coin P sub A, and, uh,

00:07:06.890 --> 00:07:08.195
if the coin says yes,

00:07:08.195 --> 00:07:10.560
we are going to create a connection between us.

00:07:10.560 --> 00:07:12.830
Why? For example, these two nodes here,

00:07:12.830 --> 00:07:14.780
they belong both to A and B.

00:07:14.780 --> 00:07:19.345
So they are going to flip the first coin and if the coin says yes, they'll create a link.

00:07:19.345 --> 00:07:22.610
Um, then they'll flip the second coin and if the coin says yes,

00:07:22.610 --> 00:07:24.150
they are going to create a link.

00:07:24.150 --> 00:07:27.170
So basically if at least one of these two coins says yes,

00:07:27.170 --> 00:07:29.185
they are going to, uh, link to each other.

00:07:29.185 --> 00:07:31.910
So this means that long- nodes that belong to

00:07:31.910 --> 00:07:34.970
multiple communities get multiple coin flips,

00:07:34.970 --> 00:07:36.660
which means they are more likely to,

00:07:36.660 --> 00:07:37.910
uh, link to each other.

00:07:37.910 --> 00:07:41.780
So in some sense, if the- if they are unfortunate the first time,

00:07:41.780 --> 00:07:43.440
they might- might be luck in creating

00:07:43.440 --> 00:07:47.255
a connection next time for the second community they have in common.

00:07:47.255 --> 00:07:50.110
So one way you can write out the probability that U

00:07:50.110 --> 00:07:53.315
and V connect with each other is through this formula.

00:07:53.315 --> 00:07:54.995
And let me just explain it.

00:07:54.995 --> 00:07:58.730
So basically what this is saying is this is the probability that, um,

00:07:58.730 --> 00:08:01.175
at least one of these coin flips with biases,

00:08:01.175 --> 00:08:03.500
P sub C comes up heads, right?

00:08:03.500 --> 00:08:05.120
So basically we are saying, let's go over to

00:08:05.120 --> 00:08:08.215
all the communities that nodes U and V have in common,

00:08:08.215 --> 00:08:13.460
for each community we flip a coin of each probability C. So 1 minus,

00:08:13.460 --> 00:08:15.605
uh, P sub C is the probability that, uh,

00:08:15.605 --> 00:08:17.740
the coin says, let's not create an edge.

00:08:17.740 --> 00:08:19.640
Now multiplying this says,

00:08:19.640 --> 00:08:22.985
what if you are super unlucky and for every coin flip,

00:08:22.985 --> 00:08:25.415
the coin flip comes up and saying no edge.

00:08:25.415 --> 00:08:29.480
So this is now the total probability that all the coin flips say no edge

00:08:29.480 --> 00:08:34.415
and 1 minus that is that at least one of these coin flips came up,

00:08:34.415 --> 00:08:36.800
uh, uh, positively and the edge is created.

00:08:36.800 --> 00:08:40.600
So basically we say a pair of nodes is connected if at least one of these coin flips,

00:08:40.600 --> 00:08:44.145
uh, was successful, and that's kind of the formula for this.

00:08:44.145 --> 00:08:46.805
So what did we learn so far?

00:08:46.805 --> 00:08:48.965
We learned that if you give me the model,

00:08:48.965 --> 00:08:52.085
uh, specified by the nodes,

00:08:52.085 --> 00:08:54.585
communities, the memberships, uh,

00:08:54.585 --> 00:08:56.160
and the parameters, uh,

00:08:56.160 --> 00:08:59.370
then we know how to generate a- a network.

00:08:59.370 --> 00:09:03.185
Um, this model is super flexible because it allows, uh,

00:09:03.185 --> 00:09:04.610
us to- to, uh,

00:09:04.610 --> 00:09:07.145
express a variety of community structures.

00:09:07.145 --> 00:09:12.015
We can create this type of traditional one that's- two dense clusters.

00:09:12.015 --> 00:09:14.275
Um, with this type of structure.

00:09:14.275 --> 00:09:18.400
We can create overlaps by saying this orange nodes belong to both communities,

00:09:18.400 --> 00:09:22.030
so they are in the overlap or we could even have a hierarchical structure,

00:09:22.030 --> 00:09:23.860
where we say, you know, ah,

00:09:23.860 --> 00:09:26.620
um, violet nodes belong to A,

00:09:26.620 --> 00:09:29.485
uh, green nodes belong to, uh, C, ah,

00:09:29.485 --> 00:09:33.640
and then all the nodes belong to B so that there is this kind of A and B are

00:09:33.640 --> 00:09:38.035
embedded or included are subgroups of the bigger group, ah, B.

00:09:38.035 --> 00:09:39.970
So, uh, that's the way, ah,

00:09:39.970 --> 00:09:43.675
the flexibility of this model to give you some, uh, intuition.

00:09:43.675 --> 00:09:47.755
So what we know to do so far is,

00:09:47.755 --> 00:09:51.640
how do we go from the model to generate the network?

00:09:51.640 --> 00:09:54.820
What we'd like to do next is to go the other way around, right.

00:09:54.820 --> 00:09:57.760
Given the network, we'd like to say, what is the model?

00:09:57.760 --> 00:10:01.420
So essentially this is called maximum likelihood estimation,

00:10:01.420 --> 00:10:04.210
where basically we'll say, we'll define the following reasoning.

00:10:04.210 --> 00:10:10.690
We say, assume there is some model that nature uses to generate graphs.

00:10:10.690 --> 00:10:15.850
And we will assume that AGM is the model that nature uses to generate graphs.

00:10:15.850 --> 00:10:17.305
Then we are going to say,

00:10:17.305 --> 00:10:21.430
let's assume that our real-world graph was generated by AGM.

00:10:21.430 --> 00:10:22.975
So we are going to say,

00:10:22.975 --> 00:10:29.800
what is the model F that is most likely to have generated my data?

00:10:29.800 --> 00:10:32.485
So in our case we say, given a graph,

00:10:32.485 --> 00:10:34.975
find the most likely, ah,

00:10:34.975 --> 00:10:39.040
model AGM, ah, that have generated the data.

00:10:39.040 --> 00:10:40.570
So what do we mean by model?

00:10:40.570 --> 00:10:43.300
It means, we need to decide that the number of communities,

00:10:43.300 --> 00:10:46.630
we need to assign- decide how nodes belong to communities,

00:10:46.630 --> 00:10:49.660
and what is every parameter P_C of every community?

00:10:49.660 --> 00:10:54.040
So we say, what are the values of these parameters such that

00:10:54.040 --> 00:10:56.290
this graph that I observe in reality

00:10:56.290 --> 00:10:59.590
would most likely have been generated by my model?

00:10:59.590 --> 00:11:02.350
So now, as I said,

00:11:02.350 --> 00:11:05.905
basically we want to fit our model to the underlying graph.

00:11:05.905 --> 00:11:09.520
And the way we do this is using maximum likelihood estimation.

00:11:09.520 --> 00:11:12.685
So basically given a real, uh, graph,

00:11:12.685 --> 00:11:15.280
we want to find the parame- the model parameters

00:11:15.280 --> 00:11:19.030
F which maximize the likelihood of our graph.

00:11:19.030 --> 00:11:21.535
So the way I wri- say this is to say,

00:11:21.535 --> 00:11:23.680
given some model parameters,

00:11:23.680 --> 00:11:26.560
I wanna generate a synthetic graph.

00:11:26.560 --> 00:11:30.280
And then I wanna, ah, ah, I wanna say, uh,

00:11:30.280 --> 00:11:33.760
what is the correspondence between the synthetic graph, uh, and the real graph?

00:11:33.760 --> 00:11:37.375
So in some sense, I wanna maximize the probability

00:11:37.375 --> 00:11:41.290
of my model F generating the real graph, right?

00:11:41.290 --> 00:11:44.980
So basically, I wanna be able to efficiently compute this, ah,

00:11:44.980 --> 00:11:50.755
likelihood that F generated my data G and then I need to- a way to,

00:11:50.755 --> 00:11:54.370
uh, optimize over F to maximize this probability,

00:11:54.370 --> 00:11:55.975
to maximize this likelihood.

00:11:55.975 --> 00:12:02.560
So let me now first tell you how likelihood is defined over a graph, right?

00:12:02.560 --> 00:12:05.620
So I'm given a real graph G and I'm given a

00:12:05.620 --> 00:12:09.130
model F that assigns a probability to every edge.

00:12:09.130 --> 00:12:11.035
And I want to genera- ah,

00:12:11.035 --> 00:12:15.730
compute the likelihood that my model F has generated the graph

00:12:15.730 --> 00:12:20.455
G. And the way I can do this is to say, that is a graph.

00:12:20.455 --> 00:12:21.730
Here's an adjacency matrix.

00:12:21.730 --> 00:12:23.050
This is my input data.

00:12:23.050 --> 00:12:28.930
I have my model and my model assigns a probability to every edge that can happen, right?

00:12:28.930 --> 00:12:30.310
Every- the model says,

00:12:30.310 --> 00:12:32.500
A has a self-loop with itself with,

00:12:32.500 --> 00:12:35.500
ah, you know, probability 0.25.

00:12:35.500 --> 00:12:37.870
Of what- node 1 links to node B,

00:12:37.870 --> 00:12:40.300
uh, to node 2 with probability 0.1.

00:12:40.300 --> 00:12:41.755
So now I can ask,

00:12:41.755 --> 00:12:45.355
what is the total probability that this kind of

00:12:45.355 --> 00:12:50.455
probabilistic adjacency matrix would generate a given graph G?

00:12:50.455 --> 00:12:53.350
I can simply do this as a- as a product.

00:12:53.350 --> 00:12:54.805
Uh, and I go, ah-ha,

00:12:54.805 --> 00:12:58.840
I wanna go- I wanna go over all the edges of G. I wanna go over

00:12:58.840 --> 00:13:02.875
all the entries of this adjacency matrix where there is a value 1.

00:13:02.875 --> 00:13:06.340
And I want these coin flips to land up heads.

00:13:06.340 --> 00:13:10.690
I want them to- I want to multiply these probabilities together because I want,

00:13:10.690 --> 00:13:12.010
uh, you know, this will be,

00:13:12.010 --> 00:13:18.250
uh, with, ah, will have value of 1 with probability 0.25 and so on and so forth, right.

00:13:18.250 --> 00:13:21.910
Then I also want to go over all the entries that are missing.

00:13:21.910 --> 00:13:25.855
So basically, over all the edges that don't exist in the network.

00:13:25.855 --> 00:13:30.145
And here I want the- the- the coin to land on the tail.

00:13:30.145 --> 00:13:31.600
I want no edge.

00:13:31.600 --> 00:13:35.230
So I go over all the non-edges of the network and I multiply 1

00:13:35.230 --> 00:13:39.030
minus the probability of edge occurring, right?

00:13:39.030 --> 00:13:42.975
And this is how likelihood of a graph under a given model is defined.

00:13:42.975 --> 00:13:45.720
Basically, I go over, ah, all,

00:13:45.720 --> 00:13:46.905
ah, all the, ah,

00:13:46.905 --> 00:13:48.980
all the edges of the graph.

00:13:48.980 --> 00:13:52.465
I see what is the probability that the model assigns to this edge.

00:13:52.465 --> 00:13:55.630
And I want the product of these probabilities to be as high as possible.

00:13:55.630 --> 00:13:59.275
And then I want to go over all the non-edges of the graph,

00:13:59.275 --> 00:14:00.550
over all the 0s.

00:14:00.550 --> 00:14:04.705
And I wanna multiply together 1 minus the probability that the edge is there.

00:14:04.705 --> 00:14:08.110
And again, I want these 1 minuses to be as high as possible.

00:14:08.110 --> 00:14:10.240
So our likelihood will be high,

00:14:10.240 --> 00:14:14.605
where the model will assign high probabilities to edges that appear in the graph.

00:14:14.605 --> 00:14:16.015
This is this part,

00:14:16.015 --> 00:14:21.370
and it will assign low probabilities to edges that don't appear,

00:14:21.370 --> 00:14:22.765
ah, in my, ah,

00:14:22.765 --> 00:14:24.370
graph, uh, G, right?

00:14:24.370 --> 00:14:28.540
Because this is- this goes over nodes and edges of the G. And this

00:14:28.540 --> 00:14:32.920
now gives me the likelihood that model F generated the graph,

00:14:32.920 --> 00:14:35.500
uh, G. So now, um,

00:14:35.500 --> 00:14:37.300
the- the question becomes,

00:14:37.300 --> 00:14:38.830
how do I find my model?

00:14:38.830 --> 00:14:40.990
How do I find parameters of F?

00:14:40.990 --> 00:14:45.220
And the way I'm going to find parameters of F is that I'm going to what we call,

00:14:45.220 --> 00:14:50.155
relax or extend my model a bit to account for membership strengths.

00:14:50.155 --> 00:14:55.000
So the way I'm going to do this now is rather than saying that every community C has a,

00:14:55.000 --> 00:14:57.790
um, has a parameter.

00:14:57.790 --> 00:14:59.350
I'm going to, uh,

00:14:59.350 --> 00:15:01.900
assign a membership to have a strength.

00:15:01.900 --> 00:15:07.675
So this would be the strength of our membership of node u to community A, okay?

00:15:07.675 --> 00:15:09.370
And, uh, if the strength is 0,

00:15:09.370 --> 00:15:12.385
this means, node is not a member of that community.

00:15:12.385 --> 00:15:16.375
Um, so now, how are we going to write out, ah, ah,

00:15:16.375 --> 00:15:21.115
the probability that node u and v link to each- to each other, ah,

00:15:21.115 --> 00:15:22.930
if they are a member of a common community

00:15:22.930 --> 00:15:25.945
C. And the way we are going to parameterize this is to say

00:15:25.945 --> 00:15:31.045
it is the strength- is a- it's a product of the strengths of memberships.

00:15:31.045 --> 00:15:33.460
And memberships can only be a non-negative,

00:15:33.460 --> 00:15:34.480
so 0 or more.

00:15:34.480 --> 00:15:36.595
So it's a product of the memberships.

00:15:36.595 --> 00:15:39.190
And, uh, I'll take the negative value of this,

00:15:39.190 --> 00:15:41.830
uh, exponentiate it, and take minus 1 of that.

00:15:41.830 --> 00:15:44.800
So this now means that this should be a valid probability,

00:15:44.800 --> 00:15:46.615
it will be between 0 and 1.

00:15:46.615 --> 00:15:48.085
Um, and the higher,

00:15:48.085 --> 00:15:50.005
the stronger the memberships,

00:15:50.005 --> 00:15:51.940
the bigger the number here will be.

00:15:51.940 --> 00:15:54.865
So E to the minus negative would be something close to 0.

00:15:54.865 --> 00:15:56.980
Which means the probability of link,

00:15:56.980 --> 00:15:58.570
uh, will be higher, right?

00:15:58.570 --> 00:16:01.390
So basically, if membership strengths are low,

00:16:01.390 --> 00:16:03.145
the probability of the edge is low.

00:16:03.145 --> 00:16:05.305
If membership strengths are both high,

00:16:05.305 --> 00:16:07.150
then the product is going to be high,

00:16:07.150 --> 00:16:10.570
which means the probability is going, ah, to be high.

00:16:10.570 --> 00:16:13.285
So this is how we parameterize it.

00:16:13.285 --> 00:16:18.160
So now, uh, nodes v and u can connect via multiple communities, right?

00:16:18.160 --> 00:16:20.455
They can have multiple, uh, communities in common.

00:16:20.455 --> 00:16:24.760
So the way we are going to do this is using the same formula as we had before.

00:16:24.760 --> 00:16:29.080
We are going to say the probability that u and v linked to each other is that they

00:16:29.080 --> 00:16:31.240
create- they link to each other

00:16:31.240 --> 00:16:34.090
through at least one of the communities they have in common.

00:16:34.090 --> 00:16:38.110
So this is again P_C that we have des- derived here.

00:16:38.110 --> 00:16:40.660
And but now I say, out of all these communities they have in

00:16:40.660 --> 00:16:43.735
common at least one of these coin flips has to come up,

00:16:43.735 --> 00:16:46.790
uh, heads, has to create a connection.

00:16:46.790 --> 00:16:50.250
And this is uh, very nice because it, uh,

00:16:50.250 --> 00:16:54.660
allows me to simplify this expression by quite- by quite a lot, right?

00:16:54.660 --> 00:16:57.270
So now this is what we had on the previous slide.

00:16:57.270 --> 00:17:00.320
Probability of a link is parameterized the following way,

00:17:00.320 --> 00:17:02.445
where P_C is, uh,

00:17:02.445 --> 00:17:06.395
parameterized this way by the product of the strands of, uh,

00:17:06.395 --> 00:17:09.310
node u and node v belonging to the common community

00:17:09.310 --> 00:17:13.520
C. So now if I write it- if I write is all out, it- this is, you know,

00:17:13.520 --> 00:17:16.080
it's 1 minus product or our common communities,

00:17:16.080 --> 00:17:19.440
1 minus- 1 minus,

00:17:19.440 --> 00:17:23.040
uh, e to the minus product of Fs.

00:17:23.040 --> 00:17:25.180
So here is how it simp- simplifies this to,

00:17:25.180 --> 00:17:26.980
1s kind of uh- uh,

00:17:26.980 --> 00:17:28.395
subtract each other out.

00:17:28.395 --> 00:17:31.385
Then what I can do, I can take this product and, um, uh,

00:17:31.385 --> 00:17:34.055
distribute it inside and the product of, uh,

00:17:34.055 --> 00:17:36.855
exponentials becomes a sum of the exponents.

00:17:36.855 --> 00:17:39.360
So here it is and why is this elegant?

00:17:39.360 --> 00:17:44.175
Because this now means that this is simply a dot product of- of- of vectors,

00:17:44.175 --> 00:17:48.040
of community memberships of nodes u and nodes v. So

00:17:48.040 --> 00:17:51.965
I can simplify this whole expression into simply saying probability is, uh,

00:17:51.965 --> 00:17:58.170
1 minus e raised to the dot-product of the community membership vector for node u times

00:17:58.170 --> 00:18:01.380
the community membership vector for node v. So

00:18:01.380 --> 00:18:04.915
this is super cool because it's a very nice type expression.

00:18:04.915 --> 00:18:08.760
So now that I have defined the probability of an edge between a pair of nodes

00:18:08.760 --> 00:18:12.665
based on their vector representations of the community memberships,

00:18:12.665 --> 00:18:16.185
now, you know, how will my graph likelihood look like?

00:18:16.185 --> 00:18:20.460
It's a product over the edges times the product of 1 minus,

00:18:20.460 --> 00:18:22.895
uh, probabilities of non-edges.

00:18:22.895 --> 00:18:26.135
And I simply, you know inserted this in, this is here.

00:18:26.135 --> 00:18:28.050
When I insert it here again, these, uh,

00:18:28.050 --> 00:18:32.645
1s will, um, cancel out and I just get the exponent.

00:18:32.645 --> 00:18:35.450
Now, if I take this,

00:18:35.450 --> 00:18:38.670
uh- uh likelihood and further simplify it a bit,

00:18:38.670 --> 00:18:40.980
I can write it in the following form, right?

00:18:40.980 --> 00:18:44.614
It's basically rather than directly optimizing the likelihood,

00:18:44.614 --> 00:18:46.890
we like to optimize the log-likelihood.

00:18:46.890 --> 00:18:51.420
And the reason why we optimize log-likelihoods is for numerical stability, right?

00:18:51.420 --> 00:18:53.595
Probabilities are small numbers,

00:18:53.595 --> 00:18:57.070
product of small numbers are even smaller numbers and then

00:18:57.070 --> 00:19:00.815
numerical instabilities and imprecisions started to play a role,

00:19:00.815 --> 00:19:03.450
so you never work with raw probabilities,

00:19:03.450 --> 00:19:06.630
you always work with logarithmic probabilities,

00:19:06.630 --> 00:19:08.280
a logarithm of the probability.

00:19:08.280 --> 00:19:11.900
So if I apply now, uh, a logarithm here,

00:19:11.900 --> 00:19:16.490
and it is a valid transform because logarithm is a monotonic transformation.

00:19:16.490 --> 00:19:21.304
So when I maximize the max- maximum is attained at the same position,

00:19:21.304 --> 00:19:23.930
regardless of whether I- whether I pass this through

00:19:23.930 --> 00:19:27.390
a logarithmic transformation because it's, um, monotonic.

00:19:27.390 --> 00:19:32.475
So if I apply a log and the log products become summations,

00:19:32.475 --> 00:19:38.970
so I get this summation over log 1 minus the first term plus a summation,

00:19:38.970 --> 00:19:44.615
over the log x- the second term and log and exponent again cancel each other out.

00:19:44.615 --> 00:19:46.800
So here only the summation survive.

00:19:46.800 --> 00:19:48.390
And this is now our objective.

00:19:48.390 --> 00:19:51.930
We'll call it the log-likelihood that we tried to, uh, optimize.

00:19:51.930 --> 00:19:54.490
So we have simplified it, uh, a lot.

00:19:54.490 --> 00:19:56.845
So how do we, uh,

00:19:56.845 --> 00:19:59.720
optimize, uh, this likelihood F?

00:19:59.720 --> 00:20:02.160
We simply start with random memberships, uh,

00:20:02.160 --> 00:20:05.010
Fs, and then we iterate until convergence.

00:20:05.010 --> 00:20:08.050
We basically, uh, update the membership F of

00:20:08.050 --> 00:20:11.350
node u while fixing the memberships of all other nodes.

00:20:11.350 --> 00:20:13.835
Um, and we can do gradient descent,

00:20:13.835 --> 00:20:19.180
where we take small changes in F that lead to the increase in the log-likelihood.

00:20:19.180 --> 00:20:21.075
Uh, and that is um- uh,

00:20:21.075 --> 00:20:22.895
all- all we do.

00:20:22.895 --> 00:20:26.260
And, uh, just to say one more is when we compute

00:20:26.260 --> 00:20:30.590
this derivative- partial derivative of log-likelihood,

00:20:30.590 --> 00:20:32.750
uh, here is the expression.

00:20:32.750 --> 00:20:34.860
What we find out is that this depends,

00:20:34.860 --> 00:20:39.125
it is a summation over the neighbors of node, uh, u.

00:20:39.125 --> 00:20:43.650
And, uh, you see how we're now multiplying the,

00:20:43.650 --> 00:20:45.820
uh- the- the- the,

00:20:45.820 --> 00:20:47.775
uh, community membership vectors.

00:20:47.775 --> 00:20:49.320
We multiply it with, uh,

00:20:49.320 --> 00:20:51.845
community membership vector of node v. But

00:20:51.845 --> 00:20:54.430
then the term that is very expensive to compute is

00:20:54.430 --> 00:20:59.835
the second term because here the summation goes over non neighbors of node u.

00:20:59.835 --> 00:21:03.275
And because usually nodes have small number of neighbors, uh,

00:21:03.275 --> 00:21:06.890
this means that this summation goes over- over the entire network, right?

00:21:06.890 --> 00:21:11.220
Basically this plus this is the entire network and this is too slow,

00:21:11.220 --> 00:21:13.315
uh, in practice to do.

00:21:13.315 --> 00:21:17.160
But we can efficiently approximate these, uh,

00:21:17.160 --> 00:21:20.685
computing this expression by realizing that this blue part,

00:21:20.685 --> 00:21:22.660
um, is, uh, slow to compute.

00:21:22.660 --> 00:21:25.130
But we can expand it, um,

00:21:25.130 --> 00:21:27.575
and uh- uh, make it,

00:21:27.575 --> 00:21:29.075
uh, to compute much faster.

00:21:29.075 --> 00:21:31.130
And basically the idea is very simple, right?

00:21:31.130 --> 00:21:34.300
If you wanna sum, over, um, uh,

00:21:34.300 --> 00:21:37.440
a set of nodes that are not neighbors of a given node,

00:21:37.440 --> 00:21:40.030
then you can write out this summation the following way.

00:21:40.030 --> 00:21:42.960
You say, let me sum up over all the nodes in the network,

00:21:42.960 --> 00:21:45.690
but then only subtract the contributions from

00:21:45.690 --> 00:21:48.900
the nodes that are actually neighbors of this node u. All right?

00:21:48.900 --> 00:21:54.105
So this now is the summation over all nodes that are non-neighbors of node u.

00:21:54.105 --> 00:21:57.335
It's basically all the nodes minus the u itself,

00:21:57.335 --> 00:21:59.360
minus the neighbors of node u.

00:21:59.360 --> 00:22:02.840
Uh, and the point is because mo- most of the nodes have low degree,

00:22:02.840 --> 00:22:04.445
you can cache this part,

00:22:04.445 --> 00:22:06.200
you just compute it once,

00:22:06.200 --> 00:22:08.505
and then you only have to update these terms.

00:22:08.505 --> 00:22:11.815
And this way, you can make your method, uh, scale, uh,

00:22:11.815 --> 00:22:15.010
much- much faster and run uh- uh,

00:22:15.010 --> 00:22:18.385
to be able to be applied to much bigger graphs.

00:22:18.385 --> 00:22:20.820
So let me summarize,

00:22:20.820 --> 00:22:23.495
uh, the part of this lecture- this part of the lecture.

00:22:23.495 --> 00:22:27.950
We talked about generative modeling of networks.

00:22:27.950 --> 00:22:31.325
We talked about how we can fit a generative model to

00:22:31.325 --> 00:22:34.965
the underlying network using log-likelihood and gradient descent.

00:22:34.965 --> 00:22:39.760
In our case, we have defined our generative model of networks,

00:22:39.760 --> 00:22:43.700
uh, that is based on node community memberships.

00:22:43.700 --> 00:22:50.300
So what this means is that by feeding our model to the underlying network structure,

00:22:50.300 --> 00:22:52.280
we can then, uh,

00:22:52.280 --> 00:22:56.190
get the co- node community memberships out of that model.

00:22:56.190 --> 00:22:58.890
So essentially we are doing community detection by

00:22:58.890 --> 00:23:01.900
fitting a generative model to the underlying,

00:23:01.900 --> 00:23:04.095
uh- to the underlying, uh, graph.

00:23:04.095 --> 00:23:08.600
So BigCLAM is a method that- that defines a model,

00:23:08.600 --> 00:23:11.950
uh- affiliation graph model based on the- based on

00:23:11.950 --> 00:23:16.680
the node community affiliations and then generates the underlying network structure.

00:23:16.680 --> 00:23:21.935
So what we then did is defined the fitting procedure that says,

00:23:21.935 --> 00:23:25.720
given that the underlying network structure and

00:23:25.720 --> 00:23:29.150
the assumption that the network was generated by the BigCLAM,

00:23:29.150 --> 00:23:34.415
what is the optimal assignment of nodes to communities such that,

00:23:34.415 --> 00:23:36.460
uh, we- we get the underlying,

00:23:36.460 --> 00:23:38.300
uh, real network.

00:23:38.300 --> 00:23:40.905
Um, in this procedure of model fitting,

00:23:40.905 --> 00:23:43.980
we defined the notion of log-likelihood, um,

00:23:43.980 --> 00:23:45.235
and then said, you know,

00:23:45.235 --> 00:23:48.290
what is the most likely model to have gen- generated this data?

00:23:48.290 --> 00:23:51.820
And we were able to feed the model, uh, to the data.


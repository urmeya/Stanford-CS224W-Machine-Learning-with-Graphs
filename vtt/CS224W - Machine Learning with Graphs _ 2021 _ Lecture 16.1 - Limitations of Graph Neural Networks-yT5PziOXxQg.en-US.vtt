WEBVTT
Kind: captions
Language: en-US

00:00:04.010 --> 00:00:07.065
Welcome everyone, uh, to the class.

00:00:07.065 --> 00:00:09.840
What we are going to talk today about is,

00:00:09.840 --> 00:00:11.985
um, some advanced topics,

00:00:11.985 --> 00:00:15.660
and in particular we are going first to talk about limitations of,

00:00:15.660 --> 00:00:17.324
uh, graph neural networks,

00:00:17.324 --> 00:00:20.700
um, and then we are also going about how do we improve

00:00:20.700 --> 00:00:24.840
their expressive power and how do we, um,

00:00:24.840 --> 00:00:26.880
then, uh, study, uh,

00:00:26.880 --> 00:00:33.075
how robust are graph neural networks against, um, adversarial attacks.

00:00:33.075 --> 00:00:37.470
So the idea for today's lecture is- lecture is the following.

00:00:37.470 --> 00:00:40.950
Um, what would the perfect GNN model do, right,

00:00:40.950 --> 00:00:44.960
if we wanna say about what are some limitations of graph neural networks and

00:00:44.960 --> 00:00:49.525
especially when we looked at their expressive power in terms of the,

00:00:49.525 --> 00:00:52.665
um, in terms of the, um,

00:00:52.665 --> 00:00:55.485
uh, WL kernel, right?

00:00:55.485 --> 00:00:58.070
If we go through a thought experiment then we could say,

00:00:58.070 --> 00:01:01.430
what would a perfect graph neural network do?

00:01:01.430 --> 00:01:05.045
And the k-layer graph neural network embeds a node

00:01:05.045 --> 00:01:09.400
based on the K-hop neighborhood structure around that node, right?

00:01:09.400 --> 00:01:11.660
And this picture tries to illustrate that.

00:01:11.660 --> 00:01:16.295
That is that basically if I wanna embed this particular, um, node here,

00:01:16.295 --> 00:01:20.210
I can take the graph structure around this node and then,

00:01:20.210 --> 00:01:21.560
um, through message passing,

00:01:21.560 --> 00:01:23.770
I wanna compute the embedding of that node;

00:01:23.770 --> 00:01:27.110
and a perfect GNN would be such that it would build

00:01:27.110 --> 00:01:32.060
an injective function between the neighborhood structure around the target node,

00:01:32.060 --> 00:01:35.510
um, and the embedding that it produces.

00:01:35.510 --> 00:01:38.120
So essentially, what we'd like to do is- with

00:01:38.120 --> 00:01:42.110
a perfect GNN would take every different node neighborhood structure,

00:01:42.110 --> 00:01:44.990
uh, and embed it into a different position,

00:01:44.990 --> 00:01:47.160
uh, in the embedding space.

00:01:47.160 --> 00:01:49.695
Um, there are two important, uh,

00:01:49.695 --> 00:01:53.195
observations, uh, building on this intuition.

00:01:53.195 --> 00:01:56.480
First is that a perfect GNN will do the following, right?

00:01:56.480 --> 00:02:01.070
If two nodes have the same neighborhood structure around the- around them,

00:02:01.070 --> 00:02:03.215
then they will have the same embedding.

00:02:03.215 --> 00:02:06.150
Again, here we are assuming there is no discriminative,

00:02:06.150 --> 00:02:08.120
uh, feature information given to us.

00:02:08.120 --> 00:02:11.180
So v1 and v2 in this- in this,

00:02:11.180 --> 00:02:12.560
uh, graph, with, let's say,

00:02:12.560 --> 00:02:15.620
two connected components will be embedded into

00:02:15.620 --> 00:02:19.130
the- exactly the same point because their neighborhood structure,

00:02:19.130 --> 00:02:21.380
uh, around them is identical,

00:02:21.380 --> 00:02:23.015
and of course, right?

00:02:23.015 --> 00:02:27.050
If we have two nodes that have different neighborhood structures, then, um,

00:02:27.050 --> 00:02:30.560
we'd like them to be embedded into different points in the space

00:02:30.560 --> 00:02:34.890
because the net- the- the neighborhood structures of these two nodes are different.

00:02:34.890 --> 00:02:36.020
One is in a triangle,

00:02:36.020 --> 00:02:37.370
the other one is in a square,

00:02:37.370 --> 00:02:40.020
so they should be embedded into different points.

00:02:40.020 --> 00:02:42.230
So that's kind of what we'd like to do.

00:02:42.230 --> 00:02:44.375
That's what we'd like our perfect,

00:02:44.375 --> 00:02:46.070
uh, GNN to do.

00:02:46.070 --> 00:02:51.925
However, these observations, 1 and 2 may not be always, uh, true.

00:02:51.925 --> 00:02:54.795
For example, the- the observation 1,

00:02:54.795 --> 00:02:57.845
um, can have kind of the following, uh, issues.

00:02:57.845 --> 00:03:02.300
Even though two nodes may have the same neighborhood structure around them,

00:03:02.300 --> 00:03:04.115
we may wanna assign,

00:03:04.115 --> 00:03:06.200
um, different embeddings to them.

00:03:06.200 --> 00:03:08.450
Um, and this is because, uh, you know,

00:03:08.450 --> 00:03:14.140
nodes may appear in different positions or in different locations in the graph.

00:03:14.140 --> 00:03:16.020
Um, and we, uh,

00:03:16.020 --> 00:03:18.285
we are going to call, uh, uh,

00:03:18.285 --> 00:03:20.360
this notion of a position in the graph and

00:03:20.360 --> 00:03:23.555
these tasks that require us understanding the position,

00:03:23.555 --> 00:03:25.970
we'll call those position-aware tasks.

00:03:25.970 --> 00:03:27.695
And I'm going to define this more, uh,

00:03:27.695 --> 00:03:30.350
precisely throughout, uh, the lecture, right?

00:03:30.350 --> 00:03:33.060
So basically even a- a perfect GNN,

00:03:33.060 --> 00:03:35.040
that has that, um, injective,

00:03:35.040 --> 00:03:37.430
uh, function between the neighborhood structure and

00:03:37.430 --> 00:03:40.595
the embedding will fail at these, uh, tasks.

00:03:40.595 --> 00:03:46.115
Uh, for example, here if I have a simple grid graph and I have nodes v1 and v2,

00:03:46.115 --> 00:03:49.850
and I'd like them to be embedded into different points in space because they

00:03:49.850 --> 00:03:53.840
are kind of at the opposite ends of the- of the underlying uh,

00:03:53.840 --> 00:03:57.320
graph, actually a graph neural network is going to embed them, uh,

00:03:57.320 --> 00:04:01.970
into the same position because the neighborhood structure around them is identical.

00:04:01.970 --> 00:04:03.440
They are both in the corner,

00:04:03.440 --> 00:04:05.080
uh, of the grid.

00:04:05.080 --> 00:04:08.830
Um, so this is kind of one issue that, uh,

00:04:08.830 --> 00:04:11.510
graph neural networks, as we have defined them so far,

00:04:11.510 --> 00:04:13.055
uh, are not able to do.

00:04:13.055 --> 00:04:15.410
Um, the second important, uh,

00:04:15.410 --> 00:04:18.590
implication of the observation 2 is that,

00:04:18.590 --> 00:04:22.600
um, GNNs that we have introduced so far are kind of not perfect, right?

00:04:22.600 --> 00:04:24.810
Their expressive power, um,

00:04:24.810 --> 00:04:26.580
is not- is not enough, right?

00:04:26.580 --> 00:04:28.820
Uh, and in- particularly in lecture 9,

00:04:28.820 --> 00:04:32.780
we discussed that the expressive power of our graph neural network,

00:04:32.780 --> 00:04:37.025
this message passing graph neural network with indiscriminative features, uh,

00:04:37.025 --> 00:04:41.150
it's expressive power is upper binded- bounded by the Weisfeiler-Lehman,

00:04:41.150 --> 00:04:43.420
um, graph isomorphism test.

00:04:43.420 --> 00:04:46.155
So, uh, for example, um,

00:04:46.155 --> 00:04:52.230
if I have nodes v1 on a cycle of length 3 and a node v2 on cycle of length 4,

00:04:52.230 --> 00:04:54.860
if I look at the structure of their computation graphs,

00:04:54.860 --> 00:04:58.490
um, the structure of the two computation graphs will be the same.

00:04:58.490 --> 00:05:01.715
So without any discriminative node features

00:05:01.715 --> 00:05:04.040
or if assuming all node features are the same,

00:05:04.040 --> 00:05:07.775
graph neural network is not going to be able to distinguish, um,

00:05:07.775 --> 00:05:13.040
or it won't be able to assign different embeddings to nodes 1 and, uh, 2.

00:05:13.040 --> 00:05:18.380
So basically nodes v1 and v2 will always be embedded into the same space, uh,

00:05:18.380 --> 00:05:21.260
under the assumption that there is no useful node features,

00:05:21.260 --> 00:05:23.525
um, because their computation graphs,

00:05:23.525 --> 00:05:25.880
uh, are identical even though one

00:05:25.880 --> 00:05:29.485
resides in a triangle and the other one resides in a square.

00:05:29.485 --> 00:05:33.830
So, uh, the plan for the lecture today is that we wanna resolve both of

00:05:33.830 --> 00:05:38.060
these issues by building or designing more expressive graph neural networks.

00:05:38.060 --> 00:05:41.710
Uh, and the way we are going to fix these issues is the following.

00:05:41.710 --> 00:05:44.980
Uh, to fix the issue, um, uh, one,

00:05:44.980 --> 00:05:48.500
we are going to create node embeddings based on their positions in the graph.

00:05:48.500 --> 00:05:52.250
Um, and the idea will be that we wanna create reference points in

00:05:52.250 --> 00:05:56.930
the graph and then quantify the position of a node against those,

00:05:56.930 --> 00:05:59.930
uh, reference points, and the class of models that

00:05:59.930 --> 00:06:03.490
allow us to do this are called position-aware graph neural networks.

00:06:03.490 --> 00:06:07.365
And then to fix the issue number- number 2, um,

00:06:07.365 --> 00:06:09.600
we- we- we are going to build, uh,

00:06:09.600 --> 00:06:14.530
message-passing GNNs that are more expressive than the WL, uh, test.

00:06:14.530 --> 00:06:18.210
Um, and the method, an example of such a mes- message- method,

00:06:18.210 --> 00:06:21.060
is called Identity-aware graph neural network.

00:06:21.060 --> 00:06:23.175
So this is what is going to be the,

00:06:23.175 --> 00:06:25.425
uh, plan for the, uh,

00:06:25.425 --> 00:06:27.495
for the first part of the lecture,

00:06:27.495 --> 00:06:31.585
and then in the last part I'm going to talk about adversarial attacks.

00:06:31.585 --> 00:06:34.395
So, uh, here is our approach,

00:06:34.395 --> 00:06:36.345
and this is how we wanna think about it.

00:06:36.345 --> 00:06:39.285
So, um, we will use the following thinking.

00:06:39.285 --> 00:06:41.610
Um, given two different, uh, inputs,

00:06:41.610 --> 00:06:43.710
for example, nodes, uh, graphs, uh,

00:06:43.710 --> 00:06:45.885
uh, edges, um, er,

00:06:45.885 --> 00:06:48.165
let's assume they are labeled differently,

00:06:48.165 --> 00:06:49.910
and we are going to say that, you know, kind of,

00:06:49.910 --> 00:06:51.650
the model fails, um,

00:06:51.650 --> 00:06:53.720
if it- i- if it is always going to

00:06:53.720 --> 00:06:56.750
assign the same embedding to these different inputs,

00:06:56.750 --> 00:06:57.880
or these different objects,

00:06:57.880 --> 00:07:02.030
and a successful model is going to assign different embeddings to these,

00:07:02.030 --> 00:07:04.490
uh, different, uh, types of objects.

00:07:04.490 --> 00:07:06.110
Um, so if we focus,

00:07:06.110 --> 00:07:08.705
let's say on node-level embeddings, then, you know,

00:07:08.705 --> 00:07:11.060
embeddings in a GNN are determined,

00:07:11.060 --> 00:07:13.570
uh, by the underlying computation graph.

00:07:13.570 --> 00:07:14.690
Right? And in my case,

00:07:14.690 --> 00:07:18.050
imagine again I have a graph with two connected components.

00:07:18.050 --> 00:07:20.300
I have two vertices, v1 and v2.

00:07:20.300 --> 00:07:23.450
Imagine v1 and v2 are labeled with different labels.

00:07:23.450 --> 00:07:24.815
V1 is labeled with A,

00:07:24.815 --> 00:07:26.330
v2 is labeled with B.

00:07:26.330 --> 00:07:29.480
The goal will be to build a graph neural network that is

00:07:29.480 --> 00:07:33.065
going to assign different embeddings to node v1,

00:07:33.065 --> 00:07:34.930
then to the node v2.

00:07:34.930 --> 00:07:38.910
Again, under the assumption that node features are the same,

00:07:38.910 --> 00:07:42.675
uh, or, non-discriminative between uh, v1 and v2;

00:07:42.675 --> 00:07:45.680
and perhaps what is- you may seem- or you may say

00:07:45.680 --> 00:07:48.425
striking or interesting is that the models we have,

00:07:48.425 --> 00:07:50.690
uh, um, developed so far,

00:07:50.690 --> 00:07:53.450
uh, actually fail to distinguish v1 and v2.

00:07:53.450 --> 00:07:54.740
Like, even though we have built

00:07:54.740 --> 00:07:58.625
so much super cool machinery that works amazingly well in practice, uh,

00:07:58.625 --> 00:08:01.680
and empirically, um, we still cannot

00:08:01.680 --> 00:08:05.565
distinguish v1 and v2 in this kind of corner case, uh, example.

00:08:05.565 --> 00:08:08.915
So what we are going to do is to understand how can we resolve this?

00:08:08.915 --> 00:08:13.765
How can we build a network that- a graph neural network that will be able to distinguish,

00:08:13.765 --> 00:08:15.900
uh, basically, uh, v1 and v2.

00:08:15.900 --> 00:08:18.240
So meaning assign them different embeddings,

00:08:18.240 --> 00:08:21.140
so that then we can assign v1, one label,

00:08:21.140 --> 00:08:23.075
and we can assign v2, the other label,

00:08:23.075 --> 00:08:27.700
because we cannot assign them different label if they both map into the same point.

00:08:27.700 --> 00:08:30.300
So a naive solution to this,

00:08:30.300 --> 00:08:32.700
that kind of doesn't work, would be,

00:08:32.700 --> 00:08:35.340
uh, to use one hold and- one-hot encoding.

00:08:35.340 --> 00:08:36.665
So we would like to say,

00:08:36.665 --> 00:08:38.375
Okay, we don't have any features,

00:08:38.375 --> 00:08:41.060
but let us- let's assign each node a

00:08:41.060 --> 00:08:45.335
different ID and then we can always differentiate different nodes,

00:08:45.335 --> 00:08:48.620
um, in- in a graph or different edges or even different graphs.

00:08:48.620 --> 00:08:50.120
So if I have, you know,

00:08:50.120 --> 00:08:51.530
two- two graphs here,

00:08:51.530 --> 00:08:52.970
uh, as we had before,

00:08:52.970 --> 00:08:56.695
I could simply assign a one-hot encoding to every node,

00:08:56.695 --> 00:08:57.950
um, and now of course,

00:08:57.950 --> 00:09:00.665
because nodes now have, uh, features,

00:09:00.665 --> 00:09:05.020
it will be- the computational graphs will be distinguishable because,

00:09:05.020 --> 00:09:07.330
you know, v1 will have, uh,

00:09:07.330 --> 00:09:10.720
two children, one with 0100,

00:09:10.720 --> 00:09:12.730
and the other one with, you know, 001;

00:09:12.730 --> 00:09:14.400
and v2 is going to have,

00:09:14.400 --> 00:09:17.910
um, different, um, types of, um, uh,

00:09:17.910 --> 00:09:19.650
neighbors because their- their,

00:09:19.650 --> 00:09:22.440
um, one-hot encodings, uh, will be different.

00:09:22.440 --> 00:09:23.820
Then, even though with the two,

00:09:23.820 --> 00:09:25.755
if they will be the same at the first level,

00:09:25.755 --> 00:09:27.440
they won't be the same on the second level.

00:09:27.440 --> 00:09:30.785
So basically, computational graphs will be different,

00:09:30.785 --> 00:09:35.300
so our GNN will be able to, uh, distinguish them.

00:09:35.300 --> 00:09:37.920
Um, what are the issues with this?

00:09:37.920 --> 00:09:40.500
The- there are two very important issues.

00:09:40.500 --> 00:09:42.810
First is that this approach is not scalable;

00:09:42.810 --> 00:09:46.169
meaning we need an order N feature dimensions

00:09:46.169 --> 00:09:49.150
where N- N is the number of nodes to be able to encode, right?

00:09:49.150 --> 00:09:52.705
Basically, we need a separate feature for every individual node,

00:09:52.705 --> 00:09:53.740
and if, you know,

00:09:53.740 --> 00:09:57.475
if we have a 10,000 or a 100,000 or a million node network,

00:09:57.475 --> 00:09:59.800
then every node has now a million features,

00:09:59.800 --> 00:10:02.695
basically a one-hot encoding of its ID.

00:10:02.695 --> 00:10:06.550
Uh, and then the second problem is that this is in- this is not inductive;

00:10:06.550 --> 00:10:09.490
meaning it won't generalize to new- new nodes or

00:10:09.490 --> 00:10:13.620
new graphs because these one-hot encodings are kind of arbitrary,

00:10:13.620 --> 00:10:15.415
node ordering is arbitrary,

00:10:15.415 --> 00:10:17.290
so the map- the network,

00:10:17.290 --> 00:10:20.580
it could basically learn according to that node ordering,

00:10:20.580 --> 00:10:21.835
and then if we, um,

00:10:21.835 --> 00:10:23.930
try to transfer this to a new graph,

00:10:23.930 --> 00:10:26.630
or if a new node appears in the network,

00:10:26.630 --> 00:10:28.700
this won't- this won't work, right.

00:10:28.700 --> 00:10:33.020
If a new node appears we'll have to expand- extend the feature dimensionality because

00:10:33.020 --> 00:10:37.655
we wanna encode- use one-hot encoding for that node as well and we'd have to retrain.

00:10:37.655 --> 00:10:40.055
Uh, or if we wanna transfer to a new graph,

00:10:40.055 --> 00:10:42.820
we have no guarantees because, uh,

00:10:42.820 --> 00:10:46.310
the one-hot encoding and node IDs are kind of arbitrary,

00:10:46.310 --> 00:10:48.980
so it won't, uh, it won't generalize.

00:10:48.980 --> 00:10:50.540
So this is why, you know,

00:10:50.540 --> 00:10:52.205
this is a bad- bad idea,

00:10:52.205 --> 00:10:54.830
but, ah, this idea kind of to enrich,

00:10:54.830 --> 00:10:57.470
uh, the nodes so that we can, um,

00:10:57.470 --> 00:11:00.725
differentiate different computational graphs is a good idea.

00:11:00.725 --> 00:11:02.465
Just one-hot encoding, uh,

00:11:02.465 --> 00:11:04.560
doesn't work in this case.


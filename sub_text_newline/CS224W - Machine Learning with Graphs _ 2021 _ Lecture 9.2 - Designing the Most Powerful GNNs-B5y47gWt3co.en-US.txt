So given the insights so far,
let's now go and design the most powerful graph neural network.
So let's go and design the most expressive,
uh, graph neural network.
And let's develop the theory that will allow us, uh, to do that.
So the key observation so far is that the expressive power of
a graph neural network can be characterized by
the expressive power of the neighborhood aggregation function they use.
Because the more expressive
the neighborhood aggregation leads to a more expressive graph neural network.
And we saw that if the neighborhood aggregation is injective,
this leads to the most expressive, uh, GNN.
A neighborhood aggregation being injective,
it means that whatever is the number and the features of the- of the children,
you- you map every different combination
into a different output, so no information get- gets lost.
So let's do the following next,
let's theoretically analyze the expressive power of different, uh, aggregation functions.
So the way you think of neighborhood aggregation, uh,
basically taking the information from the children and aggregating is that
neighborhood aggregation can be abstracted as a function over a multi-set.
A multi-set is simply a set with repeated elements, right?
So if you say, "I'm a node here and I aggregated
from two neighbors for two children," this is the same as saying,
I have a set of children,
uh, two yellow guys,
and I need to aggregate information from them, right?
And of course, in a multi-set,
um, nodes can have different colors,
node can have different features, so, you know,
could say, uh-huh, I have,
uh, I have two children,
one with yellow attribute and the other one with
blue attribute versus some other node has, um,
three children, two of them with yellow attribute or yellow feature,
and one, uh, with the blue feature.
And then we aggregate this information,
we want the- the new message,
the aggregated information not to be lost.
Somehow we wanna the,
er, to- in this aggregation,
in this compression step,
basically to retain all the information we know about the children, right?
So here we'd wanna say, two yellow and a blue,
and here we'd wanna say one yellow and one blue so that
these two sets- multi-sets still remain, uh,
distinguishable as- as we are aggregating them, uh, to their parent,
so that then we aggregate this parent further to the super parent,
uh, no information, uh, gets lost.
So let's look at, uh,
the neighborhood information- aggregation functions used by the two,
uh, models that we have discussed so far in the class.
First, we'll talk about, uh,
GCN, which uses mean pooling.
It uses element-wise mean pooling over neighborhood node features,
and then let's talk about the max pooling variant of GraphSAGE that uses
element-wise maximum pooling over neighboring, uh, node features.
And let's see what is the expressive power of mean
and what is the expressive power of max, uh, pooling.
So, uh, let's first talk about, uh, GCN,
so the mean pooling when you
average the messages coming from the ne- from the children.
Uh, if we take the, er, element-wise mean,
then in a GCN,
it's followed by a linear function,
and a ReLU, a- activation function.
Um, and, er, what is,
uh, what is the observation?
The observation function is that GCN's aggregation function cannot
disting- distinguish multi- different multi-sets with the same,
kind of, proportion of colors, right?
So for example, um,
this is a failure case.
When you average together messages,
it doesn't- it doesn't matter whether you average
one yellow and one blue message or whether you average two yellow and two blue messages.
At the end, the average is the same, all right?
And this is the failure case of the average.
It- it will combine these two multi-sets,
it will aggregate them into the same,
uh, into the same message.
So it means it will lose information in the case that here are,
um, one and one,
and here is two and two,
because the ratio is the same.
So let me be a bit more precise and give you a proper example.
Let's for simplicity, assume that node colors are represented as one-hot encodings, right?
So now every node- every node, uh,
has a feature vector that simply encodes what color is the color of the node, right?
That is its, uh, feature vector.
And this is just, kind of,
uh, a- a way to illustrate, uh,
this concept and what happens when we do, uh, aggregation, right?
So for example, when you do,
uh, average of, uh, two vectors,
uh, 1,0, and 0,1,
you- you- you get, uh, uh, half and half.
So that's your aggregated message now of these two, uh, feature vectors.
Uh, in this case,
when you have a multi-set, again,
of two yellow and two blue,
here are the corresponding feature representations.
If I take the e- the- the element-wise average of these,
uh, four vectors, I also get half half.
So it means that even if I then apply some non-linear transformation,
and activation, and so on, at the end,
I will get the same output because the aggregation of, uh,
yellow and blue is the same as the aggregation of two yellows and two blues.
Even though I encode them in- with different feature vectors,
so yellow and blue nodes are de- definitely distinguishable because, you know,
one has the first element set to 1,
and the other one has the second,
uh, element, uh, set to 1.
So you see how mean pooling can basically aggregate, uh,
multi-sets that have the same proportion
of nodes of one type of feature versus the other type of feature,
um, into the same representation,
regardless of what is the total number of nodes or what is
the total size of the underlying, uh, multi-set.
So this is the issue with the mean pooling.
This is a failure case of mean pooling
because it won't be able to distinguish a multi-set
of size 2 versus size 4 if the proportion of features is the same in both.
And now, let's look at, uh, um, um, uh,
GraphSAGE, uh, max-pooling, uh, variant of it.
So we- in the GraphSAGE,
we apply our multi-layer perceptron transformation,
and then take, uh, uh, element-wise maximum pooling.
Um, and what we learn here is that maximum pooling function cannot
distinguish different multi-sets with the same set of distinct colors, right?
So what does- what does this mean,
is that all of these different multi-sets will
be aggregated into the same representation.
Why is that the case is because as long as
multi-sets have the same set of distinct colors,
then the me- whatever is the maximum, right?
Maximum will be one of the colors,
that maximum is the same regardless of how many different nodes,
uh, and what are the proportions of colors in the,
um, uh, in the multi-set.
So to give you an example,
imagine I have these three different multi-sets,
I, uh, I encode this colors using some encoding,
then I apply some nonlinear transformation
like an MLP to it because this is what GraphSAGE does,
and let's assume without loss of generality,
that basically now, you know,
these colors get transformed to some new colors,
and we encode these colors with one-hot encoding.
So I- so everything is disti- distinguishable at to this level.
But the problem is that now if you take, uh,
element-wise, meaning coordinate-wise, maximum,
in all these different cases,
you get the same aggregation,
you get the same maximum value,
you get 1 and 1.
So this means that regardless whether the node has,
uh, two children, four children,
or three children, um,
and whatever is the ratio between blue, and, uh, uh,
yellow, in all cases,
the maximum pooling will give me the same representation.
So it means that all this information here gets lost and
all these different multi-sets get mapped to the same representation.
So clearly, uh, maximum pooling is not
an injective operator because it maps different inputs into the same,
uh, output, and that's the problem.
You get these collisions and information, uh, gets lost,
and that decreases the expressive power,
uh, of the graph neural network.
So, let's summarize what we have learned so far.
We have analyzed the expressive power of graph neural networks,
and the main takeaways are the following;
the expressive power of a graph neural network can be characterized by
the expressive power of its neighborhood aggregation function, right?
So the message aggregation function.
Neighborhood aggregation is a function over multi-sets,
basically sets with repeating elements.
Um, and GCN and GraphSAGE aggregation functions
fail to distinguish some of the basic multi-sets.
Meaning these two aggregation functions,
mean and maximum, are not injective,
which means different inputs get mapped into the same output,
and this way, the information gets lost.
Therefore, GCN and GraphSAGE are not maximally powerful graph neural networks.
They're not maximally expressive,
uh, graph neural networks.
So let's now move on and say,
can we design the most expressive graph neural network, right?
So our goal will be to design maximally powerful graph neural network,
uh, among all possible message-passing graph neural networks.
And this will- the way we are going to do this is to-
to design an injective neighborhood aggregation function.
So basically a neighborhood aggregation function that will never lose information
when it aggregates from the children to create a message, uh, for the parent.
So the property will be an injectivity of the aggregation function, right?
Um, so the goal is design a neural network that can model this injective,
uh, multi-set function because that's the aggregation operator.
So, uh, here is a very,
uh, useful, uh, theorem.
The theorem says that any injective multi-set function
can be expressed in the following way.
So it can be- so if I have a set of elements, right,
I have my multi-set function S, uh,
multi-set, uh, that has a set of elements,
then the way I can write an injective function over a multi-set is I can write it as,
uh, I apply my function f to every element of the multi-set,
I sum up these,
uh, uh, these, uh,
outputs of f and then I apply another non-linear function, okay?
So the point is that if I want to have an injective set over,
uh, over a multi-set,
then I can realize this injective, uh,
function by having two functions, f and Phi,
where f I apply to every element of the multi-set,
I sum up the outputs of f,
and then I apply another function,
another transformation, uh, Phi to it.
And this means that,
uh, this is how,
um, a gr- um,
a multi-set function can be expressed,
uh, and it will still be, uh, injective.
So the way you can think of the proof,
what is the intuition?
The intuition is that our f can, uh,
produce kind of one-hot encodings of colors, right?
So f is injective for different node,
it produces a different output, um,
and these outputs need to be different enough so that when you sum them up,
you don't lose any information.
So in some sense, if- if f takes colors and produces their one-hot encodings,
this means that then you can basically by summing up,
you are counting how many elements of each color you have.
And this way, you don't lose any information, right?
You say, uh huh, I have one yellow node and I have, uh,
two blue nodes, and that is kind of the way you can think of f, right?
f takes the colors and- and kind of encodes them as one-hot,
so that when you sum them up,
you basically count how many different colors you have.
Of course, f needs to be a function that does, that does this.
If f does not do this for you,
um, this won't, uh, this won't work.
So f has to be a very special, uh,
function, um, and then,
uh, it will, uh, work out.
So now the question is,
what kind of function f and Phi can I use?
How do I define them?
And we're going to u- to use them to basically define them with a neural network.
We are going to define them using a multi-layer perceptron.
Um, and why would we want to define it just using a perceptron?
There reason is that, uh,
there is something called an universal approximation theorem,
and it goes as follows: So, uh,
one hidden layer uh, multiple,
uh, layer perceptron with sufficiently large, uh,
hidden layer dimensionality and appropriate non-linearity can
approximate any continuous function to an arbitrary accuracy, right?
So what is this saying?
It says that I have this unknown special functions of Phi and f
that I need to define so that I- so
that I can write my injective function in terms of f and Phi,
but f and Phi are not known ahead of time.
So but- so I'm going to represent f and Phi with neural networks.
And then because multi-layer perceptron is able to
learn any function to arbitrary accuracy,
this basically means I can use data to learn f and Phi that
have the property to- to- to create these types of injective mappings.
So basically this means that, um,
we have arrived to a neural network that can model any injective function, right?
If I take a multi-set with elements x,
then if I apply a multi-layer perceptron to it,
sum it up and apply another multi-layer perceptron,
then multi-layer perceptron can, um,
approximate any function, so it can approximate my function f and Phi as well.
So this means now I have a neural network that can do
this injective multi-set, uh, mapping.
And, you know, in theory,
this embedding dimensionality, the dimensionality of the MLP could be very large,
but in practice, it turns out that, you know,
something between 100 and 500,
um, is good enough and gives you a good performance.
So what magic has just happened is that we
said any injective multi-set function can be written as,
uh, um, as, uh, uh,
uh, as a- with two functions, f and Phi.
F is first applied to every, uh,
element of the multi-set summed up and that
that is passed through the function Phi and this way,
uh, the- the injectivity, uh, is preserved.
And because of the universal approximation theorem,
we can model f and Phi with a multi-layer perceptron,
and now we have an injective, uh,
aggregation function, because MLP can learn any possible function, and, uh,
um, and, uh, the other MLP also can learn any function,
meaning, it can learn the function f as well.
So, uh, what is now the most expressive graph neural network there is?
The most expressive graph neural network there- there is,
is called Graph Isomorphism Neural Network or GIN for short.
And the way its aggregation function looks like it says,
let's take messages from the children,
let's transform them with a multi-layer perceptron,
let's sum them up and apply another,
uh, multi-layer, uh, perceptron.
And, you know, uh,
given everything I explained, this is, uh,
injective multi-set aggregation function,
so it means it has no failure cases.
It doesn't have any collisions,
and this is the most expressive graph neural network
in the class of this message passing,
uh, graph neural networks.
So, uh, it is super cool that we were basically able
to define the most powerful graph neural network,
um, out of, uh,
an entire class, uh,
of graph neural networks.
And we now theoretically understand that it is really all about the aggregation function,
and that the summation aggregation function is better than the average,
is better than the maximum.
So, uh, let me,
uh, summarize a bit, right?
We have described neighborhood aggregation fun- uh, uh, of, uh,
function of a GIN, um, and,
uh, we see that basically the aggregation is a summation.
We take messages, we transform them through an MLP and then sum them up.
And that has the injective property,
which means it will be able to capture the structure of the entire computation graph.
Now, uh, that we have seen what GIN is,
we are going to describe the full,
uh, model of the Graph Isomorphism Network,
and we are actually going to relate it back to the, uh,
Weisfeiler-Lehman graph kernel, the WL graph kernel that we talked about,
I think in lecture number 2.
And what we are going to provide is
this very interesting prospect where we are going to see that,
uh, GIN is a neural network version of the WL ker- kernel.
So, um, let me explain this in more detail.
So what is WL graph kernel?
Right? It is also called a color refinement
algorithm where basically we are given a graph G and a set of nodes V,
we assign initial color,
uh, c to each node v. Uh,
let's say we- the colors are based on degree of the node,
and then we are iteratively aggregating,
hashing colors of neighbors to create a new color for the node, right?
So we take the at- at, uh,
if you want to create the color at level k plus 1 for a given node,
we take the colors of the nodes, uh,
u that are its neighbors from the previous iteration, we take, uh,
color of node v from the previous iteration,
somehow hash these together into a new color.
All right, then hash- hash function.
The idea is that it maps different inputs to different, uh, outputs.
Uh, so hash functions are as injective, uh, as possible.
And the idea is that after k steps of this color refinement, the color, uh,
of every node will summarize the K-hop neighborhood structure,
uh, around a given node.
So let me give you an example.
Imagine I have two different graphs, um.
Here they are, they are different,
they are, um, uh, uh, non-isomorphic.
So the way we do this is, uh,
let's say we first simply initialize
all the colors to value 1 and then we aggregate, right?
So the- for example, this node has color 1 and then has three neighbors,
each one of color 1.
So this will be now one comma 1, 1, 1,
and then, you know,
every node does the same.
Now we are going to hash these descriptions,
these, uh, colors into new colors.
And let's assume that our hash function is injective,
meaning it has no, uh,
collisions, then this would be a new set of,
uh, node colors now.
Um, and for example,
in this case, this node, uh,
and that node have the same color because their descriptions,
uh, are the same, right?
They have color 1 and they have three neighbors, each with color 1,
so this particular input got mapped to a new color, uh, number 4.
And now I can then repeat this process,
uh, uh, one more time and so on.
What you should notice is that at every iteration of this WL kernel,
what is happening is we are taking the colors, uh,
from the neighbors, uh, and putting them together, together with our own color.
So if you go back and look here,
this is very similar to the, uh,
graph neural network, where we take messages from the neighbors,
combine it with the v's own message, somehow transform, uh,
all these into a new- and- and, uh,
transform this and call this- that this is the message for node V at the next level.
So this is essentially like a hard coded graph neural network, right?
We take, uh, colors from neighbors,
uh, aggregate them, take our own color,
aggregate it, and then call this to be the embedding of the node
v at the next layer or at the next, uh, level.
So the point is that as- the more iterations we do this,
the farther out information,
uh, is captured at a given node, right?
The farther out kind of the network neighborhood gets,
more and more hops get added to it.
And the idea of this color refinement is that the- if you are doing, let's say, um,
isomorphism testing, then the process continues al- uh,
until the stable coloring is reached.
And two graphs are considered isomorphic if they have the same set of colors.
In our case, the colors in these two graphs are different.
The distribution of them- the number of them is different,
which means these two graphs are not isomorphic and you- if you look at them,
you really see that,
uh, they are not, uh, isomorphic.
So now, how does this relate to the GIN model?
All right, GIN uses neural network to model this injective hash function, right?
The way we can, uh,
write out GIN is we can say, aha,
it's some aggregation over the- the, uh, embeddings,
messages from the children, uh,
from the neighbors of node v plus the color,
the message of the node V,
uh, from the previous, uh, step.
So the way we write this in terms of,
um, uh, GIN operator is to say, aha,
we are taking the messages from the children,
we aggregate- we transform them using an MLP,
this is our function f,
and we summed them up.
Um, and then we also add 1 plus epsilon,
where epsilon is some small, uh,
learnable scalar, our own message transformed by
f and then add the two together and pass through another function, uh, phi.
And this is exactly now a, um, uh,
an injective operator that basically an in- in
an injective way maps the neighborhood information plus the,
um, plus- plus the node's own information into
a unique embedding into a unique, uh, representation.
So, um, if- assume that in- input, uh,
feature is c is represented,
let's say as one-hot encoding,
then basically just direct summation is injective, right?
So if I say, how do I, uh,
have an injective function over a multiset where
elements of a multiset are encoded with one-hot?
Then basically, all I have to do is sum up these vectors and I'll get
a unique representation because every coordinate will
count how many nodes of a given color, uh, there are.
Now, if these colors are not presented so nicely,
you need to transform them with the function f so that it kind of
approximates this intuitive one-hot encoding, right?
So this means that,
uh, uh, GIN, uh, uh,
aggregation GIN type of convolution is composed of two M- uh, uh,
two MLPs, one operated on the colors of neighbors,
one, um, and then the aggregation is a summation plus some, uh,
final MLP that, again,
kind of provides the next level one-hot encoding so that when we,
again, sum up information from the children at the next level,
no information, uh, gets lost.
So you can think of these f's and, uh,
f's and phi as some kind of
transformation- transform- transformations that kind of softly do
uh, one-hot, uh, encoding.
So let's now summarize and provide the entire, uh, GIN model.
Uh, GIN, uh, node-embedding update goes as follows.
Given a graph with a set of nodes v,
we assign a scalar, uh,
vector to each node v,
and then iteratively, ap- uh, apply this, uh,
GINConv operator that basically takes the information from the neighbors,
takes its own information,
applies these func- uh, functions f and phi that are modeled
by MLPs and produces the next level embedding.
And if you look at this, this is now written exactly the same way as the WL, right?
Rather than hash here,
we write out this GINConv.
So this means that basically after K steps of GIN iteration,
CK summarizes the- the structure of the k-hop neighborhood around a given node, uh,
v. So to bring the two together,
this means that GIN can be viewed- understood as
a differentiable neural network version of WL,
uh, graph kernel, right,
wherein WL views node colors.
Um, and, uh, let's say we can encode them as one-hot and use this, uh,
abstract deterministic hash function,
while in GIN views node embeddings which are low-dimensional vectors.
And we use this GIN convolution with these two MLPs;
the MLP phi and MLP, uh,
f that, uh, aggregate information.
You know, what are the advantages of GIN over
the WL is that node embeddings are low-dimensional.
Hence, they can capture the fine
grained similarity of different nodes and that parameters,
uh, of the update function can be learned from the downstream task.
So we are going to actually be able to learn functions, uh,
f and phi that,
uh, ensure, uh, injectivity.
So, um, you know,
because the relationship between GIN and the WL kernel,
their expressive power is exactly the same.
So this now means that if two graphs can be distinguished by GIN,
they can be also distinguished by WL and vice versa.
So it means that, uh,
graph neural networks are at most as powerful or as
expressive as the WL kernel or the WL graph isomorphism test.
Um, and, uh, this is great because now we have the upper bound.
We know that GIN attains this upper bound and we also know that WL kernel,
both theoretically and empirically,
has shown to distinguish many or most of the real-world graphs, right?
So this means that GIN is the- powerful enough to distinguish most,
uh, real-world graphs, which is,
uh, great- uh, which is great news.
So let me summarize.
Uh, we design a neural network that can model
injective multi-set function by basically saying that
any injective multi-set function can be written as a,
uh, app- application of a function f to the elements of the multiset plus a summation.
Um, in our case,
we use a neural network for neg- neighborhood aggregation function, uh,
and rely on the, um, uh,
Universal Approximation Theorem to basically say that MLP is able to learn any function.
So this means that GIN is able to capture the neighborhoods in an injective way,
which means it is the most powerful or the most expressive graph neural network there is.
Um, and the key is to use element-wise summation pooling instead of mean or max pooling.
So it means that sum pooling is more expressive than mean or max pooling.
We also saw that the GIN is closely related to
the WL kernel and that both GIN and WL kernel can distinguish,
uh, most of the real-world, uh, graph structures.
To summarize, the- the- the important point of
the lecture is that if you say about mean and max pooling,
for example, mean and max pooling are not able to distinguish
these types of neighborhood structures where you have two or three neighbors,
all the same features.
Here is where maximum pooling fails because
the number of distinct- k- kind of the distinct colors are the same.
So whatever is the maximum is the same in both cases and this is, again,
the case where both mean and max pooling fail because we have, uh,
um, green and red and they are in the same proportion.
So if you rank, uh, different pooling operators by- by discriminative power, um,
sum pooling is the best,
is most expressive, is more expressive than mean pooling,
is more expressive than, uh, maximum pooling.
So in general, sum pooling, uh,
is the most expressive,
uh, to use in graph neural networks.
And last thing I want to mention is that you can
further improve the expressive power of graph neural networks.
So the important characteristic of what we talked
today was that node features are indistinguishable,
meaning that all nodes have the same node feature information.
So by adding rich features,
nodes may become, um, distinguishable.
The other important thing that we talked about today is that because
graph neural networks only aggregate features
and they use no reference point in the network.
Nodes that have the same, um,
uh, computation graph structure are indistinguishable.
And what we are going to talk about, uh,
later in the course is actually how do we improve the expressive power of,
uh, graph neural networks, um,
to be more expressive than GIN,
and to be more expressive that- than, uh, WL.
And of course in those cases,
it will actually require more than just the message passing.
It will require more advanced operations, um,
and we are going to talk about those, uh, in the future.

The second topic, uh,
I wanna discuss is,
I wanna talk about robustness of graph neural networks.
So that's, the question is how robust are these models against,
let's say, adversarial or,
uh, any other type of attack?
So, uh, this is motivated by kind
of more broader field of deep learning while in recent years,
we have- we have seen impressive performance of
deep-learning models in a variety of applications,
um, especially, let's say in computer vision,
natural language, and so on.
But what has been, uh, shown is that,
if these models are applied to the- to the real world, um,
and we have different types of actors in the real world,
then the question becomes,
how robust are these models to-, um,
uh, to various kinds of attacks?
And one example is a notion of an adversarial attack, where for example,
you can take the original input image and ask the neural network,
what do you think this is and the neural network is going to say,
"Oh, it's a- it's a- it's a panda."
But what do you can do then is very slightly perturb,
uh, values of the pixels.
Um, you know, just you- you- you- adds a bit of
noise and now the neural network will get totally confused and it will say,
"Oh, it's a- it's- it's a gibbon, right?
It's a monkey. So the point being is that to a human,
when you look at these, they both look like,
uh, images of a panda to us.
But by being very careful, uh, and systematic,
very adversarial, you can change the prediction of the neural network.
Um, and these types of
adversarial examples are also reported for natural language processing model,
audio processing and- and the point is that the adversary can very quickly,
um, change the prediction of our model by only slightly manipulating the input.
So, um, the existence of the- of
adversarial examples prevents reliable deployment
of deep-learning models in the real world.
Uh, because adversaries may try to actively
hack the deep-learning model and kind of, uh, sidestep it.
Um, and the model performance this way may- may be much,
much worse than what we'd expect, right?
Um, and like has been found is the deep-learning models are often not robust.
Um, and this is a very active area of research to mas- to make
these models robust against adversarial attacks or adversarial examples.
So, uh, let's look in this part of the lecture about how would you formalize
the notion of adversarial example to
graph neural networks and are graph neural networks
robust to adversarial examples?
Um, and the premise is that, um,
if common applications of graph neural networks,
uh, you know, uh, involve, let's say,
platforms that are accessible to the public or that there
is some financial monetary interest behind them,
then these types of attacks, uh, may happen.
So for example, in recommender systems,
social networks and social-, uh,
and search engines, there's always some types of actors, attackers,
who may wanna manipulate the system.
So the adversaries have incentive to manipulate
the input graphs and basically try to change or flip or hack,
the predictions of the graph neural network.
So how would we going to start- to set up,
um, the framework to study robustness of graph neural networks?
Um, to study robustness of graph neural networks,
we are going to, uh,
specifically consider the following setting,
we are going to look at semi-supervised node classification and we are going to
use the graph convolutional neural network model just as an example.
And the idea is, right,
we have a partially labeled graph.
So here's the example.
We have a graph, uh,
if you are thinking about node classification,
nodes have features, some nodes already have,
uh, labels, and we would like to predict, uh, labels, uh,
for the other nodes as I-,
uh, as I show, uh, here.
So, uh, then, um,
the question is the following, right?
Um, what we are going to do is,
we are going to first describe several,
um, types of attacks that the adversary may do.
Then we are going to talk about,
uh, you know, how a graph neural network, uh,
or a GCN, uh, how- how would- how would that-,
uh, how we could try to hack it?
Um, and this means that, um,
a- adversary- adversary needs to know what underlying model we are using.
And then we're going to mathematically formalize
the attack problem as an optimization problem.
And then I'll show some empirical results to show how
vulnerable GCNs are to adversarial attacks.
So let's first talk about what would,
uh, an attack to a graph, uh, look like?
Uh, and we are going to talk about direct in- and indirect attacks.
Uh, and in particular,
the way we are going to think about this is the following.
Let's say there is a target node t,
whose label pre- prediction we wanna change.
And let's say that there are some other nodes in the network,
um, uh, let's call them S that the attacker can modify.
So these are the nodes that the attacker controls.
And now, what can the-, you know,
what can the attacker-,
uh, what can the attacker do?
So let's say that there is a direct attack,
and a direct attack means that the attacker is the target node, right?
So the attacker owns the target node.
So what the attacker can then do,
the attacker can modify the target node features.
For example, they can change- change their social network profile.
They can change the website content.
They can also add connections,
um, from the target to other nodes in the network.
Like, you know, they can buy likes,
they can buy followers,
they can make frie-, uh,
friendships or things like that.
Um, and they can also of course decide to remove some connections.
You know, they can unfollow users.
They can drop friendships if that would allow them to flip
the classification label for that target node that they, uh, own.
So this is what we call a direct attack because the attacker owns the node,
the- the node that the attacker wants to flip is- is- is the attacker.
So for example, if you think about some kind of spam detection,
the attacker may want to say "Aha, you know,
the social network is classifying me as a spammer.
What do I need to change so that
the social network won't classify me as a spammer anymore?"
Um, and they may change their attributes,
they may change who they follow or they may- they may drop,
uh, some of the links in the network.
Um, there is also a- a second type of attack which we call an indirect attack.
And in an indirect attack,
a target node is not the attacker node.
So it means that, the attacker does not,
uh, account- does not control the target,
but the attacker can, uh,
control some other nodes in the network and the attacker can change,
let's say, the features of those nodes.
So, uh, the attacker can change the connections of those nodes and
attacker can also basically can add them or they can remove those connections.
And you know, what is a realistic example for this case, it is that,
for example, the attacker wants to, uh,
harm some other, let's say,
innocent node in the network.
So the attacker is going to- to change, um, uh,
the- the properties of the nodes it controls to
perhaps change the label of the target node they don't control.
So perhaps the target node is not a spammer,
but the attacker wants somebody to be excluded from the network.
So they want them to be labeled as a spammer.
So they are going to now change,
uh, the network structure,
the connections to the target node,
so that the neural network is going to,
uh, classify the target node differently even though the target node has no clue that,
uh, they are being, uh, under attack.
So how are we formalizing the adversarial attack?
The goal is that we wanna maximize something subject to some constraints,
and the idea will be that,
we wanna maximize the change of the target node, uh, label.
So we wanna change the target node label subject to very small,
uh, graph manipulation, right?
We wanna manipulate the underlying graph as little as possible such
that the class probabilities of the target node are going to change, right?
Basically, the idea is that if the graph manipulation is too large,
it will be easily detected,
um, by the-, uh,
by the- by the- by the agent,
by the social network and it will say,
"Hey, there is an attack going on."
But if a successful at- if an attack is successful,
then we should be able to change just a small number of edges,
maybe a few features here and there, uh,
and this will be kind of unnoticeably-small manipulation that is going to flip,
uh, the prediction, the- let's say,
the class label for the target node, uh, significantly.
So the way- the way we wanna think of this is we'll say,
perhaps there is a class or a label to which the target node gets, uh, classified
right now, so what we wanna do is we want to decrease the probability of
the correct class and we wanna- wanna increase
the probability of some other chosen class,
let's say class three,
uh, in our case.
So how are we going to formalize this mathematically?
We are going to say that we have the adjacency matrix of
the original graph and we have the feature information of nodes of the original graph.
And then we'll also have, uh,
A prime and X prime to be manipulated graphs.
So these are the graphs after the attack,
after we have manipulated them.
And we'll have the adjacency matrix and we'll have the feature matrix.
And our goal or assumption is that the two- that the manipulation is small, right?
Our goal is- that this manipulation, the changing
the adjacency matrix or a change in the node features is,
um, uh, unnoticeably small.
So basically we wanna preserve basic graph statistics like degree distribution,
uh, node feature statistics and so on.
And as I mentioned that the graph manipulation can either be direct,
changing the node and feat- and, uh,
features and connections of the target node or it can be indirect,
which means you're- we may want to change, um,
the connections and features of some other nodes in
the network with the hope to change the,
uh, label of the target node.
So, um, now that we said what- what we wanna do, let's formalize it.
So we're- we'll be given a target node v, um,
and let's us- what did the- what, uh,
the- the system is going to do, uh, you know,
let's say the social network is going to learn the original,
um, the classifier over the original graph, right?
So it's going to say, "Oh, let's, you know,
define this likelihood over the training data and let's optimize
our neural network model parameters over the training data."
And then, um, you know,
the- now that the model has been trained,
how is the prediction going to look like?
We'll say, a-ha, for this target node of interest v. Uh,
let's classify in- into the class that our predictor f,
our neural network parameters,
theta star that we have trained applied to this graph,
um, this is our prediction, right?
We are going to predict the class of- for this node v,
that has the highest predicted,
uh, probability, just kind of standard stuff.
Now, uh, the attacker comes into play.
So what the attacker is going to do, the attacker, um,
can-, um, what they can do is,
they cannot change the parameters of the model, right?
The- the model parameters,
uh, cannot be changed because, you know,
the social network, whoever,
the operator has already trained the model and has deployed it.
So the model parameters, um,
won't necessarily, uh, be able to be-,
uh, to be changed.
So, um, what we can- what we can do in this case is that the GCN, um,
we can- like the way we will think about it,
is that we are going to think about it, that the GCN will now be
learned over the manipulated graph, right?
So basically, we are going to change A to
be A prime and we are going to change X to be X prime.
Um, and now, the- the- we are going to train over
these manipulated graph and we are going to get a different set of parameters.
And then the GCN's prediction of the target grow-,
uh, node may change,
both because the input is different as well as because
the parameters we learned will be different because
we learned them on a different graph and with different node features.
And we want the prediction to change,
uh, after the graph is manipulated, right?
So we want the class for node v, uh,
after manipulation to be different than what did the class was,
uh, before the manipulation?
So we want to flip the,
um, uh, the- the class.
So the way we are going, uh,
to do this is the following, right?
The way we are going to do this is to say, uh,
we wanna change the predicted probability, uh,
for a- of a- of a class of a given node,
um, between the true class and the manipulated class, right?
So here I'm saying f is outputting the probability,
um, for a given node to be of a given class.
So here I want the predicted log probability of a newly predicted class,
uh, c star prime to be- to increase.
While, um, I want to, um,
decrease the predicted log probability of the originally, uh, predicted class.
We want this term to decrease and we want that term to dec- to increase.
So we are going to formulate this into this delta- function delta,
that is basically cha- changing the prediction on the target node, uh,
v. It is a change, uh,
in like- in- in likelihood of the,
uh, correct class versus likelihood of the manipulated class.
And the way we make this term, this delta high,
is to make this probability here as small as possible and we make this probability here,
um, as large, uh, as possible, right?
We want these to be close to 0 and we want this to be, uh, close to 1.
So we wanna increase,
uh, the first term,
the left term, and we wanna decrease, uh, the second term.
So the way we can write this out as an optimization problem,
we wanna say we wanna do the argmax
over the underlying graph structure and the feature vector,
where we wanna maximize this delta, right?
This delta function of the change in the prediction, uh, on the target,
subject to the constraint that the manipulation, um,
of the graph structure and the feature vector,
uh, is as small as possible.
And ideally, what we'd like to do is,
solve this optimization problem.
Uh, the challenge in optimization- in optimizing this objective if we are an,
uh, adversary, is that the,
um adjacency matrix A is a discrete object.
You cannot take gradients, um,
uh, against A to decide what edges to manipulate.
Um, and that's the first challenge,
and then the second challenge is that for every modification to,
um, to the adjacency matrix and the future vector,
we need to retrain the underlying graph neural network,
to see whether the,
prediction, uh, is- will flip, uh, or not.
And I won't go into more details here and, you know,
here's a paper, uh,
you can read, uh,
to learn, uh, more about this.
But essentially, the idea,
the idea here is,
that we wanna manipulate,
the graph structure and the future vector as little as possible,
so that the- so that the,
predicted label of target node v,
is going to change,
um, as much as possible.
So imagine we are now able to solve this, uh, optimization problem.
And we can ask ourselves,
how easy it is to find
these strategically placed edges and the changes in the future vector,
that lead- that would, uh,
let the model to completely wrongly,
make predictions on a given node.
So here we are working on a,
semi-supervised node classification task with graph convolutional neural network, uh,
model, on a paper citation network with 2,800 nodes,
and about 8,000 edges.
Um, and you know, this is a six way classification task.
In what we are doing here is we are looking at a set of nodes,
that belongs to the class, uh, six.
And we ran the graph neural network,
we trained it five times,
uh, from random starting points.
And here we are saying, you know,
what is the pre- pre- predicted probability of these nodes,
that belong to Class 6?
And you can see that, they have probability.
So- so our graph neural network is learning well.
Now, imagine, we are an attacker and we say we want this, um, Class 6.
So basically, uh, research papers that belong to Topic 6.
We wanna, manipulate the underlying graph structure,
to make them belong to Class, uh, 7, right?
So this means, we are eh,
going to change the adjacency matrix,
um, and the, um,
uh, see if we can flip, uh, their labels.
Um, and what is interesting is if you use this framework and ask,
how many edges do we need to manipulate,
in a direct adversarial attack,
it turns out that GCN's prediction will flip, um,
aft- only, after modifying just five edges,
atta- attached, to the target node.
So here I show you for a given target node, in, you know,
five different, uh, retrainings,
just kind of five different instances.
We see that every single time,
we were able to flip the label of this target node,
who was of, uh,
Class 6, to be now of Class 7.
Um, and the model is super confident,
that this node belongs to Class 7,
even in reality it is,
uh, in Class, uh, 6.
And to summarize, uh, this, uh,
robustness of graph neural networks against attacks,
uh, this is summarized here.
Where, um, 0 would mean- in classification margin intuitively,
you can, you can think a bit about,
how confident, um, is the,
is the- or how correct,
is- is the, um, is the classification.
So, if it's positive it means, the,
the classification is correct,
with a bid gap,
so we are very confident in correct classifications.
And if the, uh, classification margin is negative, it would mean,
we are wrong, and we are,
and we are wrong by a lot, right?
We assigned a high confidence to the wrong prediction.
And we see that, you know, um,
with no attacks, the graph neural net- this is individual nodes here.
We see that, for the majority of the nodes,
the graph neural network is able to make, uh,
correct prediction and it is very certain about its prediction.
And you know, the class- the prediction,
the prediction performance is not perfect.
So there are a few mistakes,
but those mistakes are also kind of borderline.
The, the network is,
unsure about the classes of this,
uh, of these nodes.
Now, if we apply, uh, a,
a direct adversarial attack,
um, then you can see how we can very easily change, um, the,
uh, the- the, the neural network to basically
always predict- to always make a mistake and be super confident uh,
that it- in the, in the wrong, uh, class,
so assign, high confidence in the,
in the wrong class.
So this is a direct attack- direct adversarial attack.
Uh, here is what would happen,
if the- if it's still a direct attack,
but it's a random attack.
Meaning, we don't adversarially strategically, change the,
the edges, but we add edges,
uh, at uh, random.
Um, and you see that basically the neural network will get confused,
but not so badly, right?
It, uh, still majority,
it is making correct predictions.
So definitely decreases performance,
but not so drastically.
If you look at uh, indirect attacks,
were we basically now manipulate uh,
other nodes in the network and see if we can change the label of the,
of the, uh, target node.
Here you see again,
we are able to do this, uh, quite well.
Meaning for quite a few of the nodes,
we are able to flip,
uh, the, uh, uh, label.
So what this means is,
adversarial direct attacks are very strong and they
can- they significantly worsen GNN performance.
Uh, random attack is much, much weaker,
than adversarial attack and somewhat decreases, the performance.
While indirect attack is,
uh, less effective than direct,
but more effective, uh, than, uh,
than direct, uh, random attack.
So, um, what do we conclude from this?
Is that- that if the, uh, um,
uh, adversary has access to the full network,
has access to the- to the data and can, manipulate it,
then it is very easy to change the predictions of this deep-learning,
uh, graph neural network, uh, models.
So to summarize, we studied the adversarial robustness of
graph convolutional neural networks applied to semi-supervised node classification.
We considered different attack strategies,
different attack possibilities, uh, in, in graphs.
We talked about direct attacks as well as indirect attacks.
Uh, we talked, uh, we then mathematically
formulated the adversarial attack as an optimization problem,
so this is the optimization problem,
that the adversary has to solve.
And we, looked at some examples to empirically demonstrate that, um,
er, GCN's prediction performance can be significantly harmed, by adversarial attacks.
So what we conclude is that, uh,
GCN is not robust against adversarial attack,
but it is somewhat robust to indirect attacks,
as well as to random, uh, noise.
And this topic of robustness of these types of, uh, methods,
is a topic of, uh,
active research, and a very fruitful, uh, research area.

So first, let's discuss about how do we define a single layer of a graph neural network, right?
So what- what goes into a single layer?
A single layer has two components.
It has a component of a message transformation.
And it has a component of message aggregation,
and as I mentioned,
different graph neural network architectures basically
differ in how these operations are being done,
among other kinds of things that differ between them.
So what is the idea of a single GNN layer?
The idea is that we want to compress a set of messages,
a set of vectors coming from the children,
from the- from the bottom layer of the neural network.
In some sense compress them by aggregating them.
And we are going to- to do this as a two-step process,
as a message transformation and message aggregation, right?
So if we think about this,
we are getting a set of children at the- at the bottom, a set of inputs.
We have- we have an output.
What do we have to do is take the message from each of the child and transform it.
Then we have to aggregate these messages into a single message and pass it on.
So the way you can think of this is that we get
messages, denoted as circles here, from the three neighbors,
from the previous layer.
We also have our own message, right?
Message of the node v from the previous layer.
Somehow we want to combine this information to create
the next level embedding or to the next level message for this node of interest.
What is important here to note is that this is a set.
So the ordering in which we are aggregating
these messages from the children is not important.
What is arbitrary?
And for this reason,
these aggregation functions that aggregate, that summarize,
that compress in some sense,
the messages coming from the children have to
be order invariant because they shouldn't depend,
in which ordering, am I considering the neighbors?
Because there is no special ordering to the neighbors,
to the lower level,
to the children in the network.
That's an important detail. Of course,
another important detail is that we want to combine
information coming from the neighbor- from the neighbors
together with a node's own information from the previous layer as denoted here.
So I'm connecting information from level l minus 1 to create
a message at level l. And I'm collecting information from the neighbors,
from the previous layer,
as well as from the representation of that node itself at  the previous layer.
So let me now make things a bit more precise.
So we will talk about message computation as the first operation that we need to decide.
And basically, the message computation takes
the representation of the node at the previous layer and
somehow transforms it into this message information.
So each node creates a message which will be sent to other nodes in the next layer.
An example of a simple message transformation is that you take
the previous layer embedding of a node and multiply
it with the matrix W. So this is a simple linear layer,
linear transformation, and this is what we talked about in the previous lecture.
All right? So that's the first part.
Then we need to decide this message function.
In this case, it's simply this matrix multiply.
The second question is about aggregation.
The intuition here is that each node will aggregate the messages from its neighbors.
So the idea is that I take now
these transformed messages m that we just defined on the previous slide, right?
So I take these transformed messages coming from nodes u,
from the previous level that I transformed,
let's say in this case with W, and I want to aggregate them into a single message.
All right, I want to take this thing and kind of compress it, aggregate it.
What are some examples of aggregation functions?
A summation is a simple aggregation function,
an average is an order invariant aggregation function as well as for example, Max,
we take the maximum message or to the maximum coordinate-wise value,
and that's how we aggregate.
And again, all these- all these aggregation functions are order invariant.
So for example, one concrete way how you could do this is to say, uh-huh
the level l embedding for node v is simply a summation of
the transformed messages coming from the neighbors u of that node of interest,
v. And this is where the messages from previous layer got
transformed and now we simply sum them up to have
the embedding for the node at level l. And of course,
now this node at level l,
to send to- to create a message for level l plus 1.
It will take now W l plus 1,
multiply it with h,
and send it to whoever is above them in the graph neural network structure.
So this was now a message operation,
message transformation, and message aggregation.
One important issue is,
if you do it the way I defined it so far is that information
from the node itself could get lost, right?
Basically, that computation of message for node v for level l does not directly
depend on what we have already computed
for that node- same node v from the previous level, right?
So for example, if I do it simply as I show here,
we are simply aggregating information about neighbors,
but we don't really say,
okay, but who is this node v?
What do we know about node v before?
So an opportunity here is to actually,
to include the previous level embedding of node v.
Then we are computing the embedding of v for the next level.
So usually basically a different message computation will be performed, right?
What do I mean by this is, for example,
the message transformation matrix W will be applied to the neighbors u.
While there will be a different message aggregation function B that
will be applied to the embedding of node v itself.
So that's the first difference, right?
So that the message from the node itself from previous layer will be multiplied by B,
while messages from neighbors from previous layer are going to be multiplied by
W. And then the second difference is that after aggregating from neighbors,
we can aggregate messages from v itself as well.
And usually, this is done via a concatenation or a summation.
So to show you an example,
the way we can, we can do this is to say,
ah-ha, I'm taking my messages from neighbors and I'm aggregating them.
Let's say with a- with a summation operator.
I'm taking the message from v itself, right,
like I defined it up here.
And then I'm going to concatenate these two messages
simply like concatenate them one next to each other.
And that's my next layer embedding for node v. So simply I'm saying,
I'm aggregating information from neighbors,
plus retaining the information about the node that I already had.
And this is now a way how to keep track of the information that the node has
already computed about itself so that it doesn't
get lost through the layers of propagation,
let's say by concatenation here, or by summation.
That's another popular choice.
So putting all this together, what did we learn?
We learned that we have this message where
each node from the previous layer takes its own embedding,
its own information, transforms it,
and sends it up to the parent.
This is denoted here through this message transformation function.
Usually, this is simply a linear function like a matrix multiply.
And then we have this message aggregation step
where we aggregate transformed messages from the neighbors, right?
So we take these messages m that we have computed here and we aggregated them.
We aggregate them with an average,
with a summation, or with a maximum pooling type approach.
And then what we can also do is,
another extension here is to also add a self message and concatenate it.
And then after we have done all this,
we pass this through a non-linearity,
through a non-linear activation function.
And this last step is important because it adds expressiveness.
Often, you know, this non-linearity is written as sigma.
In reality, this can be a rectified linear unit or a sigmoid,
or any other specific type of
non-linear activation function, popularly other types of neural networks as well.
But that's essentially how a single layer of graph neural network looks like.
So now that we have seen this in abstract,
I want to mention some of
the seminal graph neural network architectures that have been developed
and kind of interpret them in
this unified message transformation, message aggregation framework.
So, last lecture, we talked about graph convolutional neural network or a GCN.
And I've wrote this equation,
I said, ah-ha, the embedding of node v at layer l is simply an average of the embeddings of
nodes u that are neighbors of we normalized the by the- by the N-degree of
node v and transformed with matrix W and sent through a non-linearity.
So now the question is,
how can I take this equation that I've written here and write it in
this message transformation plus aggregation function.
And the way you can- you can do this is simply take this W and distribute it inside.
So basically now W times
h divided by number of neighbors is the message transformation function.
And then the message aggregation function is simply a summation.
And then we have a non-linearity here.
So this is what a graph convolutional neural network is,
in terms of message aggregation and message transformation.
Um, so to write it even more explicitly,
each neighbor transforms the message by saying,
I take my, uh,
previous layer embedding, multiply it with w,
and divide it by the,
uh node degree of v, so, uh,
this is normal- normalization by node degree and then the aggregation is a summation
over the neighbors, uh,
of node v and then applying, uh,
a nonlinearity activation function here,
uh, denoted as sigma.
So this is now a G- GCN written as
a message transformation and a message aggregation, uh, type operation.
So that's, um, number,
uh, the first classical architecture.
Uh, the second architecture I want to mention is called GraphSAGE,
and GraphSAGE builds- builds upon the GCN,
but extends it in, uh,
several important, uh, aspects.
Uh, the first aspect is that it realizes that
this aggregation function is an arbitrary, uh,
aggregation function, so it allows for multiple different choices
of aggregation functions, not only averaging.
And the second thing is,
it- it talks about, uh,
taking the message from the node itself, uh,
transforming it, and then concatenating it with the aggregating- aggregated messages,
which adds, uh, a lot of expressive, uh, power.
So now let's write the GraphSAGE equation.
In this message plus, uh,
aggregation type operations, right?
Messages computed through the,
uh, uh, aggregation operator AGG here.
Um, and the way we can think of this is that this is kind of a two-stage approach.
First is that, um, uh, we, um,
we take the individual messages,
um, and transform them,
let's say through, uh, linear operations,
then we apply the aggregation operator that basically
gives me now a summary of the messages coming from the neighbors.
And then, uh, the second important step now is that I take the messages coming
from the neighbors already aggregated, I concatenate it with v's own,
um, um, message or embedding, uh,
from the previous layer,
concatenate these two together,
multiply them with a- with
a transformation matrix and pass through a non-linearity, right?
So the differences between GCN is here and another more important difference
is here that we are concatenating and taking our own,
uh, embedding, uh, as well.
So, um, to, now to say what kind of aggregation functions can be used?
We can simply take, for example,
weighted average of neighbors,
which is what our GCN is doing.
We can, for example, take any kind of pooling,
which is, uh, you know, take the, uh, uh,
take the- transform the neighbor vectors and apply
a symmetric vector function like a min or a max.
So you could even have, for example,
here as a transformation,
you don't have to have a linear transformation.
You could have a multilayer perceptron as
a message transformation function and then an aggregation.
Um, you can also not take the average of the messages,
but your sum up the messages.
And, uh, these different, um, uh,
aggregation functions have different theoretical properties.
And we are going to actually talk about the theoretical properties and consequences, uh,
of the choice of the aggregation function on the expressive power,
uh, of the model,
um, in one of the future lectures.
And what you could even do, um,
if you like, is you could apply an LSTM.
So basically you could apply a sequence model, uh,
to the- to the messages coming from the neighbors.
Um, and here the important thing is that a sequence model is not order invariant.
So when you train it,
you wanna permute the orderings so that the- you teach the- the sequence model,
not to keep, uh,
kind of to ignore, uh,
the ordering of the messages that it receives.
But you could use something like this, um, as a,
uh, as an aggregation, uh, uh, operator.
So a lot of freedom, uh, to choose here.
And then the last thing to mention about
GraphSAGE is that adds this notion of l2 normalization.
So the idea is that we want to apply l2 normalization to the embeddings at every layer.
And when I say l2 normalization,
all I mean by that is we wanna measure the distance,
some of the squared values, uh,
of the entries of the embedding of a given- of a given node,
take the square root of that,
and then divide by the distance.
So basically this means that the Euclidean length of
this embedding vector will always be equal to 1.
And sometimes this is quite important and,
uh, leads to big, uh,
performance improvement because without l2 normalization, the embedding, uh,
vectors of nodes can have different scales, different lengths, um,
and in some cases,
normalization of the embedding results in performance improvement.
So after l2 normalization step,
as I introduced it here,
all vectors will have the same, uh, l2 norm.
They'll have the same length,
which is the length of, uh, 1.
Uh, this is how, uh,
this is defined, um,
if- if you wanna, uh, really see it.
So l2 normalization is also an important component, um,
when deciding on the design decisions on the specific architecture of the,
uh, graph, uh, neural network.
And then the last, uh,
classical architecture that I wanna talk about is called graph attention network.
And here we are going to learn,
uh, this concept of an attention.
So let me first tell you what a graph attention network is,
and then I will define the notion of attention and,
uh, how do we learn it and what- what it intuitively means.
So, uh, the motivation is the following.
Writing graph attention network when we are aggregating messages from the neighbors,
we have a weight, um,
associated with every neighbor, right?
So for every neighbor u,
of node v, we have a weight, alpha.
And this weight we call attention weight.
[NOISE] And the idea is that this weight can now tell me how important of a neighbor, um,
is a given- is a given node,
or in some sense,
how much attention to pay to a given, to a message from a given node u?
Because if these weights are different,
then messages from different nodes will have different weight,
uh, in this summation. That's the idea.
So now let's make a step back.
Explain why- why- how- how this is motivated,
why it's a good idea,
and how to, uh, learn these weights.
So if you think about the two architectures we talked about so far,
so the GCN and GraphSAGE.
There is already an implicit notion of this alpha.
So alpha_uv is simply 1 over the degree of node v. So basically this is
a weight factor or an importance of a message coming from u for the node v. And,
uh, so far, you know,
this alpha was defined implicitly.
Um, and we can actually define it,
um, more explicitly or we can actually learn it.
Uh, in our case,
alpha was actually for all the,
uh, incoming no- uh,
nodes u, uh, alpha was the same.
It only depended on the degree of node v but didn't really depend on the u itself.
So it does a very limiting kind of notion of attention, right?
So in- in GraphSAGE or GCN,
all neighbors are equally important to node v,
when aggregating messages and the question is,
can we kind of, er, uh, generalize this?
Can we generalize this so that we learn how important is a message from a given node,
uh, to the node that is aggregating messages, right?
So we wanna learn message importances.
And this notion of an importance is called attention,
and attention- the word attention is kind of inspired
by the- by- in a sense with cognitive attention, right?
So attention alpha focuses on the important parts of
the input data and kind of fades out or ignores, uh, the rest.
So the idea is that the neural network should devote more computing power,
more attention to that small important part of the input,
um, and perhaps, you know,
ignore- choose to ignore the rest.
Um, and which part of the data is more important depends on the context.
And the idea is that we are going to learn, uh,
what part of the data is important through the model training process.
So we allow the model to learn importance of different,
uh, of different pieces of input that it is, uh, receiving.
So in our case, we would want to learn this attention weight that
will tell us how important is a message coming from node u, uh,
to this, uh, node, uh,
v. And we want this attention,
of course, depend on the u as well as on,
uh, v as well.
So the question is, right, how do we learn, uh,
these, uh, attention weights, these weighting factors, uh, alpha.
So the goal is to specify an arbitrary importance, uh,
between, um, between neighbors,
um, when we're doing message aggregation.
And the idea is that we can compute the embedding, uh,
of each node in the graph following these attention strategies
where nodes attend over the messages coming from the neighborhood,
by attend, I mean give different importances to them.
And then, uh, we are going to, um,
implicitly specify different weights to different nodes, uh, in the neighborhood.
And we are going to learn these weights, these importances.
The way we are going to do this is,
uh, to compute this as a, uh,
byproduct of the attention mechanism,
where we are going to define this notion of attention mechanism
that is going to give us these attention scores or attention weights.
So let- let us think about this attention, uh, mechanism, a,
by first computing attention coefficients,
e_vu across pairs of nodes,
uh, u and v based on their messages.
So the idea would be that I wanna define some function a,
that will take the embedding of node u at previous layer,
embedding of node v at previous layer,
perhaps transform these two embeddings,
and then take these as input and prod- give me a weight, right?
And this weight will tell me the importance of, uh,
u's message on the, uh,
on the node, uh, v. So for example,
just to be concrete, right?
If I wanna say, what is the attention coefficients e_AB?
It's simply some function a,
of the embedding of node A at the previous step, uh,
and the embedding of node B at the previous step,
at the previous layer of the graph neural network,
and this will give me now the weight, uh, of this,
uh, or importance of this particular, uh, edge.
So now that I have these, uh, coefficients,
I wanna normalize them to- to get the final attention weight.
So what do I mean by, for example, uh,
normalize is that we can apply a softmax function,
uh, to them so that these attention weights are going to sum to 1.
So I take the coefficients e that we have just defined,
I, uh, exponentiate them, and then, you know,
divide by the s- exponentiated sum of them so that, uh,
these, uh, attention weights, uh,
alpha now are going to sum to 1.
And then, right when I'm doing message aggregation,
I can now do a weighted sum based on the attention weights, uh, alpha.
So here are the alphas.
These are these alphas that depend on e,
and e is the,
uh, is the, um,
is- depends on the previous layer embeddings of nodes, uh,
u and v. So, for example,
if I now say, how would aggregation for node A look like?
The way I would do this is I would compute these attention weights, uh- uh,
Alpha_AB, Alpha_AC, and Alpha_AD because B,
C, and D are its neighbors.
Uh, these alphas will be computed as I- as I show up here,
and they will be computed by previous layer embeddings,
uh, of these, uh,
nodes on the endpoints of the edge.
And then my aggregation function is simply
a weighted average of the messages coming from the neighbors,
where message is, uh- uh- uh,
multiplied by the weight Alpha that we have,
uh, computed and defined up here.
So that's, um, [NOISE] basically the idea of the attention mechanism.
Um, now, what is the form of this attention mechanism a?
We still haven't decided how embedding of one node
and embedding of the other node get- get combined,
computed into this, uh- uh,
weight, uh, e. Uh,
the way it is usually done is,
uh- um, you- you have many different choices.
Like you could use a simple, uh, linear layer, uh,
one layer neural network to do this, um, or, uh,
have alpha, uh, this, um,
function a have trainable parameters.
Uh, so for example a p- uh,
a popular choice is to simply to say: let me
take the embeddings of nodes A and B at the previous layer,
perhaps let me transform them,
let me concatenate them,
and then apply a linear layer to them,
and that will give me this weight, uh, e_AB,
to which then I can apply softmax,
um, and then based on that, ah,
softmax transformed weight, I use that weight as,
uh- uh, in the aggregation function.
And the important point is that these parameters of fu- of, uh, function a,
this attention mechanism a, uh,
the- basically parameters of these functions are trained jointly.
So we learn the parameters of
the attention mechanism together with the weight matrices,
so message transformation matrices,
um, in the message aggregation step.
So we do all this training in an end-to-end, uh, fashion.
What this means in
practice is that working with this type of attention mechanisms,
uh, can be tricky because,
uh, this can be quite finicky.
Uh, in a sense,
may- it's- sometimes it's hard to learn,
hard to make it converge,
so what we can also do is, uh, to, uh,
expand this notion of attention to what is called a multi-head attention.
And multi-head attention is a way to stabilize
learning process of the attention mechanism,
and the idea is quite simple.
The idea is that we'll have multiple attention scores.
So we are going to have multiple attention mechanisms a, um,
and each one- and we are going to train- learn all of them, uh, simultaneously.
So the idea is that we would have different functions a,
for example, in this case, we would have three different functions a,
which means I would- we would get three different, uh,
attention coefficients, attention weights for a given edge vu.
And then we will do the aggregation,
uh, three times, uh,
get the aggregated messages from the neighbors,
and now we can further aggregate, uh,
these messages into a single, uh, aggregated message.
And the point here is now that we- when we learn these functions a^1, a^2, a^3,
we are going to randomly initialize parameters of each one of them,
and through the learning process,
each one of them is kind of going to converge to some local minima.
But because we are using multiple of them,
and we are averaging their transformations together,
this will basically allow our model to- to be- to be more robust,
it will allow our learning process not to get
stuck in some weird part of the optimization space,
um, and, kind of,
it will work, uh, better,
uh, on the average.
So the idea of this multi-head attention is- is simple.
To summarize, is that we are going to have
multiple attention weights on the- on the same edge,
and we are going to use them, uh,
separately in message aggregation,
and then the final message that we get
for a- for a node will be simply the aggregation,
like the average, of these individual attention-based, uh, aggregations.
One important detail here is that each of these different, uh,
Alphas has to be predicted with a different function a,
and each of these functions a,
has to be initialized with a random, uh,
different set of starting parameters so that each one gets a chance to,
kind of, converge to some,
uh, local, uh, minima.
Uh, that's the idea and that adds to the robustness and stabilizes,
uh, the learning process.
So this is what I wanted to say about the attention mechanism,
and how do we define it?
So next, let me defi- let me discuss a bit the benefits of the attention mechanism.
And the key benefit is that this allows implicitly for
specifying different importance values to different neighbors.
Um, it is computationally efficient in a sense that computation of attention,
uh, coefficient can be parallelized across all the incoming messages, right?
For every incoming message,
I compute the attention weight, uh,
by applying function a that only depends on
the embedding of one node and the embedding of the other node,
uh, in the previous layer,
um, so this is good.
It is, uh, in some sense storage-efficient because, um,
sparse matrix operators do not require,
um- um, too many non-zero elements.
Basically, I need one entry per node and one entry per edge,
so, uh, you know,
that's cheap, that's linear in the amount of data we have, um,
and it has a fixed number of parameters,
meaning the, uh, mesh,
the- the attention mechanism function a has a fixed number of parameters that is,
uh, independent of the graph size.
Another important aspect is that,
uh, attention weights are localized.
They attend to local network neighborhoods,
so basically tell you what part of the neighborhood to focus on,
um, and they generalize,
meaning that they- they give me this, uh,
inductive capability, which means that, um,
this is a shared edge- edgewise mechanism and
does not depend on the graph structure- so- on the global graph structure.
So it means can I can transfer it across the graphs,
so this function A is transferable between graphs or from one part of the graph,
uh, to the next part of the graph.
So, um, these are the benefits and kind of the discussion,
uh, of the attention mechanism.
To give you an example,
um, here is, um, uh,
an example of a, um,
uh, of a network, uh, called Quora.
This is a citation network of different papers coming from different disciplines.
And different disciplines,
different publication classes here are colored in different, uh, colors.
And, uh, what we tried to show here is with
different edge thickness is the attention score um,
between a pair of nodes uh, i and j.
So it's simply a normalized attention score by basically saying,
what is the attention of i to j,
and what's attention of uh,
j to i across different uh, layers uh,
k. Notice that attentions,
um, can be asymmetric, right?
One mes- the, uh,
message from you to me might be very important,
while message from me to you [LAUGHTER], for example,
might be less important,
so do- it doesn't have to be symmetric.
And, um, if you look at,
you know, in terms of improvements,
for example, the graph attention networks can
give you quite a bit of improvement over, let's say,
the graph convolutional neural network, uh, because, uh,
because of this attention mechanism and allowing you to learn what to
focus- what to focus on and which sub-parts of the network, uh, to learn from.
So this is an example of graph attention network,
how it learns attentions,
and of course there has been many, uh, upgrades, iterations,
of this idea of attention on graph neural networks,
but the graph attention network,
you know, two years ago was, uh,
was the one that first developed and proposed depth.
So, um, this is, uh, quite exciting.
So, uh, to summarize what have we learned so far.
We have learned that, uh,
about classical graph neural network layers and how
they are defined and what kind of components do they include.
Um, and they can often, you know,
we can often get better performance by- by,
uh, combining different aspects, uh,
in terms of design,
um, and we can also include,
as I will talk about later,
other modern deep learning modules into graph neural network design, right?
Like for example, we'll- I'm going to talk more about batch normalization, dropout,
you can choose different activation function,
attention, as well as aggregation.
So these are kind of transformations and components you can choose to pick,
um, in order to design an effective architecture for your- for your problem.
So as I mentioned,
many kind of modern deep learning modules or techniques can
be incorporated or generalized to graph neural networks.
Um, there is- like for example,
I'm going to talk about- next about batch normalization,
which stabilizes neural network training.
I'm going to talk about dropout that allows us to prevent over-fitting.
Um, we talked about attention to- attention mechanism that
controls the importance of messages coming from different parts of the neighborhood,
um, and we can also talk about skip connections, uh, and so on.
So, um, let me talk about some of these concepts,
uh, in a bit more, uh, detail.
So first, I wanna talk about the notion of batch normalization and
the goal of batch normalization is to stabilize training of graph neural networks.
And the idea is that given a batch of inputs,
given a batch of data points, in our case,
a batch of node embeddings,
we wanna re-center node embeddings to zero mean and scale them to have unit variance.
So to- to be very precise what we mean by this,
I'm given a set of inputs,
in our case this would be vectors of, uh, node embeddings.
I then can compute what for- for every coordinate,
what is the mean value of that coordinate and what is
the variance along that coordinate, uh,
of the vector, uh,
acro- across this n, uh,
input points, X, that are part of a- of a mini-batch.
And then, um, I can also have, um, in this case,
I can have, um, uh, um,
uh, then, uh, you know, there is the input,
there is the output that are two trainable parameters, gamma,
uh, and beta, and then I can,
uh, come up with the output that is simply,
um, I take these inputs x,
I standardize them in a sense that I subtract the mean and divide by
the variance along that dimension so now these X's have, uh,
0 mean and unit variance and then I can further learn how to
transform them by basically linearly transforming them by multiplying with gamma and,
uh, adding a bias factor,
uh- uh, bias term, beta.
And I do this independently for every coordinate,
for every dimension of,
uh, every data point of every embedding,
i, that is in the mini batch.
So to summarize, batch normalization stabilizes training,
it first standardizes the data.
So stand- to standardize means subtract the mean,
divide by the, uh,
by the standard deviation.
So this means now X has 0 mean and variance of 1,
so unit variance, and then I can also learn, uh, these parameters,
beta and gamma, that now linearly transform X
along each dimension and this is now the output of the, uh, batch normalization.
Um, the second technique I wanna discuss is called, uh, dropout.
And the idea- and what this allows us to do in neural networks is prevent over-fitting.
Um, and the idea is that during training,
with some small probability P,
a random set of neurons will be set to 0.
Um, and, uh, during testing,
we are going to use all the neurons,
uh, of the network.
So the idea is if you have a,
let's say, feed-forward neural network,
the idea is that some of the neurons you set to 0 so
that information now flows only between the neutrons that are not,
uh, set to 0.
And the idea here is that this forces the neural network to be
more robust to corrupted data or to corrupted inputs.
That's, uh, the idea and because the neural network is now more robust,
you- it prevents neural network, uh, from over-fitting.
In a graph neural network,
dropout is applied to the linear layer in the message function.
So the idea is that when we take the message from, uh,
node u from the previous layer and we were multiplying it with this matrix W here,
uh, to this linear layer,
to this W, we can now apply dropout, right?
Rather than saying here are the inputs multiplied with W to get the outputs m,
you can now basically set some of the parts,
uh, of the input, uh,
um, as well as the output, uh,
to 0, and this way, mimic the dropout.
That's the- that's the idea,
and, uh, as I said,
in terms of dropout,
what it does is it helps with, um,
preventing over-fitting of, uh, neural networks.
The next component of our graph
neural network layer is in terms of non-linear activation function.
Right, and the idea is that we apply this, uh,
activation to each dimension of the,
uh, of the embedding X.
What we can do is apply a rectified linear unit,
which is defined simply as the maximum of x and 0,
so the way you can think of it based on the inputs- input x,
the red line gives you the output.
So if the x is negative,
the output is 0,
and if x is positive, the output is,
uh, x itself, uh,
and this is most commonly used, um, activation function.
And we can also apply a sigmoid activation function.
A sigmoid is defined here,
here is its shape.
So as a function of the input,
the output will be- will be on value 0 to
1 and this basically means that you can take a
x that- that has a domain from minus infinity to plus infinity and kind
of transform it into something- to- to the bounded output from 0 to 1.
And this is used when you wanna restrict the range of
your embeddings when you wanna restrict the range of your output.
And then what empirically works best is called a parametric ReLU, uh,
and parametric ReLU is defined as the maximum of
x and 0 plus some trainable parameter alpha,
minimum of x and 0.
So this basically means that empirically,
uh, if x is greater than 0, uh,
you output x itself and if it's less than 0,
you- you output some x multiplied by some coefficient,
uh, a, in this case.
This is not an attention weight,
this is a different coefficient, uh, we trained,
and now the shape of the parametric ReLU looks like I show
here and empirically this works better than ReLU because you can train this,
uh, parameter, uh, A.
So, uh, to summarize what we have discussed and what we have learned so far,
we talked about, uh,
modern deep learning modules that can be included into
graph neural network layers to achieve even better performance,
we discussed about linear transformations,
batch normalization, dropout, different activation functions,
and we also discussed the, uh,
attention mechanism as well as different ways,
uh, then to aggregate, uh, the messages.
Um, if you wanna play with these different,
um, architectural choices in an easy way,
we have actually developed a package called GraphGym that basically allows you to,
very quickly and easily try out, uh,
and test out different design choices to find the one that works best,
uh, on your, uh, individual, uh, problem.
So, uh, if you click this, this is a link,
it will lead you to GitHub, um,
and you can play with this code to see
how different design choices make a practical difference,
uh, to your own, uh,
application or use case.

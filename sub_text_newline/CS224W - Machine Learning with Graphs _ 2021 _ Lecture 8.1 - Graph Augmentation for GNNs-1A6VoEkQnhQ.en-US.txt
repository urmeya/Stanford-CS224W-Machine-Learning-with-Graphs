Welcome, everyone, to the class.
Um, we are going to continue with the discussion of, uh,
what kind of design choices that we have when, uh,
training or, uh, designing graph neural networks.
Um, and then in the last part of the lecture today,
we're going to talk about graph neural network training, um,
and various other aspects of how do we make these,
um, models, uh, work.
So to start, um, to remind everyone,
we are discussing deep graph, uh,
encoders, in particular, graph neural networks,
where the idea is that given the input graph on the left,
we wanna transform it through several layers of non-linear, uh, transformations.
Uh, several layers of a neural network to, um,
come up with good predictions at the level of node's, uh,
address, as well as entire graphs.
Um, and the, the formalism we have
defined is called graph neural network, where basically,
for every node in the network,
we define the computation graph that is
based on the network neighborhood around that given target node.
So essentially this means that
the input graph network structure
around the target node defines the neural network structure.
And then we discussed that now,
in order to make this neural network, uh,
architecture work because every node gets to define its own neural network architecture,
its own computational graph that will depend on the position of the node in the network.
Then what we can do in the, um,
to make this work is we have to define several different operators,
um, in the architecture.
So first we said we need to define what we call a message passing function,
a message transformation function that will take the message from the child,
transform it, and pass it towards the parent.
We have to define the notion of message aggregation that
will take the transformed messages from the children
and aggregate them in
an order invariant way to produce the- the one combined message from the children.
Then we said that when this message arrives to the parent,
we need to decide how to combine it with the parent's own message
from the previous level to then create the embedding of the node,
which can then be passed on.
So this is how we defined a single layer of a graph neural network.
Then we discussed how to combine or how do you link, or stack multiple layers together.
Um, and what we are going to discuss today,
is the point Number 4 around what kind of graph and
feature augmentation can we create to shape the structure of this neural network?
As well as we are going to talk about
learning objectives and how to make the training work.
So that's, uh, the plan for today.
So first, let's talk about graph augmentation for,
uh, graph neural networks.
So the idea is that raw input graph does not
necessarily need to present the underlying computation graph, all right?
So what I discussed so far was that if I wanna create
a graph neural network for a given target node in the network,
then I take the information from the neighbors of it.
Here are the neighbors,
and then each of the neighbors takes information from its own neighbors.
And this defines the graph neural network.
However, um, this translation from
the input graph structure to
the graph neural network structure does not need to be kind of one-to-one.
I don't need to take the raw input graph and,
uh, interpret it as the computational graph.
I can use, um,
various kinds of techniques to create the computation graph of the graph neural network.
And the two techniques we are going to talk about is
graph feature augmentation and graph structure augmentation.
So what we assume so far as I said,
is that the raw input graph directly defines
the computational graph of the graph neural network.
And there are many good reasons why we would- why we would want to break this assumption.
So, um, we would wanna break it at the level of node features.
Many times, for example,
input graphs may lack, um,
features, attributes, perhaps you wanna- sometimes,
um, the features are also, uh,
hard to encode so we may wanna help the neural network to learn a given concept easier.
And then in terms of graph structure, sometimes,
graphs- input graphs tend to be too sparse and it's
inefficient to do message passing over
a very sparse graph, it would take a lot of iterations,
a lot of GNN depth.
Sometimes they are too dense and the message passing becomes too costly.
If you think, for example,
doing message passing on an - on top of
an Instagram or Twitter network and you hit the Kim Kardashian node,
then you need to aggregate from all her gazillions of followers, right?
So that is very expensive.
So the question is when you hit a,
a high degree node,
what do you- what do you do?
Do you really need to aggregate from all the neighbors of that high degree node?
Or perhaps can you just select a subset of the neighbors?
And then another important consideration is that sometimes this graph is just too large,
so we cannot fit the computation graph into the GPU memory.
And again, certain augmentation techniques are needed.
So basically, the point is that sometimes
it is unlikely that the input graph happens to be
the optimal computation graph for computing
GNN-based embeddings and the techniques we are
going to discuss next will give you some ideas.
What can we do to improve the structure of the graph so that it
lends better to the graph neural network embeddings?
So we are going to talk about, uh,
augmentation approaches and we are going to talk
about in particular the graph feature augmentation, where,
um, it- it can be the case that the input graph lacks attributes,
lacks features and we are going to create features so that,
uh, GNN has easier time to learn.
And then we'll also talk about the graph structure augmentation.
As I said, if graph is too sparse,
we can- we will be able to add virtual nodes and edges.
If it's too dense,
we can decide to do some kind of sampling of neighbors when doing message-passing.
And if graph is too large,
then we can subsample subgraphs to compute the embeddings.
And this last point we are going to,
to talk in more detail when we discuss scaling up GNNs.
But these are some of the techniques we are going to learn about today.
So why do we need feature augmentation, right?
So first, I wanna talk about graph feature augmentation.
So let's discuss why do we need this.
Sometimes, input graphs do not have any node features.
This is common, right?
If the input is just the graph adjacency matrix.
And what I'm going to discuss next is several standard approaches.
How do you deal with this situation and what can you do?
So first idea is that you simply assign a constant value,
a constant feature to every node.
So basically all the nodes have the same future value, value of 1.
And then if you think of what aggregation does,
it basically counts how many neighbors,
how do- does a node have at level- at Level 1?
How many do they have at Level 2?
How many do they have at Level 3?
So this would, in some sense,
allow you still to capture some notion of how does
the network neighborhood structure about a given node look like,
even though all the nodes have the same feature,
which is uh, which has a value 1.
Another idea that you can do is to assign unique IDs to nodes.
So basically these IDs are then converted to one-hot vectors, right?
So basically it means if you have a network on, um, six nodes,
then the idea is that you can assign a one-hot encoding to every node in the network.
So what I mean by this is now a feature vector for every node in
the network is simply a six-dimensional binary vector where you know,
node ID Number 5 has a- has a value of 1 up here.
If this is, you know, the node ID Number 5.
Um, also notice that this ordering of the nodes is totally arbitrary.
So there are some issues with one-hot encodings because it might
be hard or impossible to generalize them across different graphs.
But if you work with a single graph,
then this type of approach might be fine because
every node basically has now a unique one-hot encoding.
There is a flag value 1 at the ID of that single node.
And this now allows you to,to learn very expressive models because the models know
actually what are the IDs of the neighbors of the node in the network.
Of course, it might be costly because now your feature representation, your,
your number of attributes that the node has number of features,
the node has equals the number of nodes in the network.
So- so, um, that,
that is quite an expensive feature representation
for a node if the network is large. [NOISE]
So how do these,
uh, two approaches compare, right?
How does this adding a constant feature versus one-hot encoding,
uh, how do they compare, right?
In terms of their expressive power constant feature,
so every node having a value of 1,
um, has kind of medium expressive power, right?
Not- all nodes are identical,
but as- as we will talk about this later, uh,
GNN can still learn about
the graph structure and the neighborhood structure around the node, right?
In some sense, let's say an aggregation function like summation allows you to say,
"Hi, I have three- if this is the node of interest,
I have three neighbors at level 1.
I have, let's say, uh,
two neigh- two neighbors at level 2," and so on and so forth, right?
So this is what this we'll be able to learn.
So it's still able to capture some simple part of the graph, uh, structure.
Uh, one-hot encoding, uh,
has high expressive power, right?
Each node has a unique ID,
so a node's specific information can be stored or can be retrieved or learned.
So you can learn, "Oh,
I have a neighbor with ID number 2," and
perhaps that is important to determine your own label.
So, um, the expressive power is very high.
Um, you know, do we allow for- does this approach allow for,
uh, in, uh, an inductive learning capability?
What- what does this mean is, uh,
generalization to unseen nodes or generalization to, um,
nodes that are not yet part of the network or
generalization to a new graph that I have never seen during training.
Of course, the constant feature has high inductive, uh,
learning, uh, capability, because it's simple to generalize the new nodes, to new graphs.
We assign constant feature to them and just apply a GNN.
While for example, in, um, uh,
in the one-hot encoding case,
uh, we cannot really do that, right?
Because, uh, we cannot generalize to new nodes, right?
New nodes introduce new IDs.
Those IDs were not part of the training.
The GNN does not know how to,
uh, embed unseen nodes.
Um, so this is the- this is the issue,
um, in terms of one-hot encodings.
You need to know the entire node set at the time of training,
and you need to know the edges of that node set at the time of training.
In terms of computational cost, um,
as I said, uh,
constant node feature is very cheap.
It's just one- one value per node,
while the one-hot encoding has high because each node has a,
um, uh, feature vector that is length of the size of the network, right?
Its, uh, number of vertices is the dimensionality of the feature vector,
so we cannot apply these to larger graphs.
And then, you know, when would you apply one or the other?
You would, um, you can apply the constant feature essentially to any graph,
uh, whenever you care about inductive setting and generalizing to new nodes.
Uh, but, uh, the expressive power of the model will be limited.
On the other hand, one-hot encoding, um,
is very powerful, allows you to learn much more intricate structure around the network.
But it can only be applied to small graphs and to
transductive settings basically where all the nodes
and all the edges are known at the time of the training of the model.
So this was in terms of, uh,
feature- one idea of feature augmentation when we have no features, uh, on the nodes.
Another motivation for why we would want to do
some feature augmentation is that sometimes,
uh, certain structures are hard to learn for a GNN.
So sometimes, we actually want to encode a bit of
a graph structure into the node attribute vector as well.
So, uh, the idea, for example,
is that, um, to give you an example,
like kind of an edge example is that it's very hard for a GNN to count,
uh, you know, what's the length of a cycle a node is on?
So, um, and the question is, you know,
could a- could a GNN learn the length of a cycle a given node, uh, resides in?
And unless you have discriminative node features,
uh, this is not possible, right?
So what I mean by it is, for example,
here in this example,
node v_1 resides on a cycle of length 3,
while here this, the node v_1 resides on a cycle of length 4.
And in both cases, right,
v_1 has degree 2,
and its neighbors have degree 2.
So it's kind of the question is, you know,
how do I- how does this node know that now it's in a cycle of,
uh, length, uh, 4 versus 3.
Something like this is very important, for example,
in chemistry because these- these could be different kinds of, uh,
chemical structures or different kinds of, uh, ring structures.
And, uh, the reason why a, uh, uh,
plain GNN cannot differentiate between,
you know, node 1 in, uh,
a cycle of length 3 versus a cycle of leng- length 4
is that if you look at the GNN computation graph,
re- both- for both of these nodes V_1 and V_2,
the computation graph is exactly the same.
Meaning, V_1- V_1 and V_2 have two neighbors,
um, each, and then, you know,
these neighbors have, uh,
one neighbor each and the computation graph will always look like this.
Uh, unless, right, you have some way to discriminate nodes based on the features.
But if the nodes have the same,
um, set of, uh,
features that not- you cannot discriminate them based on their attributes,
then you cannot learn,
uh, to discriminate node V_1 from V_2.
They will- from the GNN point of view,
they will all, uh, look the same.
I'm going to go into more, uh,
depth, uh, uh, around this,
uh, this example and, er,
what are some very important implications of it and
consequences of it when we are going to discuss the theory of,
uh, graph neural networks.
But for now, uh,
the important thing is to see to understand that a GNN has a hard time capturing, um,
or is not able to capture whether a node is on
a length 3 cycle or a length 4 cycle unless these nodes,
um, on the cycle would have some discriminative features.
And the reason why we are not able to distinguish between
these two nodes or why GNN is not able to dis- distinguish,
uh, between these nodes is because computation graph looks,
uh, the same in both cases, right?
A node has two neighbors,
and then each of these neighbors has two other neighbors.
And if- if all the neighbors look the same,
they don't have any discriminative colors to them,
then computation graphs, in all cases,
will look the same so,
uh, the embeddings will be the same.
So it doesn't matter whether you are part of
a cycle or you are a part of a infinite length,
uh, chain, the computation graph will always be the same.
So the GNN won't be able to distinguish the nodes again,
unless there is some discrimination between the nodes.
So if nodes have different colors,
then a GNN could,
uh, capture the pattern.
So, um, what is the solution?
The solution is to create a feature vector for every node that would,
uh, that would, for example,
give me the cycle count, right?
So basically, I would augment the node features with the cycle count information.
So one idea, for example would be is to create this,
uh, um, vector where, you know,
this is the number of cycles of length 0 the node participates in,
number of cycles of length 1,
length 2, length 3, right?
So length 1 is a self-loop,
length 2 would be, um,
if in a directed graph would be a, uh,
reciprocated connection, length 3 is a,
um, a triangle, you know,
length 4 is a square, right?
And, uh, you could now, uh,
append this type of feature vector to
whatever feature vector you already have, uh, for the nodes,
and this way increase the expressive power of the graph neural network,
especially if your intuition is that, let's say,
cycle information, uh, is important.
And of course, there are many other, uh,
commonly used, uh, uh,
techniques to augment features.
People very much like to include node degree.
It's a very simple to compute feature, but again,
allows you to disti- distinguish between different nodes.
You can include clustering coefficient that essentially
counts how many cycles of length 3 a node participates in.
So this is, um, triangle counting.
Uh, you could also have other types of, uh,
features like PageRank or,
uh, node centrality metrics, right?
So essentially, any features we have introduced in Lecture 2, um,
you could include to augment, uh,
the GNN to help it learn the network structure,
uh, better and, uh, faster, right?
So in some sense, in many cases,
the goal of the- of the machine learning, uh,
scientist is to essentially understand the intuition betw- be- behind
the learning algorithm and tries to help the- the algorithm to learn the patterns,
perhaps, we as domain scientists would know are important.
And by encoding some of the graph features,
many times, you can very much, uh,
speed up and improve the performance of the model because you kind of help,
uh, you know, you point the model to the- to the places where there might be good signal.
So this was about augmenting the features, uh,
of the node, and we talked about adding a constant feature,
we talked about adding, um,
one-hot encoding, and we also talked about,
um, adding various kinds of graph, uh,
structure information like the cycle count or node degree,
uh, to- to augment the node feature information.
Now, I'm going to switch, uh,
gears and I'm going to talk about adding and changing the graph structure information.
So we are going to augment
the underlying graph structure again to help, uh, with to the learning.
The way we are going to do this is to add virtual nodes and virtual edges.
So the first motivation we wanna discuss is we wanna augment sparse graphs,
so we wanna add a virtual address.
A common approach, for example,
would be to connect two hop neighbors via virtual edges.
So the intuition is,
or one way to say this is that instead of using
the adjacency matrix A of a- for a GNN computation,
we are going to use A plus A squared, right?
If you, again, uh,
remember early on in the course,
maybe Lecture 2, 3,
we discussed that powering the adjacency matrix, um, counts,
um, the number of, uh, nodes that,
uh, are neighbors at level 2,
level 3, and so on.
So by basically adding- by powering the matrix and adding, uh,
A adding it to the adjacency matrix,
now basically we connect all the nodes that are two-hop neighbors.
Um, and this is very interesting, for example,
especially in bipartite graphs.
Because if in a bipartite graph where for example,
authors and the papers they author,
you create a square,
then basically you create a projection and you
created a paper co-authorship network,
or an author colle- collaboration network, right?
So this would mean that you either connect
two authors that have written at least one paper together,
or you connect two papers if they were written, uh, by the same author.
It just depends do you do, um,
you know,  A times A transpose or do you do, uh,
A transpose times A,
in a case of a bipartite graph because the adjacency matrix, uh, won't be square.
So, uh, that's- that's one idea how you can kind of include
additional information and what this will help in
the graph neural network is that rather than, you know,
if you think about message passing,
an author sending a message to the paper and then paper
sending it back to the author by connecting two authors,
they will be able to directly exchange messages,
which means that the depth- the number of layers of the graph neural network will be,
uh, able to be smaller, um,
and you'll be able to train it, uh, faster.
Of course, the um,
- the issue will then become that you have too few- that you will have
too many neighbors to aggregate from and that may add er, more complexity.
But we'll talk about how to fix that um, er later.
So if graphs are too sparse,
er, as I said,
one idea is to- to connect nodes that are-that are
two edges-virtual edges between length 2 or length 3 connected nodes.
Another idea is to add a virtual node um,
and the virtual node will then connect to, let's say,
all or some carefully chosen subset of the nodes in the graph, right?
So for example, imagine you have
a super sparse graph where have two nodes are very far apart in the graph, right?
They are, let's say, ten hops apart, right?
And now if one node needs to send a message to the other node,
you need a length, you need
a ten layer graph neural network to be able to allow for that communication.
But in some cases,
you may know that these two nodes actually need to communicate.
Even though they are farther apart in the original graph,
they need to send messages to each other, right?
One depends on the other.
So what you can do in that case is you can create a virtual node and then connect,
for example, several nodes to it.
And this way, you can connect nodes that are very far in the original graph structure er,
to be able to communicate with each other much more efficiently, right?
So basically, after adding a virtual node,
er, all the nodes will have a smaller distance to each other.
So you'll- the message passing will be more er,
- more efficient, it will happen faster.
The depth of the graph neural network won't have to be er, that large.
Er, so that's another technique that sometimes um, is er,
-is-is a good idea to have in your toolbox um,
if the basic approaches don't work.
And then er, the last thing I want to talk about- about graph structure information is
not when you have too few edges in the graph and you want
to kind of make the message passing er, more efficient.
Er, the question becomes,
what if you have too many edges?
What if the graph is too large, right?
Again, think of my er,
Kim Kardashian example, right?
Or Lady Gaga used to be er,
the highest degree node in the Twitter network a few years ago,
but I think she is not number one anymore.
But the point is, right, you have these high degree nodes in networks and
aggregating messages from millions- over tens of millions or hundreds of millions of er,
people that are connected to it can become er, quite er, expensive.
So um- so far, right?
We said, let's use all the nodes, all the neighbors er,
for message passing when defining
the graph neural network and the idea that we are going to explore here and
introduce is, what if I sample node's neighborhood er, for message passing?
And of course I could do random sampling,
but it turns out there are far better heuristics er,
that allow you to really carefully select
what neighbors to collect information from and what neighbors to ignore.
So um, here is the idea.
So the idea is that, for example,
what if we randomly choose two neighbors to pass messages from in a given layer?
So for example, for our neighbors of node A,
we-we would decide that out of the three neighbors,
we are only going to select two of them and ignore the third one.
So this would, for example,
in this case mean that A only collects information from B and D but ignores
the C. So now our message-passing computation graph would look like this.
Why is this good? This is good because now computation graph is much smaller.
Um, of course, why is it bad?
It's because perhaps node C has
very important information that would allow
us to make a better prediction at node A,
but because we ignored it,
this will be- it will be harder for the neural network er,
to learn that better, right?
So this is kind of the trade-off here is,
yes, you gain computational efficiency,
but in the worst case,
you kind of lose some of the expressive power er,
because you-you-you dropped out er,
some edges, you dropped out some information that might be important.
And of course, in practice,
um, if you have a super high degree node,
you can really sub-sample how many neighbors you- you er,
aggregate information from because you really want to kind of aggregate from
important neighbors and from all the kind of noisy unimportant ones,
er, you can er, - you can ignore them.
That's kind of the intuition.
So- and of course we can, um, er,
do this sampling differently er, every time.
We could even make it, er,
such that sampling changes between er,
layers and this way,
the network- the aggregation functions actually become er,
robust to how many neighbors do you sam- do you collect the information from, right?
So the idea would be, for example,
in the-in the next layer,
or in the next er, minute- er,
in the next er, epoch of training,
we can sample different nodes er,
for the same node A , right?
So what this would mean is that, for example,
now we re-sample and we decide to collect from C and D but ignore er,
B and this is now how the computation graph would look
like and this is also good because it adds a
robustness to our neural network training approach, right?
It er,-it now means that er,
A will be able to collect information-will learn how to robustly collect
information from some subsets of its- of its neighbors and won't er,
suffer to much if, for example,
an edge is er, missing in the network.
So that's also, um,
one reason why this er,
neighborhood sampling, er, is a good approach.
Um, and why is this interesting?
It's because in expectation,
right after a lot of different er,
random samplings, we will get embeddings
similar to the case when all nodes are being used.
Er, but the benefit is that this greatly reduces computational cost and,
you know, in the small graphs,
this might not be er,
so obvious but if you-if, you know,
node A has 30 million neighbors,
then we cannot aggregate from 30 million.
The question is, can we decide what are the top 100,
maybe top 1,000 most important nodes?
Most important, let's say friends,
if this is a social network or the true friends er, of this person,
and we want to aggregate information er,
from them rather than from all the kind of, er,
random followers, er, all over the world, right?
Um, and by doing this sub-sampling,
we can make the computation graphs much, much,
much smaller, which allows us to scale GNNs to massive graphs.
And this is very important in industrial applications
like recommender systems and social networks,
where you have, you know,
networks of billions and tens of billions of er,
nodes and edges and you need these  kind of,
um, techniques to be able to scale.
Er, and in practice, this is a very good approach to scale
up- to scale up graph neural networks.
So er, this is what I wanted to say about neighborhood sampling er,
and give you this er, example.

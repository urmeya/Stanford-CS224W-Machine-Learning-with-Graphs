Uh, today we are going to talk about new,
very interesting, and exciting problem called, uh,
subgraph matching and subgraph counting or, uh,
frequent subgraph counting and frequent subgraph matching.
What is exciting is that we will, uh,
talk about how can you do this with neural networks.
So basically, how can you take a very,
uh, classical, um, uh,
problem, uh, kind of classical combinatorial problem and cast it as a,
um, machine learning problem.
Uh, and this would be the exciting part and we are
going to use embeddings and graph neural networks,
uh, to make this all work,
and we will be able to scale it up and make it accurate, uh,
and kind of swap- ste- side-stepping all the kind of complex,
um, uh, discrete base,
the matching and counting.
So, um, here's the idea for today. All right?
We are given a big graph and we would like to
identify what subgraphs are common in this graph, right?
Like we can think of the graph as a- as a being
composed of a set of building blocks, meaning small subgraphs,
and we can think of a lot of these graphs almost like being composed of
these little pieces as the same way as let say, um, you know what, uh,
once you build something out of Legos,
that object that I know how so- that you built,
it's composed of small pieces that combine it,
uh, all together and create a house, right?
So in some sense, what we'd like to identify is what
are the most common Lego bricks that are,
uh, uh, that, uh, compose the graph together.
So, um, the power here will be that we are able to
characterize and discriminate different networks based on these building blocks.
And the task today will be,
how do we identify and define,
uh, these building blocks, uh, for graphs.
So to give you an example,
you can take a set of molecules,
as I show you here,
and you can represent them as graphs.
and what are common substructures, uh, in these, uh,
And now you can ask what are common, better,
in these graphs, um, and, uh,
this way I'm able to understand,
let's say the structure of these molecules and what are the important substructures.
In this case, for example,
you could identify that this particular,
um, substructure is, uh,
common across all the molecules,
and you know, it turns out that this is actually a very, uh, important, uh,
group that, uh, would tell you whether,
uh, the molecule is acidic or not, right?
So in many domains,
you have these recurring structural components
that determine the function or the behavior,
uh, of the graph similarly as this example in, uh, molecules.
And of course the question is,
how do we, uh, extract,
identify these commonly occurring, uh, substructures?
Um, we are going to, uh,
approach this problem in three different, uh,
kind of as a set of three steps.
First, we are going to talk about subgraphs and motifs,
where we are going to define what is a subgraph,
what is a motif,
and then we'll also talk about how do we identify, uh, significant motifs.
And then after we will be done with the first step,
we'll talk about how can we use
graph neural networks and embeddings to represent subgraphs,
and how can we then quickly identify common subgraphs, uh,
using only the embedding space and no need to do,
um, a very expensive, uh,
discrete type, uh, matching.
So let me first go and define,
um, subgraphs and, uh, motifs.
So, uh, here are two ways to formalize this idea of,
uh, uh, building blocks of networks, all right?
So we are given a network,
we are given a graph with a set of nodes and a set of edges,
and the first, uh,
definition we will be using is called node-induced subgraph,
where basically the idea is that you take a subset of nodes
and all the edges that connect these nodes.
So induced means it's,
uh, determined by the node set.
So the idea is that let's say this G prime on a- on a set of, uh,
nodes V prime and a set of edges E prime is
a node-induced subgraph if node set is a subset of the nodes,
and then edge- edge set is simply, um,
a set of all the edges that the- that the- existing the big gra- uh,
in the bigger graph where both endpoints are part of my, uh, subgraph, right?
So this means that G prime is a subgraph G,
uh, induced by the vertex set,
uh, V prime, right?
So basically, induced means,
it takes- it say- it ta- it means take
all the edges between the- the vertices that you have determined, right?
So, uh, we only get to choose the vertices, the edges.
Uh, the edges are determined by the set of vertices, uh, we picked.
Um, so this is what people say node-induced subgraph or generally,
we just call it induced subgraph.
It's basically a subgraph defined by a set of nodes where we take all the edges,
uh, between that set.
Um, then a second definition, uh,
that is less common is to talk about edge-induced subgraphs.
Here we take a subset of edges and all the corresponding nodes.
So G prime is an edge-induced subgraph, um, er,
simply defined by the subset of edges
E prime that is a subset of the edges E in the entire network.
And then now in- in this case,
the V prime, uh,
the set of nodes is simply defined,
um, through the edges we have selected.
Um, so in-in- in,
um, the terminology we'll be using, usually,
we say this that this is
a non-induced subgraph or just a subgraph because it's determined,
uh, by the set of edges rather than by the set of nodes.
When we say induced subgraph,
we would mean select the set of nodes and,
um, determine the edges,
and when we say non-induced,
it would mean just select the node- the edges,
and then the nodes get automatically, uh, determined.
The- the two ways of formalizing network buil- building blocks,
um, it will really depend,
uh, on the domain we are interested in.
Most often people like to work with induced subgraphs because otherwise if you do edge,
uh, induced subgraphs, the, er,
the number of, uh,
possibilities, uh, explodes.
So especially in natural science domains like
chemistry and so on where we worry about functional groups,
we are going to use node-induced subgraphs.
Uh, in other domains, for example,
knowledge graphs, it is actually often edge-induced subgraphs.
For example, if you think about focusing edges that represent,
uh, local logical, uh, relations.
So now that we have defined these two no- notions of a subgraph,
then, um, you know,
the preceding definitions of subgraphs, uh, uh,
basically say that V prime is a subset of V and E prime is a subset of E,
which mean- basically means that nodes and edges are taken from the original graph,
uh, G. Now, um,
you could also say, okay, what if,
er, V prime and E prime comes from totally different, uh, graphs?
For example, you could- can you somehow define that,
you know, you have two different graphs,
G_1 and G_2, could you somehow say that G_1 is contained in G_2?
Right? G_1 is this triangle of three nodes,
and we can see that in G_2,
this triangle is contained by the subgraph X,
uh, Y, uh, and Z.
So how do we say that one graph G_1 is contained in another,
let's say bigger graph, uh, G_2.
The way we do this is that we need to define the- the, uh,
problem or the task of graph isomorphism,
where the graph isomorphism problem,
uh, i- is the following problem,
you wanna check basically say, yes,
no, whether two graphs are identical.
So the idea is on having graph G_1 on some nodes and edges,
and I have graph G_2 and again,
some nodes, uh, and edges.
And I say that G_1 and G_2 are isomorphic.
If there exists a bijection, basically,
it mea- means there exists
a one-to-one mapping between the nodes of one graph to the nodes,
uh, of the other graph such that,
uh, all the edges, uh, are preserved.
Meaning if u and v are connected in,
um, uh, in the graph 1,
then the mapping of node, uh, uh,
u and the mapping of node v,
uh, is also connected, uh, in graph 2.
So this a and b should actually be, uh,
u and v, so we need- we'll fix that, right?
So this mapping F is called graph isomorphism.
So to give you an example,
if I have two graphs here, you know,
one- one looks like this,
the other one looks like that,
they are isomorphic because if I, uh, map, uh,
these nodes, uh, one to the, uh,
one to the other, as I show it here,
then basically, uh, the edges,
for example, these two nodes are connected here,
they're also connected there,
this- there is this connection which is here, and so on.
So clearly, these two graphs are isomorphic.
I can map nodes from one to the other,
and I'm able to preserve,
uh, all the edges.
In this- in a similar sense,
these two graphs are non-isomorphic
because there is no way for me to map these four nodes, uh,
of the left graph to the nodes of the right graph,
such that if two nodes are connected on the left,
I'd know they are also connected on the right.
Um, so this is the problem of
Graph Isomorphism is checking whether two graphs are identical.
And- and the issue here is that we
don't know how map- how nodes map to each other, right?
It goes back to this idea that ordering or- i- i- no- ideas of the nodes are arbitrary.
So really there is no special order to them.
So we really need to check in some sense,
all possible orderings to determine if one graph is the same than the other graph.
Um, and if you ask, "Okay,
so how hard is this graph isomorphism step um, problem?
It is actually not known whether graph isomorphism is NP-hard.
But we don't know any polynomial algorithm for solving graph isomorphism.
So it seems it's this kind of
super interesting problem where we cannot prove that it's NP-hard,
at same time, we don't know uh,
any algorithm or nobody was able to
determine the algorithm that would solve this in polynomial time.
So it's somewhere in between. Nobody knows.
Still a big open question.
So this is now the notion of graph isomorphism.
So now we can define the notion of subgraph isomorphism.
Where do we say that uh,
G- G2 is subgraph isomorphic to G1 if for some subgraph of G2,
the subgraph is isomorphic to G1.
So um, what they commonly say is also that simply that G1 is a subgraph of G2, right?
And we can use either node or edge induced subgraph definition in this case.
And this problem is known to be NP-hard.
To give you an example, right,
this is G1, this is the graph G2.
I say G1 is subgraph isomorphic to G2,
or G1 is a subgraph of G2.
Because if I use this particular node mapping, right?
A maps to X, B maps to Y,
and C maps to Z,
then these connections between the three nodes in G1 are preserved in G2 as well.
Notice that we don't care about additional connections.
So this doesn't matter because this node- this structure is not part of G1.
And the other thing that is also important to note is
that this mapping does not need to be unique.
It is just enough to find one mapping where I map nodes of one er,
sub er, graph to the nodes of the other graph in a unique way,
in a one-to-one mapping.
So it's- two nodes cannot map to the same nodes.
Right, so in this case,
we have now been able to mathematically define and determine that
G1 is a subgraph of G2 because there exists this bijective mapping,
so one-to-one mapping, so that uh, every- any er,
nodes from G1 map to G2 and if two nodes in G1 are connected,
then their maps, uh their transformations are
also connected in G2 and the other way around, right?
So that's the- that's the important uh, part here.
So now um, we have th- what have we learned so far?
We defined the notion of a subgraph.
We defined the notion of a graph isomorphism problem,
and then we also defined the notion of a subgraph isomorphism problem, right?
Basically saying is wha- is a small graph contained in the bigger graph?
Now, of course, um,
when we talked about subgraphs,
usually we are interested in all subgraphs up to a given size.
Size, meaning the number of nodes if we talk about different subgraphs of given size.
So to show, actually there is a lot of
different subgraphs of a given size and this number increases very, very fast.
So for example, if here I show an example of
all non isomorphic connected undirected graphs of size 4, right?
These are all possible graphs on four nodes, um,
where the number of nodes is fixed,
number of edges can vary,
and the other constraint is that these graphs are connected.
So there is four different graphs on four nodes undirected.
Now, for example, if you say what are
non-isomorphic connected directed graphs of size 3,
if you look at that, there's already 13 of them, right?
It's only three nodes.
But because edges are directed,
I can have edges in different directions,
and this gives me 13 different uh, graphs.
So what does- wh- why is this important?
Because if I have a directed graph and I say,
what are the building blocks of size 3,
then I would need to determine the frequency,
the number of times this particular subgraph number 1 is included in the big graph.
And then I need to determine how often is this guy um,
included and so on.
So the point is that the number of these building blocks,
different subgraphs, increases super exponentially.
So in general, people usually only counted these building blocks up to size 4, 5.
Because even at the level of 5,
there is thousands of them and it's kind of a lot to
keep track of and it becomes a very hard computational problem.
So now that we have defined the notion of subgraphs inclusion,
and I showed you that there are
many different possible subgraphs that are non-isomorphic with a given number of nodes.
Then, now we define the next concept that um,
will be important for today's discussion,
and this is the concept of a network motif.
A network motif is defined as a recurrent,
significant pattern of interconnections in a graph.
So now let's unpack this and determine like this, make it precise.
What do we mean by this?
First is um, we define a network motif as a pattern,
which means a small node induced subgraph.
Then we need to say what do we mean recurring, right?
Recurring means it has to appear multiple times, right?
It has to have high frequency.
It has to be contained many times in the underlying graph of interest.
And then there is another interesting part,
where we say significant,
and significant means that it's more frequent than what we would expect.
And of course, if you say more frequent than what we would expect,
then you need to have some way to say,
okay, but what would I expect?
And this means you need to have a null model,
you need to have a random graph null model.
So you say, aha, what I would expect in the model,
what I see in the reality, is there a big discrepancy?
If there is a big discrepancy,
this- then this subgraph pattern,
this motif must be an important thing so let's surface it out to the scientists.
To give you an idea.
Imagine I'm interested in this particular motif on three nodes in a directed graph,
then when I talk about motifs,
these motifs need to be induced.
So for example, this is not an instance of the motif
of interest because actually this is a triangle of three nodes.
It's not so bad, so
there's no edge in my motif here,
but there is there, for example.
So really the instance is here because this- this particular subgraph of interest,
this motif appears here and there is one-to-one mapping.
So I say, aha, I found this- the incidence of this thing here.
Of course, you know,
there are many other places where this same motif occurs.
So um, the question is,
why do we need this notion of motifs?
And motifs help us understand how uh, graphs work, how networks work.
They help uh, make us- they help us to make predictions based
on the presence or lack of presence uh, of a motif in a data set.
So for example, uh, feed-forward uh, loops,
this is defined as a feed-forward loop motif,
were found to be important, um,
for, uh, networks of neurons for- so basically for brain networks because they,
uh, neutralize what is called biological noise, uh.
Parallel loops are important in- in food webs because it says that,
um, eh, are given predators preying, uh, uh,
on two different uh, species that have a common food source,
and you know in, for example,
in gene control networks,
you have a lot of this type of a single- what is called single input modules
where um, this gene regulates uh, a lot of uh, other uh, genes.
So these are some examples of uh, significance of
motifs uh, for the function of a given underlying network.
So now, uh, let's go and define the two things we discussed.
First is, we need to define frequency,
second, we need to define significance.
So let's define frequency first, right?
Let's say G_Q is a small uh, sub-graph of interest and Gt be the big target graph.
Uh, and then we say that we will define a um, graph, uh,
level, subgraph frequency uh, by the following definition.
We'll say that frequency G- of this graph G_Q in the bigger graph,
G_T is the number of unique subsets of nodes, um, in, uh,
the big graph, for each the sub-graph, uh, um,
of the big graph induced by- by its nodes is- is,
uh, isomorphic to this,
let's call it G_Q, so the query graph.
So to give you- to give you an example, right,
this is kind of a mouthful,
but the intuition is actually quite simple.
Here I have the query graph,
here I have the target graph.
This query graph appears twice in the target graph, you know,
there two triangles, one is here,
the other one is here,
so the frequency would be 2.
Um, here's a different example,
imagine I have this star sub-graph um,
and I want to ask it,
how often does it appear in this graph, um, of uh,
interest? Actually here um, perhaps counter-intuitively,
the frequency will- will be super large because, you know,
the center node will map to the center node,
but then the number of the satellite,
these uh, leaf nodes um,
is huge because I can select any 6 out of 100,
and any 6 I select,
it's a different mapping.
So I basically count how many different ways am I able to
take this graph and map it to the target graphs?
So in this case,
the number of different mappings would be
100 choose 6 because out of the 100 nodes here,
uh, I want to choose different subsets of 6
because the s- the star graph uh, has uh, 6 nodes.
So here the frequency of this um, would be um, would be huge.
So um, this is uh, the graph-level sub-graph frequency uh, definition.
Uh, there is uh, also
a more precise uh, frequency definition ca-
called node level uh, sub-graph frequency definition,
and here the idea is that uh the query comes
with a- uh, comes with the graph as well as with an anchor.
And then we do the mapping,
we say to how many different nodes can this anchor be mapped?
Uh, so that this uh, sub-graph Q is contained uh, in the target graph, right?
So here we are saying,
I want to be able to map uh, the edges of the- of the graph uh, Q to the graph uh, T,
and I want to count to how many different nodes can this anchor uh, node uh, be mapped?
So in our case, for example,
if I have this star graph from the previous case,
and I select a center as- as a uh, as an anchor,
then the frequency of this um, graph in my target graph would be uh, 1.
So there is only one way to map this anchor uh, to my target graph.
For example, if I were to select the anchor as one of the satellites,
then the frequency of the sub-graph would be 100 because there is exactly 100 ways to map
the anchor to one of these 100 nodes such that the
entire sub-graph maps uh, to the uh, to the target graph.
So this is um, the node level sub-graph definition,
where we have this kind of anchor and we are asking how often can we
map the anchor together with the corresponding uh, sub-graph?
Okay. So now uh, that we have defined the notion of a sub-graph frequency, uh,
the graph level and the node level,
the last thing to say is, you know,
is it a problem if the graph is disconnected?
If I have multiple small connected graphs- disconnected graphs.
Solution is very simple,
I can s- simply treat
all these small um, separate sub-graphs
as one giant graph with multiple connected components,
so that is uh, no problem uh, at all,
just to kind of address this point.
So now that we have defined frequency,
we need to define the significance,
and we will define motif significance um, in a way that we compare
it- we p- compare the number of occurrences with some null-model,
with some kind of point of comparison and the idea is that if
a given sub-graph occurs um, in a real graph much more often,
than uh, than in a random network,
then this- then it has a signi- functional significance.
So let me now define uh, quickly how do we generate random graphs?
And the first way to define a random graph is
uh, the model called Erdos-Renyi random graph model.
And this random graph model has- is a stochastic graph model that has two parameters.
It has um, n and a p,
n is the number of nodes,
and p is a probability of an edge.
So how do you generate a graph from this model?
You simply create n isolated nodes and then for each pair of nodes,
you flip a biased coin with a bias p, and if the-
the- the- tai- the coin flip says create an edge,
then you would create an edge,
and uh, right, the generated graph is a result of a random process,
so the more t- the- you can generate multiple graphs and they'll be
different because the coin flips uh, will come up uh, differently.
Even if you set the same parameters,
here, you know, I have five nodes,
so n is 5 and probability of an edge is 0.6 and, you know,
this would be le- let's say three instances of
a random graph generated from this G n, p model.
So um, now the next question is,
can we have a more precise uh, model?
Like- because in this model, all I get to
specify is the number of nodes, and the probability of an edge.
So actually there is a more precise model uh, that is called configuration
model and the goal here is to generate a random graph with a given degree sequence.
So what does this mean is, if I have my real graph Gt,
I want to generate a random version of it.
One way to have a random version of it would simply be to say, you know,
my Gt has n nodes,
so let me generate an Erdos-Renyi random graph with n nodes.
And I will set the value of parameter p such that in expectation
this random Erdos-Renyi graph will have the same number of edges as my uh, G_t.
So I match in terms of number of nodes and the number of edges.
Um, in the configuration model,
we are going to match both the number of nodes,
number of edges, but also the degrees of the nodes.
So basically, we will say I wanna generate
a random graph that has a given degree sequence,
meaning I have nodes 1- n,
and each node has a given degree.
But I don't specify how the nodes connect to each other.
Um, the way I do this,
it's actually quite simple and elegant.
I create n nodes and for every node I create, uh, uh, k_i,
uh, spokes, right? So for example,
node B has degree 4,
so it has four spokes.
Node C has, uh,
degree 2, so it has two spokes.
What I can do now is I represent every spoke,
uh, as a node, right?
I take these spokes and I create spokes as nodes and- and the spokes from,
er, every, er, every, er,
supernode are- are kind of belong to a given box.
What I do now is I go and, uh,
I randomly pair up, uh, the spokes, right?
I basically- I randomly pair up these nodes.
And then I determine, um,
an edge between a pair of nodes if at least one spoke
from one partition links to the spoken the other partition.
So for example here, A and B are connected here because there is a node in,
uh, A that links to a node B.
Uh, of course, what is the issue here is sometimes it will happen that I will have,
uh, multiple spokes linked to each other
and that will only result in a single- single edge.
So I'm going to ignore that.
And of course it can also happen that for example,
these two nodes would link to each other.
So this would correspond to a self-loop.
And I'm also going to, uh, ignore this.
And kind of the reason why I can ignore all this is because, uh,
in practice, the probability of there being multiple edges between, um, er,
spokes coming from the same node or, um,
being a se- generating a self-loop is so- is so small that I can, uh,
ignore it for all practical, er,
purposes and also kind of mathematically,
uh, you can ignore it because it is, uh, so rare.
So basically this means that now I have
a very useful null-model of networks because I can compare
the real network with
a random version of the network that has the same degree sequence, right?
Node have the- nodes have the same degrees as in the, uh, G-real.
And now this gives me another different null-model where basically I create spokes, I
then create a G n, p by randomly connecting the spokes and then join these spoky nodes.
These mini-nodes it to get back,
er, a resulting, uh, graph.
So now that we have defined, uh,
two random models, the configuration model and the Erdos-Renyi model,
uh, generally, we would prefer to use the configuration model.
Now, we need to def- determine,
uh, and define what is motif significance.
And the- the intuition is that motif is already represented in a network,
uh, when we compare it to this random null-graph.
So the idea is the following.
I'm going to pick a sub-graph of interest and
I'm going to count its frequency in the real graph.
Then I'm going to generate a lot of random graphs, um,
that kind of match the real graph in terms of some statistics like the number of nodes,
number of edges, uh,
as well as degree sequence.
And I'm going to count the frequency of the same motif in this,
um, um, uh, random graph as well.
And then I'm going to define next a statistical measure that will
tell me kind of how- how- how big is the discrepancy between the,
um, frequency of the motif in the real g raph versus in the random version of it.
And in the statistic I'm going to define to quantify these is called a Z-score.
So let me explain, uh, what the Z-score is.
So, z- score,um, uh, of a given sub-graph
or a given motif I is- captures its statistically significance.
And the way we are going to do this is simply say, uh,
what is the number of times this motif I appears in the real graph?
What is the average number of times this same
motif I appears in the random versions of the real graph?
So because we have multiple instantiations,
I can compute the average and of course,
I can also compute the, uh,
ra-standard deviation of the frequency of
that motif in these different random instantiations of the,
uh, random graph corresponding to the, uh, real graph.
And the Z-score now,
will basically tell me how much over-
represented or under-represented is the motif, right?
And we are, uh, doing two things.
We compute-we compute, compare
the frequency in the real graph versus in the random graph.
But then we also, um,
divide by the standard deviation,
by the variance of that, uh,
of that count across multiple instantiations of the random graph, right?
So basically, what does this mean is that we somehow
normalize the count based on the natural variability,
uh, of- of the- of the count of that motif.
So now this gives me the Z-score of a given motif I.
And then what people compute and define is called network significance profile,
where basically we- we do it such that
the-the-the sum of the squares of the Z-scores, uh, equals to one.
So basically we normalize, um,
the significance profile where at the ith- ith component of the significance profile,
we take the Z-score of- of, uh,
sub-graph i and divide it by the square root of the sum of squares,
uh, of the, uh, Z-scores.
Uh, notice that Z-score, uh,
Z-score is such that if it is 0,
this means that, uh,
the motif occurs as often in the real graph as in the random graph.
And then, um, if you know the Z-score is bigger than plus or- or, uh, minus 2,
then we would say that a given motif is statistically significant,
appears statistically significantly more often or less often,
than, uh, what we- what happens,
uh, in the random graph?
So, um, and this allows us now to compare
networks of different sizes because the row counts can be quite different,
but the Z-scores are,
uh, size, uh, invariant.
So significant- significance profile is that basically for every sub-graph,
we have to count how often it appears in the real graph,
how often it appears in the random graph.
We need to do this over multiple random instantiations
so that we can then compute, uh, the Z-score.
And we need to do this for every possible sub-graph,
uh, of a given, uh, size.
And then for example,
we can take different networks,
like gene regulatory network,
neural networks of synaptic connections between neurons.
We can take the network of the world wide web,
we can take a social network, um,
or even like a network def- defined based on text,
based on word adjacency and compare, uh,
frequencies and significant prof- significance profiles, uh, between them.
And what is interesting, for example,
here is, these are the 13,
uh, sub-graphs of size,
uh, size 3 for directed graphs.
So these are now, uh,
my motifs and the y-axis here, is the Z-score.
And here are different instances of the same type of a- of a network.
For example, here are three instances of, uh,
uh, web graphs and three instances of social networks.
And you can see how basically they have the same significance profile.
You see, for example, how this triangle of,
uh, um, mutual connections is heavily overrepresented.
You notice how this particular motif, for example, here,
is very, um, is very much under-represented.
And for example, in social networks,
this makes sense because this says, you know,
imagine this is, uh, yourself or myself.
This means I have two friends to whom I have very strong relationship,
but these two friends are not friends with each other.
And actually, social science theory says that what would happen in this case is
either these two people become friends with each other and you end up with this motif 13,
or one of these edges will break because simply it is too- too hard
for you to maintain two separate relationships with two separate persons,
um, rather than, you know, bringing them together.
It's almost like saying you have to go to two coffees every
week versus all three of you going for a coffee and having good time, right?
Like this is much more hard,
uh, to maintain, uh, in practice.
And you see that in, um,
social networks, this motif is heavily underrepresented.
Um, but for example,
you can see that in other types of networks like signaling,
it's actually the feed-forward type loops here that are, uh, over-represented.
So basically you can get inside into the, um, uh,
function of networks by looking at this,
uh, motif, uh, profile.
So let me summarize.
Why did we decide is how do you detect network motifs?
You, uh, first count sub-graphs,
I in the real network.
Then you count the same sub-graph I in the random version of the real network here,
denoted as G_rand.
Uh, G_rand is a null model that has the same number of nodes,
same number of edges,
and the same degree distribution or the same degree sequence as the real network.
And then you assign or compute a Z-score for every subgraph i, where you simply say,
how often did this- this subgraph appear in the real graph minus how
often does it tend to occur in a random graph
divided by the standard deviation of the count,
um, of it in the random graph and,
uh, you know, motifs with high absolute z- scores.
This means they are either heavily over- represented
or heavily under-represented in my graph.
And that's why we say that they are, uh, significant.
So, um, and, you know,
the last thing to say in this case is there are
many variations of this notion of a motif concept.
You know, there are extensions to directed and undirected graphs.
There are extinctions to colored, uh,
nodes, so meaning nodes with different types.
There is also extensions to, uh,
temporal, uh, temporal graphs as well.
So in temporal motifs.
Um, and then there is also a lot of variations in terms of how do define frequency?
How do you define statistical significance?
How do you define under-representation?
And can you kind of count anti motifs as well.
So basically, absence of an edge is important.
And also, uh, how do you do different null models?
So there is a huge and very rich literature and
very active research area in this notion of, uh, motifs.
So to summarize, motifs and sub-graphs are building blocks of networks.
Sub-graph isomorphism and sub-graph counting are NP-hard problems.
Understanding which motifs are frequent or uh,
significant in a dataset gives us insights into the unique characteristics of the domain.
And we use random graph, uh,
null-models as basically as reference points to evaluate
significance of a given motif by computing, uh, the Z-score.

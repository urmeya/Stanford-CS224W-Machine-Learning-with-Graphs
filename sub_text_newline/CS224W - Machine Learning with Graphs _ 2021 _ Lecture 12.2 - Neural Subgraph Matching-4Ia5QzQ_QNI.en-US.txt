So now that we have defined the notion of
a network motif and a notion of a subnet network subgraph,
and I'm really kind of using these two terms interchangeably, now we need to- um,
and then we also defined what is a frequency of
a given motif or a frequency of a given subgraph in a given graph.
Now, we wanna basically develop
a neural network approach that will allow us to quickly determine whether a given,
uh, graph is a subgraph in a bigger graph.
Um, so to give you- um, the next,
we are going to talk about neural subgraph, uh, representations.
So, what's the problem?
The problem is called subgraph matching.
I'm given a large target graph,
and I'm given a small, uh, query graph.
And what I have to decide whether a query is a subgraph into- in the target graph.
Right? So for example,
here's my query, here's my target graph.
In this case, the answer should be yes because this particular set of
four nodes can map to this particular set of four nodes,
and edges are preserved.
Here, I use different colors to deter- to denote,
you know, that this node can map to that node.
You know, this node maps to this node and so on and so forth.
So query Q, is included in the target, uh, graph.
Um, so that's the, uh,
that's the first- uh,
that's the problem you want to do.
We just wanna say yes, no.
Right? It's not about the co- the number yet.
You just wanna say is query in- included?
Is it a subgraph of the target graph?
Um, how are we going to do this?
Rather than doing this as a kind of
combinatorial matching and trying to check edge by edge if,
uh, query is in- included in the target,
we're going to develop a machine learning approach to it.
So basically we're going to formulate this as a prediction task.
And our intuition will be that we're going to exploit
the sh- geometric shape of the embedding space to capture the subgraph relationship.
Let me, uh, now kind of, uh,
er, unpack what do I mean, er, by that.
So the task is that I wanna do a binary prediction to return
true if query is isomorphic to a subgraph of the target graph,
and otherwise I return false.
So again, in this case,
I give you the query, the target,
and the dashed, uh,
edges repres- lines, represent the node correspondences.
So in this case, just note,
we won't be interested in actually finding
correspondences because that's another challenging problem.
What- uh, and we're not going to talk about it.
All we want is just true, false.
Right? Given query, given target,
return true if query appears as a subgraph of the target, otherwise, return false.
We're just interested in the decision problem in the binary task.
So how are we going to approach this is, er, the following.
This is kind of the high level overview of the approach.
We'll take a big input graph, the target graph,
and we're going to decompose it in a set of- uh,
in- in a set of neighborhoods.
And these neighborhoods will be,
let's say, relatively small.
Um, so we are going to decompose it into a set of neighborhoods.
And then we are going to use a graph neural network to embed, uh, each neighborhood.
We are going to basically apply
graph embedding to create an embedding for each neighborhood.
Then at the query time,
we'll also take the query and we are going to embed the query.
And then what we are going to do is we're going to build a predictor that will say,
given the embedding of the query and embedding of a neighborhood,
you know, predi- predict yes or no.
Yes would mean a given,
um- a given, uh,
um, er, a given query is included,
is a subgraph, um,
of a given, uh, uh, er,
neighborhood versus, uh, uh,
that given query not being a subgraph of a given neighborhood.
So for example, for this query and this neighborhood,
we should predict, uh, no.
For this neighborhood in the query we predict yes.
Here, we also predict yes because it's an, uh,
triangle and an edge,
a triangle and an edge and, you know,
our query is a triangle and an edge, right?
So basically, based on the embeddings,
we wanna make these predictions.
And this can be super fast because as I'm giving a query,
I just kind of run through these neighborhoods and make predictions.
Yes, no, yes, no, yes, no.
That's essentially the idea.
How are we going to do this?
Let's give a bit more detail.
We're going to work with node-anchored definitions, right?
Meaning we are going to have a notion of an anchor and we are trying to
predict whether a given node anch- node anchored query,
um, is a subgraph of a, again,
a given anchor into the target, uh, graph.
So we are working with, uh, anchored, uh,
definitions where we- what means- what
has to do is one anchor has to map to another anchor,
and then all the edges and all the other nodes,
uh, have to map as well.
So that's the first important point,
is we have a notion of the anchor.
Um, the second, um, impor- important, uh,
notion is that we are going to decompose the target graph into a set of neighborhoods.
So in these neighborhoods will be node-anchored.
And what does this mean is essentially we'll pick
a node and then we'll take a k-hop neighborhood around it.
But let's say one hop, two hop.
Um, and then this will create a neighborhood and we are going to embed this neighborhood.
So we'll take the target graph and we are going to create many neighborhoods,
uh, and embed them.
And then for a given query,
the task will be given the query, uh,
in that neighborhood, predict whether query is a subgraph of the neighborhood.
So the entire approach is the following.
As I said, we create a query,
pick an anchored node, um,
uh, and, uh, embed it.
We take the target graph,
decompose it into many, uh,
neighborhoods and embed those neighborhoods.
And now given the embedding of the query and embedding of the neighborhood,
we wanna predict return true or false.
True if, uh, this query, uh,
is a, uh, node-anchored subgraph of the neighborhood,
and return false if, um,
these node-anchored query is not a subgraph of the node-anchored, uh, neighborhood.
So that's, uh, how,
uh, we are going to do this.
Um, so now, of course, the question is,
how do we use embeddings to make predictions?
Right? In terms of the, uh, uh,
creating the embedding of the neighborhood,
we believe- we can use our standard graph neural networks.
So kind of not too much,
um, uh, uh, importance there.
I'm kind of going to skip that detail.
There are some interesting kind of architectural details,
but just a standard graph neural network for embedding graphs,
uh, would already be good.
So now we need to decide and kind of r- I
wanna talk a bit about why do we pick anchored neighborhoods?
Why not non-anchored?
Um, you know, recall that the node level frequency definition says,
you know, the number of, uh,
nodes u in G_T for which some subgraph of G_T is
isomorphic to the query and the- and the isomorphism maps node u,
uh, in G_T to node v in the queue.
Right? So basically the anchors have to map plus all the edges,
uh, and the remaining nodes can also map.
Um, the point is that we are going to, uh,
create this anchor, the embeddings,
because we can then create basically a graph neural network,
um, for- um, around each node, uh,
u and each node v. And this way basically create embeddings of the neighborhoods.
So that's the reason why.
So we will use these embeddings to decide if neighborhood of u is
isomorphic to subgraph of neighborhood of v. And, um,
we not only predict if there exists a mapping,
but we can also, um,
in some cases be able to corres- to identify
corresponding nodes because we know that u corresponds,
uh, to v. So we'll also find the correspondence of, uh, anchors.
So how are we going to decompose G_T into neighborhoods?
Basically, for every node in the target graph,
we are going to obtain a k-hop neighborhood around the anchor.
We can simply pre- do this with- using breadth-first search.
Usually, you know, our parameter,
k, will be around,
you know, maybe three, maybe four.
So basically, we go three hops out,
we go four hops out.
And we can- and this way, we can, um,
decompose G_T, the target graphing to a lot of different neighborhoods.
And now, as we have created the neighborhood,
we simply apply our graph neural network embedding of the- of the anchor node,
um, v, to map that anchor node into the embedding space.
And then we can also do the same procedure,
uh, to the query to obtain neighborhoods in the query graph.
Um, and then we are going to embed these neighborhoods, as I said,
using a GNN by computing simply the node embedding for every anchor,
um, in its corresponding neighborhood.
Now, what is the- the cool part and actually,
the most important part of this lecture is this notion of
an order embedding space.
So, you know, what we talked so far was kind of
clear and you have perhaps- perhaps heard about it,
but you haven't heard about this topic.
This is now super cool.
So order embedding space.
Let me explain what we mean by this, right?
So, um, we map graph,
let's say A to point Z_A in a high-dimensional space,
let's say 64-dimensional embedding.
Um, and we are going to assume that the embedding space is non-negative,
so all coordinates are either 0or, uh, positive.
And then what we would like to do is we'd like to capture
partial ordering transitivity in the embedding space, right?
Then we are going to use this notation to say that,
you know, the- the left node is- is- is less than,
equal than the right node, um,
if all coordinates of the blue node- of
the left node are less or equal to the- all the coordinates of the right node.
So for example, in our- in this- in this case,
what this means is we have this transitive, uh, relation.
Because intuitively, when I say all the coordinates to be less,
it really means, um,
a given point has to be to the low- lower left of some other point, right?
So if- if this point is lower left
of that point and that same point is lower left of another point,
then also the first point is to the lower left,
uh, of the, uh, third point, right?
So here, you know, um,
this particular point is to the le- lower left,
uh, of, uh, that particular point, right?
So basically, what we want is we wanna, uh,
have these relationships of being to the lower- to the lower left,
which means in any- any part of the- of the space, right?
Basically, all the coordinates have to be less or equal,
which means you have to be embedded to the lower left of something else.
So- and this is called order embedding because
this partial ordering, this transitivity, uh,
is captured by this relation,
are you embedded lower left of something else.
So, you know, why- why- why should you care, right?
Why is lower left so cool and so important?
The point is that, uh,
lower left is so important is because it captures subgraph, uh, relations, right?
Imagine, uh, for example,
uh, the case here is that I have, uh,
the target graph, I have the neighborhood,
and I have different- different queries, right?
Then in my case, imagine that this is, uh,
the node anchored, uh,
neighborhood that I embedded here.
And I have two- two anchored queries,
Query 1 and Query 2.
And now, because basically the point is the following,
because Query 1 is a subgraph of the neighborhood,
Query 1 should be embedded to the lower left of the neighborhood,
while Query 2 is not a subgraph,
so it should not be embedded to the lower left, right?
So here, this notion of a subgraph relationship is preserved
because the Query 1 is embedded to the lower left of, uh, Query 2.
And simply by comparing the positions of the embeddings of this,
uh, anchor nodes, we can determine that, you know, uh,
Query 1 is a subgraph of, uh,
anchor node t, while, uh,
Query 2 is not a subgraph of anchor node,
uh, uh, t. So that's the cool part.
We can very quickly read from the embedding whether one is a subgraph of the other.
Um, you know, why does this work?
Why do we care about this, uh,
transitive, uh, partial ordering,
uh, in the embedding space is because subgraph isomorphism relationship,
um, can nicely be encoded in
this order embedding space where the order is defined by this relation,
are you lower left of somebody else.
And the reason is because the- the order- order relations,
so the lower left, uh,
relation is transitive and subgraph isomorphism is also transitive.
It has this property of anti-symmetry,
which is also, um,
encoded in the order embedding,
is that if G_1 is a subgraph of G_2 and G_2 is a subgraph of G_1,
then G_1 and G_2 are- are isomorphic.
They are the same. So if one point is to the lower left
of one and the other one is of the lower left of the first one,
then the points are on the same location,
so the two graphs are isomorphic.
Transitive would mean if G_1 is a subgraph of G_2,
G_2 is of G_3,
then G_1 is a subgraph of G_3 as well, which again is,
uh, encoded by the, uh,
you know, the subgraph, uh, relation.
And the last one is this notion of closure under intersection that,
uh, the trivial graph of one node is a subgraph of any node.
In our case, it would be the embedding at the coordinate origin,
at 0, 0, 0, is a subgraph of every other embedding.
It is to the lower left of any other- of any other embedding.
So basically, this order embedding space defined by this relation is one point to
the lower left of the other has
all these properties that the subgraph relation, uh, also has.
So, um, the reason now that- why we are interested in it,
as I- as I said and here I show,
uh, more- more formally, is that,
you know, the order embedding space captures transitivity, right?
In a sense that if, uh,
first point is the subgraph of the second and the second is of the third,
then first is also a subgraph of the third.
We have this notion of anti-symmetry, uh,
that if one is to the le- lower left of the other,
and the other is to the lower left of the first,
then they are- they are equivalent, they basically overlap.
Um, and then the last one is this, uh, closure, uh,
under intersection, uh, illustrated here,
uh, on the- on the right.
So basically, order embedding space defined by this lower left relation,
captures the same type of patterns,
properties that the subgraph, uh, relation has.
And that's the important part and the cool part of the order embedding space.
So now we are going to actually learn the embeddings of these,
uh, anchored neighborhoods such that the subgraph relation is preserved, right?
So we are going to use a GNN to learn the embeddings of neighborhoods,
basically to learn the embedding of the anchor node,
to preserve this order embedding structure,
to preserve the subgraph structure.
So the question is,
what kind of loss function should we use so that the learned,
uh, embedding operator reflects the subgraph relationship?
Um, and we are going to design a loss function based on what we call order constraint.
An order constraint specifies the ideal order of the embedding,
um, lower left property that reflects
a subgraph, uh, relation.
So this specify this, what we call,
order constraint to ensure that
subgraph properties are preserved in the embedding space, right?
So basically, what this means, uh,
here is- it's written in mathematics,
but basically it says that if a query is a subgraph of the target,
then every coordin- every embedding coordinate of the query should be less than the,
uh, embedding, uh- every coor- embedding coordinate of the target.
Right. So if Q is a subgraph of T,
then the embedding of the anchor node, uh,
in T should be to the, um, to the,
uh, greater and equal than the embedding of the, uh, query q.
So the relationship is- is this, right?
This is the query, that's the target,
so the anchor node, uh,
from the query should be embedded to
the lower left of the anchor node of the target because,
uh, que- this is, uh,
anchored subgraph, uh, of the target.
So that's basically what we mean by order constraint,
is that you have to be to the lower left.
Now, um, GNN embeddings are learned using,
uh- by minimizing what is called a max-margin loss.
So basically what we are going to do is to define
this notion of a loss where we are saying,
okay: so how much, um, is this,
uh, constraint, um, violated?
So basically here we say this is the maximum of zero and the,
uh, and the difference in the coordinates, right?
So if coordinate Z_t is always larger than the coordinate of q,
then this difference will be negative,
so maximum of negative in 0 is 0,
so the violation will be 0.
But if the subgraph relation is not preserved,
which means that along the given coordinate q is to the right or to the top of Z,
this means Z_q is greater than Z_t,
then this difference will be positive and then maximum
of 0 and a positive number will be a positive number,
so this E is the margin.
It will be basically, um,
the amount of violation of
the order constraint between a given query and a given, uh, target, right?
So here there is no violation,
while in this case there is violation because q is a subgraph of t,
but q is not embedded to the lower left, uh,
of t. So, um, eh,
according- al- along the first dimension,
this difference will be positive,
so the entire maximum,
uh, will be positive, uh, as well.
So this is now how we've arrived,
and what is important here is that now, um,
this, uh, loss, this, uh, penalty, uh,
E is differentiable, so we'll be able to, uh,
back-propagate this penalty into the graph neural network, uh, architecture.
So the embeddings are learned by,
uh, minimizing this max-margin loss,
so we have this E that determines the- the amount of, uh,
order constraint violation between a given graph, uh,
and a target, and we call this,
uh, penalty, this violation the margin.
So we wanna learn the correct order embeddings so- so that, uh,
the- the penalty is 0 when G_q is a subgraph of, uh,
G_t, and the penalty is greater than 0 when G_q is not a subgraph of G_t, right?
So we want penalty of 0 when, uh,
one is a subgraph,
so it has to be embedded to the lower left,
and then the penalty will be zero,
and if it's another subgraph,
then we want this penalty to be high because G_q
should not be embedded to the lower left of, uh, G_t, right?
So to learn this,
um, uh- this, uh,
embedding function, we need to construct training examples, right?
Of G_q and G_t, where, uh,
you know half of the time G_q will be a subgraph,
and the other time- half of the time it won't be.
Um, and then right when we are going to do the training of the embedding neural network,
we are going to, uh,
make it such that for positive examples we wanna minimize the penalty,
and for negative examples we wanna,
uh, maximize the penalty.
So here is how- how you can write this all out, uh,
by another kind of, um, uh,
huge- huge loss, uh,
type expression where we again say,
um, if, uh- if,
uh- if it will be, uh, um,
for positive examples, I want this to be zero.
So, um, you know,
I'll- I'll get some Alpha and for negative, uh,
examples, uh, uh, I- this will be greater than 0,
so this- I'll- this,
uh- this expression will be smaller.
So I'll wanna be able to uh,
uh, minimize this, uh, maximum.
So, um, now how do I generate training examples?
Is by simply picking an anchor node,
and then doing a breadth first, kind of,
a probabilistic breadth-first search around it,
and this means that I'll have the- I'll generate
the query that will be a subgraph of a given- of a given neighborhood.
And then to generate a negative example,
I can, you know, corrupt the query by,
you know, perhaps removing a node,
ending an edge, removing an edge,
uh, things like that, right?
So, uh- so that it is no longer, uh, a subgraph.
So, you know, how many of these training examples do I
choose so that I can then train my embedding neural network?
The idea is, as I train this, I wanna,
uh, at every iteration sample new training pairs.
Uh, the benefit is that at every iteration the model,
uh, will see, uh, different,
uh, subgraph examples, and it will improve performance,
and it will avoid, uh, overfitting.
How deep do I wanna make
my Breath-First Search training example sampling a neighborhood sampling?
Um, it's, kind of, a trade-off between the runtime and performance.
The deeper- the deeper I go,
the longer the runtime, but usually,
uh, the more, the better embeddings I get.
So usually we would use the- the depth between, uh,
3 and 5, also depending a bit,
uh, on the data set.
So, um, you know,
how do I now apply this when a new query arrives?
When a new query arrives,
I- I- the query has an anchor.
I simply embed the- the anchor,
um, and then, uh,
the procedure is that basically for every other,
uh, target, um, anchor at the
neighborhood of the target graph, I,
um- I compare the embedding of the,
um, anchor neighborhood with my,
uh, anchored query embedding.
And if the- if the query is embedded to the lower left of that, um, neighborhood,
then I say that, uh,
query is a subgraph of the neighborhood,
and otherwise I would say, uh,
that it's, uh, not a subgraph, right?
So basically, um, I can quickly check this by simply asking,
is one embedding to the lower left,
uh, of the other embedding?
So let me summarize, uh, this part.
So um, we talked about neural subgraph matching, which is, uh,
a way to formulate subgraph matching as a machine learning problem, uh,
and this way sidestep the NP-hard problem of sub- subgraph isomorphism.
Uh, basically given a query and given a ta- target graph,
we embed the query,
we embed the node anchored neighborhoods of the target graph,
and we train our neural network embedding function such that, um,
it embeds subgraphs in,
uh- to be in such a way that they are located to the lower left of each other, right?
If q is a subgraph of t,
then q is embedded to the lower left, uh,
of t. So this is- and at the training time of this embedding neural network,
we force it to obey this subgraph relation,
and this means we can then very easily and quickly find
out whether a query is a subgraph of a given, uh,
target neighborhood t. So basically embedding graphs with this order embedding
property or order embedding space allows us to test subgraph isomorphism very,
very quickly by simply just comparing the coordinates.
Basically saying, is the query embedded to the lower left,
uh, of the target?
And given the properties of the subgraph isomorphism,
uh, operator or re- relation,
we see that we can perfectly encode it into the order embedding space,
which means that actually all this is possible, um,
and we can, uh- we can do it,
and it, uh, works well in practice.

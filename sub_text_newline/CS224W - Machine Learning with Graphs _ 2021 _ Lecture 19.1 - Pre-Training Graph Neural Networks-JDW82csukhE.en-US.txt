Thank you for the opportunity and I'm excited to talk about
my work on pre-training graph neural networks.
Okay, so, um, here I, uh,
I want to talk about applying
graph neural networks to application in scientific domains.
Uh, for example, uh, in chemistry,
we have this molecular graphs where each node is
an atom and each edge represents the chemical bond.
And here we are interested in predicting the properties of molecules.
For example, given this kind of molecular graph,
we want to predict uh,
whether it's toxic or not.
In biology, there's also a graph problem.
For example, uh, we have a
protein-protein association graph where each node is a protein and the edge represents
certain association between proteins and the associations are
very important to determine the functionality of the protein.
And our goal is to given this, uh,
subgraph centered around the protein node,
we want to predict whether
the center protein node has certain biological activity or not.
So what I want to say is that in bio- in many scientific domains, this, uh,
graph problem appears a lot and we want to apply GNNs to solve this problem.
And just to review how GNNs can be used for graph classification.
So GNNs obtain an embedding of the entire graph by following two steps.
First, um, given this,
the molecular graph, let's say graph, let's say molecule,
we obtain the embedding of node by either the aggregate neighboring information.
Once we do that,
we- once we obtain this node embedding,
we can pool this node embedding globally to obtain the embedding of the entire graph.
And once you obtain this embedding of the entire graph, we can use it,
or we can apply a linear function on top of it to predict whether this entire graph is,
let's say, toxic or not.
And just to, [NOISE] uh, say what this means is that, uh,
here the node embedding is trying to capture
the local neighborhood structure, uh, around each node.
If we apply k-hop GNNs,
we will just know that it should capture k-hop local neighborhood.
And then the embedding of the entire graph can, uh,
is basically aggregating this local structure though,
the node- node embedding that is capturing the local neighborhood structure.
And, uh, and, uh,
this is how GNN works,
and now I am going to talk about challenges of how- applying machine learning to,
uh, the scientific domains.
And there are basically two fundamental challenges.
The first challenge of applying machine learning to
the scientific domains is that all the scarcity of labeled data.
Obtaining labels in these scientific domains requires expensive lab experiments.
For example, to, or say whether a molecule is toxic or not,
you need to perform wet lab experiments,
and these are very expensive.
As a result, we cannot get that,
a lot of training data and machinery models tend to overfit to this small training data.
Another important issue is that out-of-distribution prediction, meaning that,
test examples we want to make a prediction on
tend to be very different from training examples.
And this is really the nature of scientific discovery because in scientific discovery,
we want- we want to discover some new molecule that is
inherently very different from- from your training molecules.
And in these domains,
machine learning models are extrapolate poorly.
And especially these two challenges are becoming more challenging for deep learning.
The first, uh, the first point of the label scarcity,
deep learning models have a lot of parameters to train,
uh, typically in the order of millions.
And uh- and in this regime,
the number of training data is much less than the number of parameters.
And deep-learning models are extremely prone to
overfitting on small data- small labeled data.
Another issue is that deep learning models are known to extrapolate poorly.
And that's reported that models often make, uh,
predictions based on spurious correlation in a dataset without
understanding the true causal mechanism of how to make prediction.
So let's consider this a toy example of
a- a image classification between polar bear and brown bear,
uh, this image is shown here.
So during training, let's say, uh,
our training data for- in our training data,
most polar bears have snow background like this,
and most brown bears have a grass background like this.
And as a result, the model can learn to predict,
make prediction of whether it's, uh,
the given image is polar bear or brown bear based on the image background rather than
animal itself because that's sufficient
to make predictions over the- over the training dataset.
But what if at the time same,
if we see polar bear on the grass,
then the model will, uh,
because the model is not understanding the- this prediction task,
the model will perform poorly on the- on
the data or test data that is different from training data.
And the model is, uh, just capturing this kind of
spurious, spurious correlation in the training dataset.
And our key idea or the goal, uh,
given these challenges is that we want to improve
model's out-of-distribution performance even with limited data.
And the way we do this is to inject domain knowledge
into a model before we apply them on scarcely-labeled tasks.
And that, this work- this may work because
the model already knows the domain knowledge before the model is
training on- on our downstream data so that the model can generalize
well without many task-specific labeled data and uh,
and uh, the model may be able to extract essential,
non-spurious, uh, pattern in the data, uh,
which allows the model to better extrapolate to the-
to the test data that still- that still very different from training data.
And a very effective solution to inject
domain knowledge into model is called pre-training.
And we pre-train a model on relevant tasks that's different
from a downstream task where data is abundant.
And after we pre-train the model,
the model's parameter, uh,
already contains some domain knowledge and once this is done,
we can transfer this pre-trained model parameter to the downstream task,
which is what we care about and which where we have small number of data.
And we can start from the pre-trained parameter and fine-tune the-
the parameters on the downstream tasks.
Um, and just to mention that pre-training has been hugely successful in- in the domain of
computer vision and natural language processing and it's
reported that pre-training improves label efficiency.
Also, pre-training improves out-of-distribution performance.
And because of this and within
pre-training can be a very powerful solution to the scientific applications.
And those two challenges are scarce labels and out-of-distribution prediction.
So now we motivated
this pre-training GNNs to solve an important problem in scientific applications.
So let's consider, you know, actually pre-trained GNNs.
And our work is about designing
GNN's pre-training strategies and we want to
systematically embedded- investigate the following two questions.
How effective is pre-training GNNs,
and what is the effective pre-training strategies.
So as a running example,
let's think about molecular property prediction task,
prediction for drug discovery.
For a given molecule, we want to predict its toxicity or biological activity.
And the very naive strategy is multi-task supervised pre-training on relevant labels.
This means that in the, for example,
chemical database we have, uh, all the,
uh, experimental measurements of
tox- various toxicity and biological activities of a lot of molecules.
And we can first pre-train GNNs to predict those,
uh, very diverse biological activity of toxicity.
And then we expect GNNs parameter to capture
some chemistry domain knowledge which can be tran-
and then we can transfer that parameter to our downstream task.
And the setting that we consider is, uh, to,
to study whether this naive strategy is effective,
we consider this setting.
We consider this binary classification of molecules.
Given molecule, we want to judge whether it's negative or positive.
And- and we, for the supervised pre-training part, uh,
we consider- we consider predicting
more than 1000 diverse binary bioassays annotated over 450,000 molecules.
So there are a lot of data,
uh, in this pre-training stage.
And then we apply transfer the parameter to or downstream task,
which is eight molecular classification datasets,
those are relatively small, uh,
about 1,000 to 100,000 molecules.
And- and for the data split,
uh, for the downstream task,
we consider the scaffold split,
which makes the test molecules out of distribution.
And it turns out that the Naive strategy of
this multi-task supervised pre-training on relevant labels doesn't work very well.
It gives a limited performance improvement on downstream tasks and even, uh,
leads to negative transfer,
which means that the pre-trained model performs worse than randomly initialized model.
So here's the, uh, table,
and here is a figure.
So we have our eight, uh,
downstream datasets and -and
the y-axis is the ROCAUC improvement over no pre-training baseline.
So this purple is a no pre-train baseline.
And we see that Naive strategy sometimes work well,
but for these two datasets,
it's actually performing worse than non pre-trained randomly initialized baseline.
So this is kind of not desirable,
we want pre-trained model to perform better.
So how- what is the- then we move on to our next question,
what is the effective pre-training strategy?
And our key idea is to pre-train both node and graph
embeddings so that GNNs can capture domain-specific knowledge about the graph,
uh, at the both, uh,
local and global level.
So- so in the naive strategy,
we pre-train GNNs, uh,
on the- at the level of graph, use this, uh,
graph embedding to make a various prediction.
But we think, uh,
it's important to also pre-training, uh, this, uh,
node embedding because these node embeddings are
aggregated to generate the embedding of the entire graph.
So we- we need to have a high-quality node embedding
that captures  the local neighborhood structure well.
So and the- the intuition behind this is that- so here's the figure,
the- the upper side means the node embedding
and these node embedding are proved to generate the embedding of the graph,
and we want the, uh,
the quality of the node embedding and graph embedding to be both high quality,
capture the semantics in the- in the- this is,
uh, different from the naive pre-training strategy where, for example,
if we- on the node-level pre-training,
we capture the semantic at the level of nodes,
but if those node embeddings they are globally aggregated,
we may not be able to obtain nice, uh,
graph embedding, um, or if we only pre-training,
uh, GNNs at the level of graph,
we can obtain nice graph embedding,
but the- there's no guarantee that the- before aggregation,
those node embeddings are of high-quality,
and this may lead to a negative transfer,
um, because the node embeddings are kind of not robust.
So- so in order to realize this, uh,
strategy to pre-train GNNs at the level of both nodes and graphs,
we- we designed three concrete methods.
Uh, two self-supervised method, uh,
meaning there is no need for external labels for node-level pre-training,
and one graph-level of prediction of pre-training strategy,
pre-training method, um, that's,
uh, that's, uh, supervised.
So let me go through these three concrete methods.
[NOISE] Okay.
So the first method is the attribute masking.
It's a node-level pre-training method.
Um, the algorithm is quite simple.
So given that input graph, we first mask,
uh, uh, rand- randomly mask our node attributes.
So let's say we mask these two node attributes.
In the molecular graphs, this node that we are
basically masking the identity of the atom.
Um, and then we can use GNNs to generate, uh,
node embeddings of these two mask nodes,
and we use these node embeddings to predict the identity of the mask attributes.
So here we want to use the node embedding to predict the- that
this X is the- is the oxygen and this X is the carbon.
And the intuition behind this is that
the- through solving the mask attribute prediction task,
our GNN is forced to learn
the domain knowledge because it kind of needs to solve this quiz, and, uh, and, uh,
and- and through this,
solving this kind of quiz, GNN, uh,
can learn the- the parameter of the GNN can capture the- the chemistry domain.
The next self-supervised task that we propose is called the context prediction.
And, uh, and here the idea- the algorithm is as follows.
So for each graph,
uh, we sample one center node.
Let's say this is a,
um, um, red nodes,
and we, for this, uh,
center node, we extract neighborhood and context graph.
So neighborhood graph is just the k-hop neighborhood graph, uh, in this case,
two-hop neighbor graph like this,
and then the context graph is the- is
the surrounding graph that's directly attached to this,
uh, k-hop neighborhood graph.
Once we do extract this graph,
we can use two separate GNNs to encode this neighborhood graph into, you know, uh,
one- use one GNN to encode this neighborhood graph into a vector,
another GNN is to encode this context graph into a vector.
And our objective is to maximize, uh,
or maximize the inner product between the true neighborhood context pair
while minimizing the inner product between the false or negative,
uh, uh, false on neighborhood context pair,
and these false pairs can be obtained by, uh,
randomly sample context graph from other neighborhoods.
And here the intuition is that we're using- we are assuming that
subgraphs that are surrounded by similar contexts are semantically similar.
So we are basically pushing, uh,
the embedding of the neighborhood to be
similar if they are- have similar context graphs.
And this is the kind of widely used, uh,
assumption in natural language processing,
it's called distributional hypothesis.
So words that are appearing with similar- similar contexts have similar meaning,
and it's exploited in the famous, uh, Word2vec model.
Um, so finally, um,
I'm going to talk about this, uh,
graph-level pre-training task, which is essentially what I introduced before.
So multi-task supervised pre-training on many relevant graph level- labels.
And this is really the direct approach, uh,
to inject, uh, domain knowledge into the model.
[NOISE] So to summarize the overall strategy,
we perform- we first perform node-level pre-training,
uh, to- to obtain good node representation.
We- we use that, uh,
pre-training parameter, uh, to- and then, uh,
to- to further perform pre-training, uh,
at the level of graph using supervised graph-level pre-training.
Once- once we've done,
uh, this, both steps,
we fine-tune, uh, the parameters on the downstream task that we care about.
So this is the overall strategy.
And it turns out that our strategy works out pretty well.
Uh, as you can see, these, uh,
green dots are our strategy,
and our strategy first avoids the negative transfer in these, uh, two datasets,
and also consistently perform better than- than- than this,
uh, orange naive strategy baseline across the datasets.
Uh, and also, um,
another interesting side note is that, uh,
we fix the pre-training strategy and the pre-training different GNNs models,
uh, use different GNNs models for pre-training.
And what we found out is that the most expressive GNN model,
namely GIN, that we've learned in the lecture,
um, um, benefits most from pre-training.
Uh, as you can see here,
the gain of pre-trained model versus non-pre-trained model is the- is the largest,
um, in terms of accuracy.
And- and the intuition here is that
the expressive GNN model can learn to
capture more domain knowledge than less expressive model,
especially learned from large amounts of,
uh, data during pre-training.
So to summarize of our GNNs, we've, uh,
said- learned that the GNNs have important application
in scientific domains like molecular property prediction or,
um, protein function prediction,
but, uh, those application domains present the challenges
of label scarcity and out-of-distribution prediction,
and we argue that pre-training is a promising,
uh, framework to tackle both of the challenges.
However, we found that naive pre-training strategy of this, uh,
supervised graph level pre-training gives
sub-optimal performance and even leads to negative transfer.
And our strategy is to, um,
effective strategy is to pre-train both node and graph embeddings,
and we found this strategy leads to
a significant performance gain on diverse downstream tasks.
Um, yeah, thank you for listening.

Next, I wanna talk about how do we train,
uh, graph neural networks, right?
So far we talked about how do we augment
the feature vector of the node and how can we augment the- the graph structure.
And we talked about how to augment the graph structure by adding edges to improve
message-passing or how do we drop edges to increase the efficiency,
especially in natural graph social networks where you have high degree nodes,
you don't wanna aggregate from the entire neighborhood of the node,
but you wanna kind of carefully sub-select,
uh, the part of the network to aggregate from.
So, uh, this is the reason,
uh, why you wanna do these augmentations.
Now I wanna talk more about how do you do the training?
How do you deal with the outputs?
How do you define, uh,
the loss functions, measure performance, and so on?
So the next talk is,
um, how do we train a GNN, right?
Like, what kind of learning objective do we wanna define and,
um, how are we going to, uh,
do, uh, all these, uh, together?
So GNN training pipeline has the following, uh, steps, right?
So far we talked about the input graph,
we talked about how to define the graph neural network,
and we talked about how the graph neural network produces node embeddings.
What we haven't talked about yet is
how do you get from node embeddings to the actual prediction?
And then once you have the predictions,
how do you evaluate them based against some ground truth labels?
And how do you compute the loss,
or how do you define the losses,
the discrepancy between the predictions and the true labels?
Right? Um, so far we only said, aha,
GNN produces a set of node embeddings, right?
Which means that this is a- a representation of node L at the final layer,
layer L, uh, of the graph neural network.
And I can just think of this as, you know,
some representations, some vectors attached to the nodes of the network.
Now, the question is, uh, you know,
how is this second part defined?
What do we do here in terms of prediction heads, evaluation matrix,
where do the labels come from,
and what is the loss function we are going to optimize?
So let's first talk about the prediction head, right?
Uh, prediction head, this means the output of the g- of the- of the final model,
um, can have di- we can have different prediction heads.
We can have node-level prediction heads.
We can have, er, link-level,
edge-level, as well as entire graph-level prediction heads.
So let me talk about,
uh, this, uh, first.
So, er, for prediction head,
the idea is that different tasks require different types of,
uh, prediction outputs, right?
As I said, we can have our entire graph-level,
individual node-level, or, um,
edge-level, which is a pairwise between a pair of nodes.
So for the node-level of prediction,
we can directly make prediction using node embeddings.
So basically after a graph neural network computation,
we have a d-dimensional node embedding,
uh, for every node in the network.
Um, and if, for example,
we wanna make a k-way prediction,
which would be basically a classification of nodes
among k different classes or k different categories,
um, this would be one way.
Or perhaps we wanna regress,
uh, against 10, uh, sorry,
k different targets, k different,
uh, characteristics of that node.
Uh, the idea would be quite, uh, simple.
We just say, um, you know,
the output, er, head of, uh,
for a given node is simply some, er,
matrix time the- times the em- final embedding of that node, right?
So this basically means that W will- will map node embeddings, uh,
from this embedding space to the- to the prediction,
uh, to the prediction space.
To the, in this scalar- case, let's say, uh,
k-dimensional output because we are interested in,
uh, k-dimensional, um, uh,
prediction, so a k-way, uh, prediction.
Um, in or- one more thing I will add for the rest of the lecture,
I'm going to use this hat symbol to denote
the predicted value versus the ground truth value, right?
So whenever I use a hat,
this means this is a value predicted by the model,
and then I can go and compare y hat with y,
where y is the true- true label and y hat is the predicted, uh, label, right?
And now that I have y-hat,
I can compare it to, uh,
y and I can compute, uh, the loss,
the discrepancy between the prediction, uh, and the truth.
This is for node-level tasks.
For edge-level tasks, we have to make a prediction using pairs of node embeddings, right?
So, again, suppose we wanna make a k-way prediction,
then what we need is a- is a prediction head that takes
the embedding of one node and the other node and returns, uh, y hat.
Now, y hat is, uh,
defined on, uh, pairs of nodes.
This will be, for example, for, uh, link prediction.
So let me tell you what are some options for creating this,
uh, um, edge-level, uh,
head, uh, for prediction.
So one option is that we simply concatenate, uh,
embeddings of nodes u and v and then apply a linear,
uh, layer, a linear transformation, right?
And, ah, we have seen this, er,
this idea already in graph attention, right?
We said when we computed the attention between nodes u and v,
we simply concatenated the embeddings,
passed them through a linear layer,
um, and that gave us, uh,
the prediction of the attention score between,
uh, a pair of nodes.
Here, we can use the same,
the same idea, where basically we can take the embeddings of u and v,
concatenate them, basically just join them together,
and then apply a linear predictor on top of this.
So basically multiply this with the matrix and
perhaps send through a non-linearity or anything like that,
like a sigmoid or a softmax if we like, right?
So, uh, idea would be that,
um, the prediction is simply,
um, you know, it's a linear function that then takes the, um, h 1, er,
h of u and h of v, concatenates them,
um, and up- and maps this, uh, er,
to the- 2D dimensional embedding into a k-way,
uh, prediction or a k-dimensional output.
Another idea, uh, rather than concatenating is also we can do,
uh, a dot product, right?
So we basically say,
our prediction between u and v is simply a dot product between their embeddings.
If I simply do the dot product between the embeddings,
then I get a single scalar output.
So this would be a one-way prediction,
like link classification or link prediction.
Is thi- is that our link or not, right?
So basically just that a, um,
one variable kind of binary, uh, classification.
Now, if I wanna have a k-way prediction, if I wanna, for example,
predict the type of the link and I have multiple types,
er, then I would basically have this kind of, uh,
uh, al- almost similar to this kind of multi-hat prediction,
where basically I can have a different, uh, um,
uh, matrix, uh, W that is trainable,
um, and I have one for every output class, right?
So for every, uh, output, er, class,
I would have a different, uh,
matrix W that essentially,
the way you can think of it is it takes, er,
let's say the vector u and then it transforms it by shrinking or extending,
rotating, and translating it,
and then, uh, multiplying that- that with, ah, h of, uh,
v. So it's still a dot product,
but the input vector gets transformed, right?
And, um, every- every class gets to learn its own transformation,
how to basically, uh,
rotate, uh, translate, um,
and- and shrink or expand the vector so that the dot product,
um, is, uh, is, uh,
such that the predict- that the value,
the output values, uh,
are well, um, are well predicted.
And then, right, once I have a prediction for every of the classes,
I can simply concatenate them,
and that's my final prediction, right?
So for k-way prediction in binary, just to summarize,
I can define this matrix W, one per output class,
and then learn this type of, uh, uh,
linear, uh, predictor based on a dot product.
And then, er, the last thing to discuss is,
how do we do, er, graph-level, er, prediction, right?
Here, we wanna predict using all the node embeddings in our graph, right?
And, again, let's suppose we wanna make a k-way prediction.
So what we want is we wanna have these, uh, uh,
prediction head that, uh, makes one prediction on the entire graph.
So what this means is we have to take the individual node embeddings, right,
for every node and somehow aggregate them to- to
find the embedding of the graph so that we can then make a prediction, right?
So in this sense, this, uh,
head for graph, uh,
prediction- graph-level prediction is
similar to the aggregation function in a GNN layer, right?
We need to aggregate all these embeddings of nodes to create
a graph-level embedding and then make a graph-level, uh, prediction.
So let me tell you how you can define this,
uh, graph, uh, prediction head.
There are many options,
uh, for us to do this.
[BACKGROUND] So one needs to do global mean pooling, right?
So basically you take the embeddings of all the nodes and you average them.
That would be one possibility.
Another possibility is max pooling,
where you would take- take
coordinate-wise maximum across the embeddings of all the nodes.
Um, and then another option is that you do summation-based pooling,
where you basically just sum up the embeddings of all the nodes, uh, in the graph.
Um, and this will,
depending on the application and depending on the graph- graphs you are working with,
uh, different, um, options are going to work the, uh, uh, better.
You know, mean pooling is interesting because
the number of nodes does not really play the role.
So if you- if you wanna compare graphs that have very different,
uh, sizes, then perhaps mean pooling is the best option,
but if you really wanna also understand how many nodes are
there in the graph and what is the structure of the graph,
then sum-based pooling, uh, is a better option.
Um, of course, there are also more advanced,
uh, graph, uh, pooling, uh, strategies.
And I'm just going to give you next an idea,
uh, how, uh, how you can improve this.
Um, the reason why we may wanna improve this is that the issue is that
global pooling over a large graph will use a lot of information.
Um, and I wanna illustrate what I mean by this is by- with this simple toy example,
where you can think that we have nodes that have only one dimensional embeddings, right?
So the embeddings are just a single number.
And imagine I have two graphs.
In one case, you know,
I have the values like minus,
uh, node 1 has embedding minus 1,
node 2 has embedding minus 2, node 3, 0,
you know, 4 has embedding 1,
and 5 has embedding, uh, 2.
And perhaps I have a different graph,
um, where embeddings are very different, right?
Like minus 10, minus 20,
0, 10 and 20, right?
Then I can say look clearly G_1 and G_2 have very different node embeddings.
Their structures could be very, very different.
But if I do any kind of global sum based pooling, for example,
if I sum or if I take the average,
then for both of these, um,
uh, I will get the same value.
So it means that from the graph embedding point of view,
these two no- these two graphs will have the same embedding value.
So they'll have the same representation.
So we cannot differentiate,
we cannot separate them out.
We cannot classify them into
two different classes because they have the same representation.
They both have the, uh, the representation, uh, of 0.
So this is one issue,
kind of, uh, uh,
a very simple, uh,
kind of edge case example, um,
why- why global pooling, uh,
many times can lead to unsatisfactory results,
especially if their graphs are, uh, larger.
A solution to this is to do a hierarchical pooling.
And hierarchical pooling would mean that I don't
aggregate everything together at the same time,
but I'm aggregating smaller groups and then,
you know, I take a few nodes, aggregate them.
I take another subset of nodes, aggregate them.
Now I have two aggregations.
I further aggregate these,
and this way I can hierarchically,
uh, aggregate things, uh,
subsets of nodes together.
So let me give you a- a toy example and then I'll tell you about how one can do this.
Um, so imagine I will been going to aggregate using, uh, a
rectified linear unit as a non, uh,
as a nonlinearity and a summation as the aggregation function, right?
And imagine that I decide to aggregate
hierarchically in a sense that I first aggregate first two nodes,
then I aggregate that the last three nodes,
and then I aggregate the aggregates, right?
So , uh, for, uh, graph 1,
how will this look like is,
I first aggregate minus 1 and minus 2,
um, and then pass it through ReLU,
I get a 0, then I aggregate the last three nodes.
Uh, here's the aggregation.
I get the value of 3.
Now I aggregate 0 and the 3 together,
and I obtain a 3.
So this means the embedding of this graph G_1 is 3,
because we worked with single-dimensional embeddings right?
Now for G_2, here is my,
uh, here's my graph, right?
So again, if I do the, uh,
I- if I do the first two nodes,
the ReLU will be 0.
If I do the second,
uh, the last three nodes, the,
uh, the ReLU output of this aggregation will be 30.
If I now further aggregate this using the same aggregation, uh,
function, so I aggregate zero 0 and a 30,
um, I will get, uh, a 30.
So now the two graphs have very different embeddings.
One has an embedding of 3, the other one of 30.
So we are able now to differentiate, right?
We are to distinguish them because they have different embeddings.
They- they do not overlap in the embedding space.
So that's an idea or an illustration how hierarchical pooling, uh, may help.
So now, of course the question is, uh,
how do I decide who tells me what to aggregate first and how to hierarchically aggregate?
And the insight that allows you to do this really well in graphs is that
graphs tend to have what is called community structure, right?
If you think of social networks,
there are tightly knit communities inside social networks.
So the idea would be that if I can detect these communities ahead of time,
then I can aggregate nodes inside communities into,
let's say community embeddings,
and then I can further aggregate community embeddings into
super community embeddings and so on and so forth hierarchically.
This will be one strategy,
would basically be to apply what is called a community detection or
a graph partitioning algorithm to split the graph into different, uh,
clusters, denoted here by these different, uh, colors,
and then aggregate inside each of the cluster,
each of the communities,
and then keep to- to create basically for each community a supernode.
This is now an aggregate that embedding of all the members of the community.
And then I could again look how communities link to each other,
uh, aggregate based on that,
get another supernode and keep aggregating until I get,
uh, to the prediction head, right?
And, uh, one option,
as I said, to do this,
would be to simply apply a graph partitioning, graph clustering community detection,
uh, algorithm to identify what are the clusters in the graph,
what are these densely connected groups?
And then you would, you know,
do this in the level of the original network.
Then you will do this again at level 1,
you do this at level 2,
until you can have a single supernode,
which you then can input into a prediction head.
Uh, what is interesting is that you can do this
actually in a way so that you learn how to partition the network, right?
You don't need to download some external software and make this assumption that,
you know, communities are important.
What you can do, and there is our paper linked up here called DiffPool,
because this is kind of differential pooling operator that
allows you to learn how to aggregate nodes in the network.
And the simple idea how to do this is to have two independent graph neural networks,
uh, at each, uh, level here.
And one graph neural network is going to compute node embeddings.
This is standard, what we have talked so far.
But what is clever is that we will also have
a second graph neural network that will compute the clusters that nodes belong to.
So what I mean by this is it will determine
which nodes should belong to the same, uh, cluster.
Which nodes should be aggregated, er, together,
which embeddings should be aggregated together to create this, uh, supernode.
And the cool thing is that you can train GNNs A and B at each level together in parallel.
So this means that you can supervise how to
cluster the network and how to aggregate the, uh,
the network infor- the node embedding information to-
to come up with the optimal way to embed,
uh, the underlying network.
So this is kind of the most advanced way how you can-
how you can learn to hierarchically pool, uh,
the network in order- in order to make a
good faithful embedding, uh,
of the entire network.
So, uh, this is what I wanted to,
uh, uh, show in this case.
Now that we have talked about prediction heads,
let's talk about actually the predictions,
uh, and the labels.
So the second, uh, part I wanna talk about, uh,
is this part here,
which wi- which is about,
uh, predictions and labels.
So, um, we can broadly distinguish between supervised and unsupervised, uh, learning.
Supervised learning on graphs would be where,
uh, labels come from some external sources.
Perhaps, for example, nodes have belonged to different classes.
Users in social network, uh,
you know, uh, are interested in different topics.
Uh, if you have graphs, molecules,
perhaps every molecule, you know,
we know whether it's toxic or not- not.
Or we know how is i- its drug likeness,
which is something that chemists,
uh, worry about, right?
This is supervised, basically l- supervision labels come from the outside.
And then there is also the notion of unsupervised learning on graphs where the signal,
the supervision comes from the graph itself.
An example of this would be,
for- for example, uh,
link prediction task, right?
Where we want to predict whether a pair of nodes is connected.
Here, we don't need any external information.
All we need is just, um,
pairs of nodes that are connected and pairs of nodes that are not connected.
Um, and sometimes the difference between supervised
and unsupervised is blurry because both you
can formulate as optimization tasks and in both you kind of still have supervision.
Uh, just in some cases supervision is external and
sometimes supervision is, uh, internal, right?
So, um, you know, uh,
for example, if you train a GNN, uh,
to predict node clustering coefficient,
you would kind of call this unsupervised learning on
graphs because the supervision is not external.
And sometimes unsupervised learning is also called self-supervised, right?
Basically, it's the data that say- that-
the- the input data gives you the su- the supervision to the model.
So a link prediction task is an example of a self-supervised,
uh, learning task, right?
Where basically we take the unlabeled data but still
define supervised prediction tasks based on the structure,
uh, of that data.
So, um, let me first talk about supervised,
uh, labels on graphs.
So supervising labels come from specific use cases.
Um, and let me give you a few examples, right?
For node labels, you know,
you could say, oh,
in a citation network perhaps, uh, uh,
subject area that a node,
that a paper belongs to,
that's my external label,
is defined for every node.
Uh, for example in, um, [NOISE] uh,
in, uh, link prediction for pairwise, uh, prediction tasks,
for example, in a transaction network, um,
I could have the label y to- for
every transaction to tell me whether that transaction is fraudulent or not, right?
I have some external entity that tells me, it verifies
every transaction and says which ones are fraudulent and which ones are not.
So that could be the label.
Is it fraudulent or not?
And, you know, for entire graphs, for example,
if I work with molecules, as I said,
drug likeness or, uh, uh,
toxicity would be an example of an externally defined , uh,
label that we can now predict for the entire graph,
for the entire, uh, molecule.
Um, and you know, uh, one- one advice is,
is that to reduce your task to a node,
edge, or a graph,
uh, labeling task sees these tasks are standard and easy to work with, right?
So, um, what this means is that sometimes
your machine learning modeling task will come in as a,
not as a node classification task but as a link prediction task.
But if you can formulate it as one of these three tasks,
this means- means that you can reuse a lot of existing research and you can reuse,
um, a lot of existing methodology and, uh, architecture, right?
So a heavy fo- casting
the prediction tasks in terms of these three fundamental graph level tasks,
definitely helps because it's, uh, easy,
because you can lean on a lot of, uh,
prior work and prior, uh, research.
So now that we've talked about supervised tasks,
let's talk about unsupervised signals on graphs.
Um, the- the idea here is that sometimes
you only have our graph and we don't have any external labels.
Um, and the solution here is to define self supervised learning tasks, right?
So the models will still be- be the same,
they will still be the loss
just the supervision signal will come from the input graph itself.
So what are some examples of this?
For example, for node level tasks, you could say,
let me predict statistics such as node clustering coefficient or a page name.
Perhaps what one option would also
be that if you work with the molecule- molecular graphs,
maybe you would say, let me predict what type of atom is a given node, right?
Is it a hydrogen, is it a carbon,
um, is it oxygen.
That would be one way of a self-supervised task in a sense that you are
trying to predict some attributes- some property of that node.
For link prediction for edge - edge level tasks.
Very natural way to self-supervise is to hide a couple of answers
and then say can I predict whether a pair of nodes are connected or not?
Right? Can I predict whether there should be a link or not?
And that's a level of self supervision.
And then for graph level tasks.
Uh, again, we can think of different graph statistics.
For example, we can say,
are two graphs isomorphic?
Uh, you know, what kind of motifs, graphlets, do two graphs have?
Um, and you could use these as a way, uh,
to supervise, uh, at the graph level, right?
Um, and notice that in all these tasks that I defined now,
we don't require any external ground truth labels.
we just use the graph structured information in whatever,
um, is the input, uh, data.
So now that we've talked about,
uh, uh, predictions, uh,
and labels, now let's discuss the loss function and, um,
and talk about what kind of loss functions would I
use to measure discrepancy between the prediction and the labels, uh,
so that I can then optimize this loss function and basically back propagate all
the way down to the parameters of the graph neural network, uh, uh, model.
So the setting is the following,
uh, we have n data points,
and each data point can either be an individual node
and individual graph or an individual edge.
So for node level prediction will say,
each node has a - has node i has a label.
And here's the predicted label.
For edge level again,
we'll say each edge has a-
each potential edge has a label and we're going to predict that label.
Label could be does the edge exist or not?
Maybe it's the type of the edge,
whether it's fraudulent or not, and so on.
And similarly, last, right, for, uh,
graph level, I'm denoting this as g, you know,
for each graph, you have the true label - the true value versus the predicted value.
And I'm going to use this notation y-hat and
y to refer to the prediction- to the prediction,
uh, predicted value and the true value.
And I'm- I'm going to omit the superscript- sorry
the subscript so that would basically to denote what,
uh, prediction- specific prediction task we're talking about.
Because at the end, y is just an output and I can
now work with these outputs and compare the y-hat versus y.
So, uh, an important distinction is are we
doing classification or are we doing regression?
Uh, classification means that label's y,
uh, have discrete categorical values, right?
It would be, you know, what topic does the user like?
While in regression, we are predicting continuous values.
You want to predict drug likeness of a molecule or toxicity level of a molecule, right?
Binary prediction would be or a classification would be is it toxic or not?
Regression would be predict the toxicity level.
Um, and GNNs can be applied to both of
these settings to classification as well as to regression.
Um, and the difference will be between
classification regression is essentially in the loss function and the,
uh, in the evaluation method.
So let me first tell you about classification loss.
The most popular classification loss is called cross entropy.
We have already talked about this in Lecture 6,
where basically the idea is if I'm doing a K-way prediction for i-th data point then, uh,
cross entropy between the true label and the predicted label y hat is simply a sum over,
um, uh all these K different classes, uh,
the y value of that class,
for i-th data point times the log, uh,
predicted, uh, class value and y hat,
you can interpret as a probability.
Um, and the way this basically works is to - to say the following, right?
You can imagine that my,
uh, y is, uh, like this.
So this is not a binary vector that tells that
my particular let's say node belongs to class Number 3.
So it is a one-hot label encoding.
And then, you know, the y hat would now be,
um, a vector, uh, a distribution.
Perhaps we can apply, we apply Softmax to it.
So it means that all these entries sum to 1.
And this,  now you can interpret as the probability that, uh,
y is off, uh,
class number, uh, number, uh, 3.
And the idea is if you look at this equation, right?
Because the- the- the predicted probabilities- the probabilities will sum to 1.
What we want is that,
uh, wherever there is a value of,
uh, the true class is- is not here.
We want the probability there to be very high because 0 times anything is 0.
So we want it- we want the, uh, probabilities, uh,
to be- to be low here because log something close to 0
gives me a high negative value but if I multiply it with 0, it doesn't matter.
So I would want to, uh, predict low numbers here,
but wherever the- the value is 1,
I want to predict a high probability because, um,
here I'd be multiplying with 1,
but log of something that is close to 1 is, uh, is 0.
So again, the cross entropy loss- the discrepancy in this case will be small.
So basically this- this loss will force
the predicted values to the class- to other classes that
what data point i does not belong to- to have
low values, and where whatever class it belongs to, it will force it to have high value.
And that's the idea because if this is 1,
I want the second term to be as small as possible.
The way I make it small is to make it as close to 1 as possible, right?
Remember that these entries have to sum to 1.
And this is now loss
defined at a single data point i.
So the total loss is simply a sum over all the data points and
then the cross entropy loss for each individual data point.
So these is in terms of classification loss,
uh, the most popular one.
For regression loss, what is a standard loss is called mean squared
error or- or equivalently also known as the L2 loss.
And essentially what we are doing, we are saying, uh,
if I have a K-way prediction task and trying to
predict K area and values for a given node, uh, i,
I'm simply summing over all the K and I'm taking the discrepancy between
the true value minus the predicted value
and i square that so that this will always be positive.
And basically the idea is like the- the loss will take
the smallest value when y and y hat are as aligned as possible.
So when these differences are as small as possible.
And the reason why we like to take the quadratic loss
here is because it's smooth, it's continuous.
It's easy to take derivative of.
It's always positive.
A lot of kind of nice properties.
So again, this is a loss- mean squared error loss on- on a- on a pair on one data point.
And now over if I have N training examples,
then I simply sum up the losses of individual data points.
And this is now the - the loss on the entire dataset.
So this is the in terms of classification and regression loss.
There are also other losses that,
uh, they like to use, for example,
there are these losses called maximum margin losses that are very useful
if you don't care about predicting a binary value,
or a regression, but you care about the node ordering.
Perhaps you want to sort the nodes according to- according to some value.
And the idea is that what you care is for the nodes to be sorted properly,
and you don't care so much what exact values they have.
You want to know who are the top K nodes.
In this case, you would use some kind of triplet based loss it's
called, because you want to enforce one node to be ranked higher than,
um, than the other node or you would be using,
um, some kind of max margin, uh, type loss.
So now that we talked about loss functions,
uh, let's also talk about, uh, evaluation metrics.
So for evaluation metrics, um,
we - I - for regre- how we evaluate
regression is that we generally compute what is called,
uh, the mean square - squared, uh, error.
Um, and the way this is defined is, uh, again,
it's the squared difference between the predicted value and true value.
You defi- you - you take the average, so you, uh,
divide it by the total number of data points and then you take the square root.
So this is kind of analogous to the, um, uh,
to the L2, uh, loss that you optimize.
This is now, uh,
how you report performance,
uh, of your model.
You can also do mean absolute error,
where now you just take diff- absolute differences and divide,
uh, by the number of data points.
Um, if you look into Sklearn,
so the scikit-learn Python package that all of us will be using during the class, uh,
there is a lot of different, uh,
metrics, uh, already, uh,
implemented there, and here I just give you,
uh, two most common ones.
Uh, for classification, uh, you can, uh,
what is - a very standard way to report is what is
called classification accuracy where basically you'll just say,
"What number of times did my predicted, uh,
variable, uh, predicted class match the true class?"
And you say, "What fraction of times did I cor- correctly predict the class?"
Uh, this is nice, ah,
metric if your classes are, um,
about balance, balance meaning is that I know
half of the data points is positive and half is negative.
If you have very imbalanced classes,
imagine that you only have 1%,
uh, uh, positive, uh, class, uh,
data-points and 99% are negative,
then the problem with accuracy is that it's very easy
to get high accuracy by just predicting,
a negative class all the time, right?
So if I - if I'm for example, um,
if I have the case where I say is a transaction fraudulent or not,
and let's say 99% of transactions are non-fraudulent,
then my - my - I can get very high accuracy of 99%
if my classifier always says, uh, non-fraudulent, right?
So this is the problem with the accuracy is that if classes are imbalanced,
then trivial classifiers may have very high, um, uh, accuracy.
So, uh, to get more, uh,
to go deeper and - and to deal with some of these issues, um,
there are other metrics that are more sensitive to the- this decision,
what do we, the- what do we call as positive and what do we call as negative?
Because the - the models will usually output,
let's say some probability,
a value between 0 and 1,
and we have to decide on the threshold to
say everything above this threshold is positive,
everything below the threshold, uh, is negative.
Um, and the, uh,
precision-recall type metrics, uh,
uh, are one example to do this, um,
and there is also the, uh,
ROC AUC so the area under the receiver operating characteristic that I'm also going to,
uh, discuss and talk about.
So first is if you wanna know more than just accuracy,
then what you can define is this notion of, uh,
what is called confusion matrix,
where you basically say, "A-ha,.
What is the predicted value?
Is it predict positive or negative versus what is the actual,
the true value, is it positive or negative?"
And then you can count how many examples,
how many data points are each - in,
uh, in each of these four cells, right?
When you correctly predict negative you - you do plus 1 here,
when you correctly predict uh, positive,
you do a plus 1 here because predicted and actual match,
and then you can make two, uh,
types of mistakes called false positives.
These are, uh, examples where you predicted positive,
but they are actually negative.
And you then also have false negatives where,
uh, you predicted negative,
but the class is actually positive.
So, uh, accuracy is simply true positives plus
true negatives divided by the sum, uh, of all of them.
So this is here, right, it's the number of data points.
Precision, it's called out of all - um,
it says, "How many true positives are there?"
And I normalize this by true positives and false positives.
So pre- precision says,
"Out of all the positive predictions I made,
what fraction of them are true?"
And then recall says, um,
it says again true positives,
but I divide it by true positives and false negatives, right?
So I'm basically saying,
out of all positive, uh,
examples in the data set,
how many did I actually predict positive, right?
So recall says, "Out of all positives in the data set,
how many I have predicted positive?"
And precision is, out of predicted positives,
how many are actually positive?
Um, and then you can combine precision and recall into
some - into a single metric that is called, uh,
F1 score, uh, defined as
2 times precision times recall divided by precision plus recall.
This formula is a harmonic mean of precision and a recall.
That's why, uh, uh, it is defined this way.
This is harmonic mean or harmonic average of precision and recall.
Uh, in information retrieval, in, uh,
text-mining, people like to use,
uh, F1 score, uh, a lot.
And then the last, uh,
evaluation metric that I wanna talk about is, uh, ROC AUC.
So basically a receiver operating characteristic
or a receiver operating characteristic curve.
And this measures the trade-off between
the true positive rate and what is called the false positive rate.
Um, and true positive rate is really recall,
and false positive rate is defi- divided by
false positives divided by false positives plus true, uh, negatives.
And the way you usually, uh,
write it is that you've - you - you - you
draw true posi- a false positive rate versus true positive rate.
And - and this, um,
e- evaluation matrix comes from the field of, um,
medicine where in basically use - rather than thinking of it as - what - who is,
I know who is sick or not,
or you can kind of think of it as you sort people by a risk of having a disease.
And now you can imagine, uh, asking,
aha, I will take top,
two top, three top, four,
and you ask how good is the top,
how good - how clean are the to- top k candidates?
Because this classification threshold that determines
who - what - what points are positive and what points are negative,
in some sense is, uh, arbitrary.
And uh, what is interesting,
if you, um, take the positive and negative examples and, uh,
randomly sort them, then the - then you would get this,
uh, true positive versus false positive rate,
it will be a straight line.
So a random classification will give you a straight line.
And then, um, if you do better classification than the random,
then, uh, this, uh,
this particular line is going to approach, uh,
kind of more and more,
uh, this top corner here.
So if - if it would be a perfect classification,
basically it would go up immediately and then, er, remain flat.
So them, uh, the main trick that people like to use to - to, uh,
characterize the performance of a classifier is the area under the ROC curve, right?
So a random classifi- classifier will have the area
of 0.5 because it cu- covers half of the square,
and a perfect classifier would have an area, uh, of one.
So ROC AUC is the area under this,
uh, ROC curve, this,
uh, this is called the ROC curve.
And, uh, the higher the area,
the better the classifier,
uh, and a random classifier has an area of 0.5.
So 0.5 is not good,
0.5 is bad, it's basically random.
Um, one intuition - what did the, uh,
area under the AUC, uh,
the, uh, uh, ROC curve tells me,
is it tells me the probability that if you pick two, uh,
a positive and a negative point at random,
what's the probability that a positive point will be ranked higher than a negative point?
So if it's 1,
this means you gave a perfect classification,
all the positive points are ranked above all the negatives.
And if you give a random classification,
then the probability that the positive is above a negative is - is half because,
uh, it's random, right?
So, um, this is one intuition about the AUC, uh, curve.
So, um, we have talked about the training pipeline today, um, about, uh,
how do we define the prediction head,
we talked about the evaluation metrics,
we talked about where the labels come from,
and we talked about the loss function.
What we are going to talk about, uh,
next week is we are actually going to talk about how do you set up training,
how do you do training, how do you set up the datasets,
how do you split your data between test evaluation,
um, and, um, yeah,
training datasets to be able to do, uh,
efficient, uh, training and to get, uh, good results

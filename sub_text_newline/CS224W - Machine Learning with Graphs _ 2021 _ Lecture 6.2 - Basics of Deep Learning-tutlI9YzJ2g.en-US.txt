So what we are going to, uh,
discuss today is, uh, three topics.
First, I'm going to give a basic overview
of rudimentary deep learning neural network concepts.
Uh, and this will be important so that we all get on the same page.
And then I'm going to spend majority of the time talking about
deep learning for graphs and this concept of graph neural networks.
And then I'm going to discuss two specific, uh, architectures.
One is called graph convolutional networks and the other one is called,
uh, GraphSAGE, uh, to give you some intuition.
And then through, uh,
uh- through then series of the next lectures,
we are then going to go more in- deeper into the theory,
more go- deeper into different applications, different,
uh- different architectures, different design choices, um, and so on.
So we are going to talk about this graph neural networks topic,
uh, for the- for the next,
uh, couple of lectures.
So let's talk about, uh,
basics or do a quick tutorial introduction,
uh, to deep learning and deep neural networks.
So we will think of machine learning,
uh, supervised learning as an optimization problem.
So the idea is that we are given some inputs x,
and the goal is to predict or to produce outputs, uh, y.
And these outputs y we will call labels or classes and so on.
Um, and x can be represented in different ways.
X can be a vector of real numbers.
X can be a sequence,
like a natural language sequence,
sequence of word, sequence of tokens, voices.
It can be matrices,
meaning it can be a fixed size matrix like a-
the images that are already sized to be the same size.
Or it can be also an entire graph,
um, or a node in a graph,
which is what we will be, uh,
interested in, uh, later down,
uh, in the lecture today.
And then, right, the goal is that we want to map this abstract x,
whatever it is into the label y.
So we wanna be able to predict, uh, the label y.
And we are going to, uh,
need to learn this function that makes this mapping.
And we are going to formulate learning of this function as an optimization problem.
So we formulated as an optimization problem in
a sense that we say this function f that we are interested in,
that will take input x and produ- produce output
y would be parameterized by some parameters Theta.
And our goal will be that we will define this notion of a loss,
um, that we will say what is
the discrepancy between the predicted value and the true value?
So there are two important,
uh, things here, right?
Is that first, we are going to say, let's minimize, uh,
over these model parameters, uh,
Theta such that this discrepancy,
this loss is minimized.
So, uh, Theta is a set of parameters,
uh, we want to optimize.
Uh, you know, this could be a scalar value,
a vector, an entire matrix,
or a set of matrices.
Uh, and for example,
if you think in these, uh- uh,
laws-based framework, um, our, um, uh, deep,
uh- shallow encoders like, uh,
DeepWalk and node2vec ,
they're- in their- in those- those cases,
our parameter matrix Theta was really
the embedding matrix z in the shallower encoder case, right?
So- so Theta is just a generic, uh,
symbol to describe the parameters of the model.
And then loss function, as I said,
measures the or quantifies the discrepancy
between the predicted value and the re- and the, uh, true value.
So for example, um,
if you think about a regression,
so predicting real value numbers,
then y is the correct prediction,
f of x is the prediction,
what our modulary terms,
and L2 loss is simply ,
uh,a square, uh, of the distance- of the difference between the two numbers, right?
So we would say, I want to- I want to find the model parameters Theta
so- so- such that the sum of the squares of the differences is as small as possible,
square differences between the true value and the predicted value.
And of course, there are many different types of losses one may want to,
uh, use depending on the problem,
whether it's a regression wets- whether it's like
classification, whether it is, you know,
this, uh, um- whether it is a ranking task,
whether it's a classification task.
Um, and, uh, here's the link where you can,
uh, uh, talk- learn more about various types of losses.
Um, I won't go more into details of this,
but, you know, we mostly work with,
uh, L2 loss, which is used for,
uh, regression most often.
So when we are trying to predict, uh,
real values, uh, or cross entropy loss,
which I'm going to define later,
which is all about classification,
which is like classifying, you know,
single colors, for example.
So now let me give you an example of a loss function, right?
One common, uh, loss function that we are interested in is called a cross entropy.
Um, and let's say that we are talking about multiclass classification,
so we can have multiple color- colors.
So in this case, let say we have five classes- five different colors.
So, uh, this means that we are going to encode the-
the color using what is called one-hot encoding,
which means that we will say, aha,
now what we are trying to predict,
we are trying to predict a vector of dimensionality five, where, you know,
the first they mentioned perhaps corresponds to blue,
second corresponds to red,
third corresponds to green,
fourth corresponds to black and I don't know,
uh, the last corresponds to white, right?
So now I have- and if I given node of interest is- is green,
then, you know, the third entry of this vector is set to one.
So this is now how I encode the, uh- uh,
the, the- the colors in using this what is called one-hot encoding because there's 1,
1 and the rest is zero,
and then I can say, aha,
what I am going to do is I'm going to model this now in some sense,
probability distribution over colors, um, using,
uh, a function f, that will be a softmax of some function g. Um,
and, you know, lecture 3,
we defined the notion of soft max,
which is simply you go over to the ne- over the entries,
you expo- exponentiate them, and,
uh, you make sure that they sum up to one, right?
So for example, in our case,
maybe f of x, uh,
after we put in, uh,
f, uh, de- denote x,
we would produce the set of numbers
where basically you would say 100 probability point, you know,
we think the color is blue,
which points three-eighths and had read 0.40 is green and so on and so forth.
So now, what we wanna do is we want to measure the quality of this prediction, right?
We wanna say, what is the discrepancy between
the predicted probability of being green and the- the item truly being green.
And the way we computed this is no- is- is, uh, denoted, uh,
or called cross entropy loss,
where basically we are seeing,
let's sum up over all the classes.
Um, you know, here,
we have five classes,
so C goes up to five.
We say white is y_i?
What is the true probability of
that class times the lock predicted probability of that plus, right?
So in this sense, what this means is y is the actual and
f of x is the predicted value of the ith glass or the ith color.
And to intuition is the lower the loss,
the closer the pred- prediction is to one-hot, right?
If- if the value here would be one,
then log of one is zero.
So we made the correct prediction.
So now that we have a loss- defined the loss or
a discrepancy over any individual example or an individual data point,
we can then define the notion of a total loss,
which is just a loss summed up over all the training examples.
So it's a total amount of discrepancy between the predicted value and the true value,
summed up over all the training examples, right?
And what we want, right,
we want to find our function f,
our parameters to function f Theta,
so that this total discrepancy between the true values and predicted values is minimized.
And in this case,
we measure the discrepancy, uh, ah, for each,
uh, data point, for each prediction using this notion of a cross entropy loss.
So that's essentially, uh, the idea.
So now that we have defined the notion of loss and we have defined the notion, uh,
of the optimization problem,
basically trying to model parameters that minimize the loss.
The next question is,
how do we optimize this objective function, right?
And a classic way to- to minimize objective functions goes through,
uh, various kinds of more or less advanced notions of gradient descent.
So this notion of a gradient,
notion of a derivative,
the becomes central and most important, right?
So recall that gradient vector, uh,
at the given point is a direction and the rate of fastest,
uh, increase of a function.
So I can say, aha, I have my loss function,
and now I can ask my loss function, I can evaluate it,
um, with respect to my parameters,
and I can- this will tell me if I'm right- if
my parameters have a certain value right now, um,
what is the direction in which,
um, this, uh- this,
uh objective function, this loss function would,
um- would increase the fastest.
And- and that is very important because this means
then that I can think about what is called a directional derivative,
which is a multi- of a multivariable function, like for example,
the loss function, which is um,
where the variable are our model parameters theta.
Basically, it, uh, it basically tells us that alo- at a given point,
along a given vector represents instantaneous rate of change of a function along,
uh, at that vector.
So this means that I can now say given my current parameters,
in which direction should I change them such that the loss will decrease the most,
and that's essentially, uh,
what we are trying to do, right.
We would say we have a current estimate of our parameters,
let's compute the directional derivative of
our loss function surface or up that point where we are,
and then we are going to move into the direction, um,
of the fastest decrease of the loss and hopefully reach some good local,
uh, solution or a global minimum solution.
So gradient evaluated for the given point is the direction or derivative,
um, that gives me the direction of the largest increase, right.
Um, we are not interested in the increase,
we are interested in the decrease,
so we are going to walk in the direction opposite of the gradient.
We are going to walk- walk down,
not walk up, in terms of the gradient, uh, update.
Right. So a way how we think about this is to use
the algorithm called gradient, uh, descent.
This is the most basic version and then everything just kind of,
uh, uses the same intuition,
but it's just an improved over this.
And essentially, what this is saying is let me, uh,
repeatedly update the weights or the parameters of the model
in the opposite direction of the gradients until I converge, right.
So I say I have my current estimate of the grade- of the parameters,
let me evaluate the gradient, uh,
the derivative of the loss function at that, uh,
set of parameters at that point where my parameters currently are.
And then, you know, let me make a step in the direction that is opposite of the gradient.
So that's why I had minus here.
And this constant eta,
this is the learning rate, right?
It says how big of a step I wanna make.
And then this gives me the new updated set of parameters.
And now again, I put them here,
I evaluate the gradient,
and- and, uh, I make an- an update.
So basically in training, we say that, you know,
we optimize this Theta parameters iteratively,
and one iteration is one step of gradient descent.
And as I said, uh, eta here,
is the learning rate which is a hyperparameter that controls the size of a step, right.
Uh, and the idea is that usually at the beginning you could make bigger steps.
But as you get closer to the minimum,
you wanna make smaller steps because you don't wanna overstep.
You don't wanna kind of jump over the value,
you want to slowly descend into the value.
If you think of a function like this, right,
you don't want to kind of jump across.
And an ideal termination condition is when the gradient is zero,
which means you got stuck in some local minimum
where the function is flat so you know you are at the bottom.
In practice, um, we would stop training if
it no longer improves the performance on the validation set.
So rather than stopping when the gradient is zero, in practice,
we have a separate validation set over which we
validate the predictions of the model but we don't use it to compute gradients,
and, um, as soon as our performance on that
validation set stops improving, we stop the training.
That's usually the case,
even though, you know,
it might be still, uh,
possible to keep, uh, optimizing your, uh, objective function.
So the problem with this general gradient descent idea is that
computing the exact gradient requires a pass over the entire data set.
Uh, this is the case because if you remember earlier when I defined the loss,
I said the loss measures the discrepancy between
the data point- the predicted value and the true value,
and the total loss is a sum of the losses over all the training examples.
So now, this means that this- even when you are computing the gradient of the loss,
it means you have to dis- kind of propagate the gradient
inside the sum over all the training examples.
So it mean- which- which means that when you compute the gradient, each discrepancy,
each training example evalu- uh,
and the loss evaluated at the training example will have some contribution to the,
uh, to the total gradients.
So it means that one iteration of gradient descent in this case would
allo- would require to make a pass over the entire, uh, training dataset.
Um, and this is problematic because modern data sets
often have or often contain billions of data points,
um, and this can become very expensive and very slow.
So the solution, the speedup,
is called stochastic gradient descent or SGD.
And the idea is that rather than computing the loss over all the training examples,
we are only going to s- to compute the loss
and the gradient of the loss over what is called a minibatch.
And the minibatch is simply some small subset of the data.
And this is what we will call an x.
So let me now, uh,
define a couple of very important concepts
that you are going to hear over and over again.
So first, we talked about the notion of batch,
which is a subset of the data over which we evaluate the gradient, right?
Rather than evaluating it over the entire training dataset,
we are going to evaluate it on a small subset of the training dataset,
maybe hundreds, maybe thousands examples.
Batch size is the number of data points in the minibatch, right.
Um, so this is, uh, uh, important.
Usually, we like to make batches bigger,
but bigger batches make the- make the optimization slower because for every step,
we need to compute, uh,
over, uh, larger batch size.
Iteration in terms of stochastic gradient descent is then one step of, uh,
stochastic gradient descent where we evaluate the gradient on a given minibatch,
and we call these an iteration.
And then an epoch is basically a full pass over the datasets.
So basically, it means we get processing batches 1, 2, 3,
4, 5 all the way until we exhaust the training dataset.
So if we have, I don't know, a million examples and, uh,
we have, I don't know,
uh, um, 100,000 batches,
each one of size 10,
so basically after we have pre-processed,
uh,100,000 batches, this is one- one epoch.
And then we go to the beginning and start from the,
uh, beginning again, right.
So the number of iterations is equal to the ratio of the dataset size,
uh, and the batch size.
And as I mentioned, if you create these batches, uh, uh,
uniformly at random, then SGD is unbiased estimator of the full gradient, right?
Um, of course, there is no guarantee on the rate of convergence.
And in practice, uh,
this means it requires tuning the learning rate.
And this SGD idea is kind of a common core idea that then many other, um,
optimizers, uh, improve on,
like ada- adagrad, adadelta,
M RMSprop and so on.
Essentially, all use this core idea of selecting the subsets of the- subset of data,
evaluating the gradient over it and making the steps.
Now- now the details, uh,
vary in terms of what data points you select,
how big of a step you make,
how do you decide on the step size,
um, and so on and so forth.
But essentially, this minibatch stochastic gradient descent is the core of,
um, optimization in deep learning.
So now that we have discussed the objective function,
we discussed the notion of a minibatch,
we discussed the notion of a stochastic gradient descent, now,
we need to, uh,
talk about how is- is this actually done?
How are these, um,
gradients, uh, computed, evaluated, right.
Because in the old days, pre-deep learning,
you actually had to write down the model with the set of equations
and then you have to do by hand computed these gradients essentially,
you know, like we did it in high school, uh,
many of you are computing the gradients by hand- by hand on the whiteboard.
So essentially, you would have to compute those gradients by
hand and then code them into your, uh,
software C++, Python, Matlab, whatever,
uh, to be then able to,
uh, run the optimization.
Um, and interestingly, you're writing deep learning.
In deep learning, this prediction functions f can be very complex, right?
It can be this complex multi-layer deep neural networks.
Uh, and what I'm going to show you now is that basically,
the benefit of these deep learning approaches is
that the gradient computation is actually very,
very simple and um,
it comes for free in a sense that as you- as you made more complex models,
you- the complexity of gradient computation
in terms of what you have to do as a- as a programmer,
um, uh, doesn't really affect you.
So the idea is the following, right?
Let's start with a very, uh,
simple function, uh, f, that, uh,
basically take the input x and multiplies it with W. So
our parameters Theta of the model is this, um, is this,
uh, object W. Now if f returns a scalar,
if f returns a single number,
then W should be a vector, right?
X is a vector times a vector gives me a scalar.
So then for example, the gradient,
with respect to, uh, uh,
v of f respect- respect to W, is simply, uh,
vector where- where we differentiate, uh,
f with respect to w_1,
w_2, w_3, which are the components of our, uh,
vector W. And this, um,
gradient is then simply the derivative evaluate at- at a particular,
specific point, uh, W. So basically,
we have to work out what these derivatives are,
then plug in the concrete number for w and, um, get to the value,
and that would be then the gradient of that point W. Now for example,
if f returns a vector,
so f is a more complex function,
then W would be a matrix, right.
We would have matrix times a vector, gives me a vector.
So, uh, in this case,
W would be what is called a weight matrix.
It is also called a Jacobian matrix.
And then the way you would compute the gradient is
exactly the same sum up but now you would take the derivate
with respect to every entry of that W. So with respect to w1 1,
w1 2, w1 3,
and then you know to end of the first level,
and then it'll be W2 1,
2 2 and so on, right.
But essentially it's, uh, it's the same,
so now the gradient would be, uh, the matrix.
Now, we just had this very simple, uh,
predictor that just states the input and multiplies it with the- with the W. But now,
what if we wanna create more complex predictors?
Imagine, uh, just for the sake of the example,
I wanna have this complex predictor, uh, f,
that now first takes input x,
multiplies it with W,
and then multi- w_1 and then multiplies it with w_2.
So this now seems kind of more complex because first,
we are multiplying with
a one weight- weight matrix and then we are multiplying with the second weight matrix.
In this case, parameters of the model are the two,
let say rate, uh, matrices.
And now what we'd like to do is we'd like to compute the derivative,
both with respect to W_1 and W_2.
And what happens here is that we can actually apply the chain rule, right?
The chain rule says if you take a deriva- if you wanna take a derivative of variable,
uh, uh, z with respect to some variable x, but, um, uh,
variable z depends on variable y,
then the way you can do it is you say, aha,
I take z with respect to y,
and then I have to take y and, uh,
they could- derivative with- of it with respect to x.
So basically, this is how I can apply this chain rule to create this, uh,
deriva- partial- these derivatives, um,
and- and chain them together based- based on the dependencies.
So in our case,
if I wanna take my function f and compute the derivative of it with respect to x,
I could first take the function and take a derivative of it with respect to
W_1x and then I think the W_1x and take a derivative of it with respect to, uh, x.
And the notion of, uh,
back-propagation uses the chain rule to propa-
propagate gradients of intermediate steps and,
uh, finally obtain the gradient with respect of
the loss with respect to the model parameters.
Um, and this is very, um, uh,
interesting because it means we can mechanically compute, uh, the gradients.
So let me, uh,
give you an example, right?
So it- we are still working with this simple,
uh, two layer linear network, right?
Here is kind of the neural network representation of this,
but essentially, it takes,
let say, two dimensional on input x,
multiply it- it with W_1.
This is happening here,
and then multiply it with W_2 here,
to get an output, right?
Imagine I have some loss function.
Let's say I have a, uh, L_2 loss,
a squared loss that sim- simply says- was a discrepancy between the predicted value,
uh, and the true value?
And then, um, you know,
I evaluate this over the minibatch- mini-batch B.
And then I also have the notion of a hidden layer, uh,
and hidden layer is an intermediate representation for,
uh, input x, right?
So here, I'm using this, uh,
h of x to be W1 times x to denote the hidden layer, right?
It's some transformation of x that is not yet,
uh, the final output, uh, of the network.
And then of course, then I can rewrite this to say f of x is,
you know, uh, h of x,
which is the first product and then evaluate it, uh, um,
on the- on the s- on the g of h, uh,
which is the multiplication with, uh, W_2.
What this means now, if I wanna do what is called a forward pass,
I start with x, I multiply with W_1,
and then I multiply with W_2 to get the output.
So the way I can think of this as I start from x,
I apply W_1 to- to basically compute h. Now I,
uh, take h to, uh, um,
and apply function G to it, which is again,
I multiply with W_2 and I- I get the- I get now
the output f. And now I want to evaluate f,
uh, with respect to the loss.
So we have this kind of, uh,
nesting or chaining of functions.
And if I wanna do back-propagation now,
back-propagation means I have to now compute the derivative,
the gradient, and I wanna work backward.
So what does this means is that if these are my model parameters,
I start from the loss and compute gradients backwards.
So I would start with a loss, for example,
and I'm interested to compute the gradient of the loss,
uh, with respect to W2.
Then I have to go from the loss,
compute- take the derivative with respect to f,
and then I have to take, uh,
f and take a derivative with respect to W2, right?
So I went from lost to f to W, uh,2.
While, for example, to compute the derivative of the loss,
uh, with the- with respect to W_1,
I have to take the- the loss compute f of
the derivative with respect to f. Take f compute the derivative with respect to W_2,
and then kind of take the result of that W_2,
take a derivative, uh,
with respect to, uh, W_1.
And you can see kind of how I'm working backwards and how, uh,
as I go deeper into the network,
I can kind of re-use,
uh, uh, previous computations.
And this is why this is called a back-propagation because I kind of- kind of, uh,
working backwards, um, uh,
from the output all the way towards the, uh- the input.
And this then tells me how to update my parameter values so that,
uh, the discrepancy, the value of the loss will be smaller.
Um, note that in- in my case that I showed you so far,
we used a very simple two layer neural network
which is- which if- if you look at it carefully, uh,
is still a linear, uh,
function because W_1 times W_2 is,
uh- is another matrix or- or- or a vector.
But basically, it means that by chaining things,
we did not get any more expressive power.
This was still a linear model, right?
So, um, in this case,
in this simple example,
f is still a linear model with respect to x.
Now ma- no matter how many weight matrices do we compose,
how many Ws do we have.
But if we introduce non-linearities, for example, um,
a rectified linear unit defined like this and here's how it looked
like or a sigmoid function defined like this and here is,
you know, the pictorial version of it.
Then, um, these things become much more interesting
because now, by introducing non-linearity,
so actually increase the expressivity, uh,
of our model and the more than Ws we chain,
um, the more, uh, expressive the model will be.
And this now leads us to the model that is called multi-layer perceptron.
And in each layer of a multi-layer perceptron,
we combine linear transformation with the non-linearity.
So meaning so far,
we talked about W times x.
What to do now is, uh,
also apply a non-linearity tool,
for example, a sigmoid, uh,
or a- or a, uh, RELU, uh, function.
Um, here b is just a bias to a- a constant to exclusively take it out.
One way is also to assume that the feature vector is
one- one element or one entry longer,
and that entry is always value one,
and then these buyers becomes kind of part of b,
be- becomes a, uh, uh,
uh- a row- a row in b.
So, um, this is- this is now how,
uh, multi-layered perceptron works.
It's the same as we had before,
but just I sent things through a non-linear, uh, activation function.
And now, you know, if I want- now I can take this access at a given layer and I can,
uh, keep, uh, uh, chaining them by- by multiplying with another W,
sending through another, uh,
non-linear layer, multiplying with another W,
another non-linear layer, and I can make deeper and
deeper and more and more complex, uh, neural networks.
But in terms of optimizing them because of the,
uh, chain rule we explained here.
Um, the gradient computations can basically be done, uh,
mechanistically by the deep learning framework and we don't need to worry
about actually writing them down or worrying about how to do optimization,
uh, you know, which is great and really speeds up the development of,
uh, machine learning algorithms.
So to summarize, we talked about how to define machine learning using
objective function of minimizing the loss with respect to model parameters.
Uh, f, as we said,
this probability function can be a simple linear layer,
just W times x or a multi-layered perceptron where it's W times x,
uh, passed through a non-linearity, or, you know,
some other more- more complex neural network.
Um, and the idea is that we sample a batch,
uh, of input, uh, x.
We call this a mini-batch.
We then do the forward propagation,
um, to compute, uh,
the value of loss.
And then we do the backward propagation,
where we obtain gradients of the loss with
respect to the model parameters using the chain rule.
And then that- now that we have computed
the gradients with respect to the model parameters,
we use stochastic gradient descent, um, uh,
over this mini-batches to optimize our parameters Theta over multiple iterations.
And this, uh, you know,
now concludes our, um,
deep learning tutorial, and what we are going to talk about next is actually,
uh, graph neural networks.

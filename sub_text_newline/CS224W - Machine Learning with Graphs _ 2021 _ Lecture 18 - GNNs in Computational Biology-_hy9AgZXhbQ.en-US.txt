Today, I will talk about graph neural networks in computational biology.
And, um, there has been, um,
a tremendous surge in- of interest in leveraging,
ah, graph neural networks and graph representation learning,
in particular, ah, for learning meaningful representations of biology and
meaningful representations of datasets and
entities when co- we encounter in biology and medicine.
And so over the last few years,
we have seen, ah,
several prominent examples where GNNs have been used successfully,
ah, to learn, uh,
representations that have enabled critical predictions in a variety of downstream tasks,
including predictions about discovering novel interactions about- between proteins,
information about, ah, uh, treat- uh,
succes- potentially successful disease treatments, um,
opportunities about, uh, novel- regarding novel drugs and so on.
And so that has,
um- that of course,
is- is very interesting and particularly, um,
a natural question to ask is,
why do we see that graph neural networks,
achieve such success in biomedical and biological data?
And so to answer this question,
we need to think a bit about, um,
[NOISE] biology and- and
observe that biology is actually highly interconnected to the- uh,
in- in the sense that, um,
the way drugs work when- when people take
them is not that they work independently of each other,
but the way drugs work is they actually affect certain number o- of molecules, ah,
in humans cells that are typically proteins,
and those proteins interact with each other and
then- then these effects are then propagated.
The way they are propagating in- in human body and in
human cells is through those underlying biological networks.
So we can really see that biology is interconnected at various different scales,
from the scale of genomic sequences where we
see interactions between parts of the genome,
to the scale of interactions between proteins and molecules in- within a cell,
to- to interactions of external agents that are included in the human body,
such as drugs and therapeutics,
[NOISE] all the way to the scale of populations or entire ecosystems where we have,
for example, health care systems and dependencies between different institutions,
different hospitals, or different kinds of
symptoms within a heal- within healthcare records.
So the effects of many of biological then interventions,
many of biological, um,
e- entities are- they're really network effects.
And because of that, it makes lots of sense to study the biology through the lens
of networks and interactions between biologically relevant entities.
So to unpack this a bit,
we can try to understand why are networks in biology so powerful?
And I'll look- I'll look- I will take a particular example to- to explain that.
So let's look for a- let's take a look at the human protein interaction network.
This is a network where nodes,
um, represent human proteins,
and two proteins are connected by an edge if,
um, the proteins physically interact.
Meaning that at some point in time the two proteins are physically close in the cell,
and, um, you might want to know, well,
why- why are then proteins interacting with each other?
Um, if we look at a particular sub-region of protein interaction network,
which might be this particular subgraph on- o- on five human proteins where we see, ah,
five interactions, is there any significance between
these two protein MXS1 and B9D2 so- that they- in,
uh- in- in the sense that,
what does it mean and what are implications of them interacting with each other?
And to answer this question,
what we can do is to overlay on that,
um, protein interaction network
information about what are diseases that those proteins are involved in.
And if we do that for this particular example,
we can see that the these three proteins that are highlighted here in
red are all associated with the disease- with the same disease.
Meaning that when there are certain alterations in those two proteins,
a particular disease would develop.
And so what is interesting to- to- to
explore and obs- and observe is that if we overlay, um,
information about involvement of proteins in various diseases onto this- uh,
onto protein interaction network,
we can see that proteins associated with
the same disease tend to agglomerate or cluster in certain regions,
in certain neighborhoods of the network.
And that has, over the last two decades,
yield to very powerful paradigms in, um,
biology and medicine because it's powerful as it
allows us to identify new proteins that might be involved in
disease by simply exploring the surrounding and lo-
n- network neighborhoods of those proteins
that were already known to be associated with the disease.
That particular longstanding paradigm is known as local hypothesis.
And in other words,
it means that proteins involved in the same disease,
proteins that when mutated are,
um, lead to the same disease,
they- they have an increasing tendency to interact with each other,
meaning that they would be connected in our PPI network.
So the implication of that local hypothesis is that, ah,
m- mutations in interacting proteins often lead to similar diseases.
And now that we know that,
we can effectively leverage that information as inf- uh,
as prior information for- for our- for our machine learning models.
While I'll explain this particular local hypothesis in
the case of human protein interaction networks and diseases,
similar findings apply to
a broad range of different kinds of networks that we can see in biology.
Going from protein interaction networks to networks where nodes are patients
related and connected based on
familial or- relationships or similarity of the genomic profiles,
to networks representing hierarchies of several systems,
networks representing disease pathways, um,
gene interaction networks, cell-cell similarity networks, and many others.
And so that, um,
notion of local hypothesis where we see that, um,
entities with similar properties all- all tend to cluster
and agglomerate in certain network regions, um,
are- is really one of the core observations and
motivation and guiding principle that can explain why
graph neural networks are so well suited for the analysis of
biological networks and why they have already led to several very successful,
uh, applications to biomedical problems.
So- so that's all great and fantastic vision that we can share.
It's a nice motivation for great- for why it makes
sense to use graph neural networks for biological data.
But biological networks also present
some fundamental challenges that we can solve through algorithmic innovation.
So there are a number of challenges,
I will just mention tree here.
Um, so the first challenge is that
biological networks tend to involve heterogeneous interactions
that span from the mo- molecular level
all the way to the level of whole populations or societies.
And so typically [NOISE] the challenge is how to computationally operationalize the data.
What- how to even construct then- the graphs
that are then amenable to- to machine learning.
The second challenge is that,
um, networks in biology, uh,
contain data that you are co- that come from a variety of different resources,
including experimental readouts, curated annotations, and metadata.
And we really need to consider all those data
jointly because no single datatype
can really capture all the fact- factors that are necessary to
understand a particular complex phenomenon such as a disease.
It really is, um,
that we wo- need to, uh,
co- co- jointly consider the data from diverse resources to do so.
And finally, biological networks are by definition noisy and there by
definition are incomplete because our understanding
of the nature and biology is incomplete.
And they are noisy because inherent natural variations as well as
limitations of current biotechnological platforms that generate the data,
which means that we need to operate with the assumption that our data will be missing,
it will contain po- repeated measurements,
even contradictory observations that can plague the analysis,
and methods need to be robust against [NOISE] these issues.
So the plan for today lecture is the following,
um, I will [NOISE], uh,
talk about three vignettes that will
highlight some of these challenge- some of the challenges, uh,
for- for wor- dealing with biological networks as well as propose
solutions and show examples of
successful applications of graph neural networks in computational biology.
So the part that I will touch are- are the que- first
will be the question related to modeling safety of drugs and,
um, safety of combinations of drugs.
Second, I will talk about predicting patient outcomes and disease classification.
And finally, we will,
ah, discuss, um, um,
m- methods for identifying effective disease treatments and for
identifying promising therapeutic opportunities for emerging diseases.
And along the way, I will also highlight some of new- uh,
technical work that was motivated by- by- by these specific biomedical problems.
So let's start with the first topic, which is that of,
ah, [NOISE] modern drug safety and modeling drug combinations.
So this work is really motivated by the problem that is known as polytherapy,
sometimes also called polypharmacy.
It refers to the situation where patients take
multiple drugs at the same time to treat complex or coexisting disease.
This is a problem that is very common, actually.
46% of people over 65 years take
more than five drugs that are
prescribed to them by the do- their doctor at the same time.
It's not uncommon to encounter patients that
take more than 20 drugs to treat heart diseases,
de- depression, cancer, some other complex disease.
So the problem with that is that when a patient takes m- several drugs at the same time,
they're at much higher risk for developing adverse events.
And there are estimates that
fi- around 50% of- 15% of
the US population is affected by those unwanted side effects,
and that represents a huge burden both for individuals as well as for
the entire health care system in terms of rising health care costs.
So when I said that individuals are at
higher risk of developing adverse- unintended adverse events,
what did I actually mean by that?
So when patients take m- multiple drugs at the same time,
because we now know that drugs don't act independently of each other,
they might give rise to unexpected interactions between drugs.
So what does that mean? Let's look at
this hypothetical scenario where we have a system with just two drugs.
We have a blue pill and a red pill.
So when a patient takes blue pill alone, um,
there are no side effects observed and
the desired therapeutic effect- e- effect is achieved and everything is- is- is well.
When a patient takes red pill alone,
no side effects are observed and everything is okay.
However, when a patient takes blue and red pill together,
then with certain probability,
the patient would experience side effects,
um, uh, when taking that combination of drug.
So that is surprising and unexpected
because if the two drugs would act completely independently of each other,
then you would not expect to see
any significant side effects when the two drugs are taken together.
So our learning task for- for- for this part of the talk,
we will- we- uh, we- is the following.
We want to design a model that will be able to predict how
likely a particular combination of drugs lead to a particular side effect.
So that is the- uh,
the- the- the test that you want to solve.
Okay, so na- natural question to ask is,
now that we know what is our question,
is modeling drug combinations and modern drug interactions really a challenging problem?
As I answer to that is yes,
it's a very challenging problem.
Why is it so challenging?
First, and that's perhaps the core challenge,
is that we can see a combinatorial explosion of possible combinations of drugs.
Even if you take only a set of drugs that are approved in
the US and we consider only combinations that consists of two drugs,
there are over 30 million possible combinations of two drugs.
There are over 20 billion possible combinations of
three drugs and many more higher other drug interactions.
So the implication of that is- that is- is certainly infeasible in po- in- to test,
er, safety of those combinations of drugs in the lab,
which effectively means that these drugs that are approved by- uh,
by FDA and other regulatory agencies,
they get to the market,
and then patients take them,
and then we see what happens.
All right. So this is a huge opportunity for computational models to, ah, prioritize,
and a priori identify, and flag those combinations of drugs that
might be unsafe before they are really used in real patients.
The second problem is that, ah, dr- er,
un- unexpected drug interactions are by definition, non-linear, non-additive effect.
Right? An interaction is defined as an effect that is different
from what wou- is an additive effect of individual drugs,
which presents its own set of challenges.
And finally, you might think of
this problem as a kind of big data problem because we'll be working
with the set of all drugs in the US and all adverse events associated with those drugs,
and there will be millions of those adverse event reports.
But very quickly, we get down to the small data problem.
Where actually for any particular combination of drug there is
just a small subset of patients that take that combination of drugs and,
um, there might be actually no information available
about those combinations that are not yet used in patients.
And so that's another challenge.
So to tackle this particular challenge,
what I will mention is an approach that is called decagon,
that uses- that use graph neural networks to predict safety of combinations of drugs.
So decagon was defined on
a polypharmacy knowledge graph that had the following structure.
It's a graph with, ah,
two kinds of entities, drugs and proteins.
So in this particular, uh, case, um,
drugs are represented by green triangles,
an edge between two drugs has a particular type.
So an edge of type r_1 between two drugs would
indicate that a specific type of interaction,
a specific adverse event,
r_1 is observed when- uh,
in patients that take the two, um, um,
drugs, ah, together and their side effects cannot
be clearly attributed to either drug alone in that part.
Of course, in addition to knowing- um,
um, include information about existing interactions between drugs,
we want to understand how those drugs actually work in terms
of ha- what part of the body they affect,
and how the- they change activity of human cells.
And so for that, the second mode information in
the knowledge graph connects drugs to their protein targets.
And that, uh, proteins are represented by, ah, orange circles.
And then we know that proteins,
ah, interact with each other,
which gives rise to this underlying protein-protein interaction network.
So given this, ah, um, multimodal, ah,
polypharmacy network, the project I would describe it is called Decagon,
ah, has, er, two components.
And so in the first component,
in the encoder phase, it will take the multimodal network,
it will learn an embedding, ah,
for every drug, and every protein out in that, um,
graph, and then it will average those embeddings to predict labeled edges between
drug nodes that would correspond to predict
its drug interactions or adverse events of combinations of drugs.
So let's just, uh, briefly take a look at, ah, that.
Um, so the key idea for generating embeddings of
drug and protein nodes in- in that- that graph was to,
um, explore the local and look at the local network neighborhoods for
every node in the graph that are and separates them by distinct edge types.
So that is- is achieved to two phases,
where in the first phase for every node,
say node v, um,
we would de- determine a node's computation graph and
there would be a separate computation graph for each edge type that- uh,
that- that exists in the local neighborhood of that node v. And in the second phase, ah,
the idea is to take those computation graphs and learn what is
the best way to propagate neural messages along the edges of- um,
of computation graphs and trans- and- and then non-linearly transform them.
And so in this particular case,
we see that for node v,
node v has two- uh,
two first-order neighbors that are of type r_3,
and three second-order neighbors that are of type r_3.
So that would be- that would then define
a particular computation graph for node v for an edge type r_3 that would,
ah, specify how information is propagated along the edges of that edge type.
So this gives rise to multirelational graph encoder.
And in 2018, when the model was developed,
the- the architecture was relatively simple.
And- and I'm sure you're very familiar with
these kind of architectures by this far in the course.
So I'll very briefly describe it.
Um, the idea here is that in- er, that in Decagon,
node embeddings were initialized by, uh, node features, um,
which in the case of,
um, polypharmacy problem, were um,
features representing additional information about
drugs and proteins such as drug structure,
and protein memberships in pathways, and so on.
So embedding for node v and then at layer,
um, 0 was just initial,
um, node feature of that node.
And then at each layer,
um, of- uh, of the GNN,
the- the embedding for node v was- was
updated based on this particular equation which what it did.
It look at all possible edge types that- um, uh,
that node v was involved in,
and for each na- uh,
neighbor of node v in that particular edge type,
for each neighbor u, um,
the- the method took the embedding of that node u from the previous layer k minus 1,
it combined that embedding,
um, by- er, by, uh,
trainable, ah, weight matrix that was
specific for the edge type that you're currently looking at.
And so to that aggregated result was that
co- was then combined with the embedding for node v
from the previous layer and non-linearly
transformed to get an updated embedding for node v at layer k.
Um, the weights for combining the, uh,
the- the embedding, the- each- each, um,
each nodes embedding from the previous layer as well as the embeddings of the, uh,
neighbors at that point was defined by normalization constant,
which was fixed based on the node degrees.
Today, what- certainly, we would want to use our attention mechanisms.
But this particular equation was then applied,
um, some large K times where- where large K,
would define the number of layers and the final embedding of the, uh,
the node is the embedding after
K layers of the neighborhood propagation, uh, aggregation schema.
So that is fairly standard multi-relational graph encoder by now.
So the- the result of multi- uh, uh, uh,
of the graph encoder were embeddings for drug and protein nodes.
And so once em- em- embeddings for those nodes were obtained, um,
then the edges were- labeled edges between graph nodes were predicted.
And for that the input was, uh, was,
em- consists of embeddings of two nodes,
C and S. Those would be two particular drugs.
And so embeddings of those two nodes, C and S,
were combined by, um, um,
a particular type of- of tensor factorization, um, that,
um, was a- a style of the- tensor factorization that took embeddings for,
um, a pair of drug nodes, um,
and a combined- and multiplied them by,
uh, two diagonal matrices that we're relation specific, and essentially,
it encoded in a contribution of specific embedding dimensions towards a particular,
um, um, side effect.
And then, um, a matrix R,
which was, uh, a- a small, uh,
um, non- a fully non-zero matrix, um, that, um,
allow the model to capture any kind of dependencies
between any pair of dimensions in the learned embedding. And so the output
was then of- of these decoder were- were
then predicted edges of their- for every pair- for every particular side effect,
um, the model would return the probability that- that- that
a given pair of drugs would interact through that particular side effect.
Okay. So in order to then apply this model to the polypharmacy knowledge graph,
then the first thing to do is to construct the actual polypharmacy knowledge graph.
So the data that was used to do so was captured
molecular drug and patient data across all drugs prescribed to the US.
And for that, a unique dataset was built that consisted of,
um, over 4 million drug-drug edges.
Those are currently known interactions between pairs and drugs that was,
uh, that were retrieved from the FDA.
Information on drug-protein edges and information protein-protein edges,
as well as site features on nodes.
So that gave rise exactly to this multimodal knowledge graph that we have seen earlier,
which had over, um,
around 5 million edges all together,
and it was highly multi-relational as there
were 1,000 different edge types in that, um, graph.
One is the model where the dataset was constructed,
um, and, um, Decagon was fully specified.
Then, um, the Decagon, uh, uh,
model was trained on the polypharmacy, um,
knowledge graph in an effort to answer
questions of the following form so the users could then pose,
uh, a query, uh, for example,
whether a form of statin and ciprofloxacin where- which are two drug,
when they come together would they lead to breakdown of muscle tissue?
That effect should effectively meant that in the backend,
that the model was asked with predicting the probability that, uh,
these two drugs, um,
are connected in the graph by an edge of type r_2,
if r_2 would indicate breakdown of muscle tissue.
So, um, once the model was trained,
it was compared against, uh,
several baselines, uh, baseline algorithms at that point in time.
So these see- the results are shown in the slide,
where in the- on the y-axis we see performance,
high performer- higher values indicate better performance as
me-measured by AUROC and average precision at top 50.
And, uh, the performance of Decagon model is shown who read
performance of- of- other baselines,
um, is, uh, is shown in,
um, uh, brown, blue, and green colors.
So the particular baselines that we'll consider here was, uh,
a simple RESCAL tensor factorization,
which can be thought of as Decagon,
that does not use the deep graph encoder.
And so that test- that test has shown that it's really
important to use graph convolutions to learn powerful embeddings.
So then Decagon was compared to a multi-relational factorization,
which is as, uh, which, um, um,
any- the- the relative improvement of Decagon
over that factorization showed that it's- it's really
important to be able to model side effects jointly
because there are some correlations between side effects.
And- and finally, then, uh,
Decagon was compared to a shallow network embedding model, um,
embedding model which was a node2vec style model.
Um, and the improvement there showed that, um,
the- the- the new- the message passing network is- is
much better in- in terms of performance in this particular application,
uh, than, uh, some of the random walk,
uh, based, uh, embeddings at the time.
So another test that was done was to,
um, do, um, apply Decagon in a temporal fashion.
Meaning, the Decagon was trained on all the data generated prior to 2012.
And then, uh, the model was frozen and it was asked to make predictions,
um, that the model is most confident about. And we then test that,
well, how many of those prediction have been confirmed after 2012?
So what is shown in this particular slide is our top 10 predictions made by the model,
and so we- we recorded.
Each prediction comes in the form of a pair of drugs and
the side effects associated with that pair of drugs.
And so for five out of those top ten predictions,
direct evidence was found,
published after 2012, um,
showing that- it- showing examples of real patients who were on
a particular combination of drugs and have experienced
the kind of side effect the data has actually predicted would happen.
So for example, for drug,
uh, prediction number 8,
the- there- there are several reports of- of patients who took,
a statin and amlodipine together and experienced muscle inflammation.
And those reports were made after 2012 and- which
is- which provides evidence for- for- for prediction made by the model.
Some of the novel predictions in this space are
novel hypotheses that can- that warrant further analysis and- and
downstream investigations in several collaborations with
domain experts who are in- working
these disease and drug areas that has actually been done.
Um, and- and, uh, a re- more recent follow-up, um,
on the Decagon study is concerned with modeling
adverse events at the level of patient groups in an effort to
make predictions about what are adverse events that might be
associated with certain group of individuals.
There were- those groups can be defined by age,
by gender, or by individuals that have certain kinds of certain diseases.
And so, um, what- what- I'm mentioning this because, um,
the data is already, um,
preprocessed and provided to the study, um,
and- and so those who are interested in potentially leveraging
GNNs for doing more personalized level adverse event prediction,
which is certainly something that can- that is possible now to do with
this dataset that contains over 10 million ad- adverse event reports.
Okay. So [NOISE] this concludes my- my first, um,
vignette or first pa- topic- a large topic for today.
And the next topic, we'll- we'll change the gears,
and we will move away from predicting safety of drugs.
Instead, now we will, um,
we will think about diagnosing patients,
which is important crucial question that, uh,
first need to be answered before any drugs can be actually- or
treatments can be actually identified and recommended to the patient.
So let me start with the motivation of disease diagnosis.
Uh, so, um, the core information for disease,
uh, uh, for diagnosing the disease,
broadly speaking, is the information about
phenotypes that are seen and observed in a particular patient.
So phenotypes can be defined as some observable characteristics that,
uh, that um, can be seen,
er, uh, in- in a patient,
and they result from various interactions between
the genotype and the genomic information within the patient as well as, um,
environment in which the patient lives. So physicians typically utilize, uh,
various standardized vocabularies of phenotypes to describe human diseases.
For example, a very prominent standardized voca- vocabulary phenotypes
is known hu- as human phenotype, uh, um, ontology.
And, um, that codifies there- uh,
a variety of different phenotypes and how they map to human diseases.
So by mo- by- by modeling diseases then as collections
of phenotypes or- or that represent the disease,
we can then diagnose patients based on what are the phenotypes that they have,
based on what are the symptoms that those patients would have.
So in machine learning,
we can define the diagnosis task then in the following manner.
Uh, we will start with a graph.
Consi- consider for- in this particular example,
a graph that is built from the standardized vocabulary phenotypes.
That would be a graph where nodes are phenotypes that- those might be
visible symptoms as well as various phenotypes of the molecular or genomic level.
And edges between those phenotypic nodes would
indicate different kinds of relationships between phenotypes.
We can then- in- in- in that setting,
the disease would then be represented as a set of nodes or
a subgraph in that, uh, phenotypic network.
So the learning task would then be the diagnosis test,
is then concerned with- with predicting the, uh,
the label or predicting the disease that is most consistent with,
uh, a particular subgraph S that describes the patient.
So you can think of now that patients are
these different subgraphs of phenotypes the patients have,
and the goal is to predict labels,
which in this particular case are colors of the subgraphs,
and those labels are the diseases that, uh,
the- that- that are, uh,
candidate diseases patients might have.
Okay. So, if- given this formulation, uh, let's, uh,
let's discuss the algorithm for this problem a bit
and then we will return back to the data in the diagnosis task.
So in the context of graph neural network,
our problem formulation is the following.
The goal here will be actually to learn embeddings for subgraphs such that,
uh, uh, those embeddings for,
uh, the- the- for subgraphs that are learned,
we capture well, uh, the topology of
interactions between nodes as well as where they are,
where they exist in the context of,
um, the underlying graph.
So that means that,
um, that the input, um,
which then consists of the- some underlying graph G and a
large number of subgraphs that have labels,
and the goal is to learn subgraph embedding such that two subgraphs is a pioneer Sub_j,
that have similar subgraph topology,
and I will define next what that means.
They should be embedded close together in the embedding space,
so representing by nearby points in the embedding space that you want to learn.
So why are subgraphs challenging?
So why can we, for example,
just take our favorite node level embedder,
and by now, I'm sure you have heard of many node level embedders in the course,
and essentially, just learned node embeddings and- and
taken every- of those node embeddings or somehow,
um, aggregate those node embeddings to arrive at a subgraph embedding.
So there are several reasons why we- we might lose a lot of information if you do so.
So one of them is that, um,
the subgraphs that we would encounter in the diagnosis task,
um, are subgraphs of varying sizes.
And, um, then the question is really,
how to represent those subgraphs effectively that are not
simple k-hop neighborhoods around a sing- a- a single center node.
Second, subgraphs could- could- would have reached connectivity structures both
internally as well as externally to interactions with the rest of the graph.
So how can you inject that information into our GNNs?
And finally, subgraphs that are given by,
uh, sets of, uh,
phenotypes for patients can be localized
in the sense that they would reside in a single region,
in a single large neighborhood of the graph,
such as this hypothetical example where the subgraph is
indicated by this red blob that is core to the center of core of the graph,
or subgraphs may be distributed across many multiple local neighborhoods.
And so, um, then, uh,
a simple averaging of node embeddings might not be an effective strategy.
So to tackle this problem, uh,
what was developed was a subgraph neural network model.
And so the problem that subgraph neural network is solving is the following.
It assumes that given are some set of subgraphs S,
the set has an, uh,
n subgraphs, S_1, S_2 up to S_n.
SubGNN specifies then a neural message passing architecture that will generate,
um, D then- Dsub as dimensional subgraph representation for every subgraph in that set.
And would- they'll use those representations to learn a subgraph level classifier.
So a function that will then map those subgraphs to some number of discrete labels.
And the way it will do so is by really
taking into account different kinds of subgraph topology, in particular,
the- the parts of subgraph topology to- to- you're interested in,
is related to capturing the connectivity within the immediate local neighborhood of, uh,
each subgraph, both, uh,
the internal in- in- with- within a subgraph as
well as at the border of the subgraph in the, uh, um,
the graph G. The information about the structure of the subgraph in
the sense of other certain motifs that are over or underrepresented in a subgraph,
as well as where is that subgraph located relative to the graph G in- in- in which,
uh, it, uh, it exists.
So a- as a quick note on problem formulation that I want to- to give
here is that SubGNN puts forward a definition of subgraph prediction learning task.
Um, this is different from other canonical tasks on graphs that are primarily
concerned with a node level predictions
and representations where the goal is to predict properties,
um, from- of an individual node in the graph,
or link prediction which is concerned with predicting properties of a pair of nodes,
or graph level prediction,
which is concerned with predicting properties of entire graphs.
So SubGNN is concerned with predicting properties of subgraphs of non-trivial size,
meaning, not subgraphs of size just 2, which would correspond on edge.
The way this is done is that SubGNN consists of two parts.
So the first important part is, uh,
message passing scheme that is hierarchical in nature,
and it specifies the way neural messages are propagated from anchor patches,
which are helper graphs sampled from G to subgraphs S that, uh,
we want to embed, and how those messages are then embedded- um,
aggregated to arrive at the final subgraph embedding.
And in part 3 the- uh, in part 2,
that routing of messages is really down to three distinct channels where
each channel corresponds to a part- distinct aspect of subgraph topology,
position, neighborhood, and structure if you want to capture.
Uh, very blief- briefly,
let me describe these two parts.
So the subgraph message passing, uh,
scheme is- has the following structure.
Um, there are, um, um,
neural messages s- specified to
those messages are specific for each of the three properties.
So they're denoted in sub x,
and those messages are propagated from anchor patches to subgraph components.
Anchor patches are just helper subgraphs that are
sampled randomly from the underlying graph G,
and there are three different kinds of patches depending of the- of the- to capture for,
uh, to capture position, neighborhood, and structure.
So, um, then the- the- the message passing
is then formulated as the message from this anchor patch A to subgraphs S,
um, is, uh, is sent and it's scored based on its weight.
Its contribution is weighted by similarity function between a subgraph component in
an anchor patch that- that- that represents a weight
of how much an embedding of the anchor patch will,
uh, contribute to embedding of the subgraph.
So this is visually shown here in the right part of the slide where the-
the subgraph that- that is part of the data that you want to embed is S_1.
This subgraph actually has two disconnected components.
It- one is here, um, um,
on the left part and the right part of the, uh, the-
the right, uh, uh,
extreme of the graph,
and in the message passing phase, uh,
those helper patches- um,
anchor patches which are in gray,
send their messages to the subgraph component.
Those set of messages that are received at the subgraph component are aggregated,
and then the- the aggregated message of
the subgraph component is combined with its previous components- uh,
with its previous embedding to get
a property specific representation of, uh, subgraph component.
So H_x c is then an embedding for,
um, subgraph component c,
so it could be just for this fir- one component of the subgraph S_1, ah,
that captures the aspect x,
which can be position, neighborhood, or structure.
So the way this message passing is done,
is done in three separate channels.
And each channel, it describes, um, a particular- uh,
it's designed to capture one aspect of topology,
position, neighborhood, and structure.
Each channel x has three key co- elements.
Those elements are similarity function that weighs,
um, messages exchange between anchor patches and subgraph components.
Then a sa- uh, an anchor patch sampling function that specifies how
these helper patches need to
be sampled from the underlying graph in order to capture,
um, neighborhood or see- and- and capture
diverse neighborhoods and diverse structure and diverse positions.
And finally, how to encode those anchor patches,
and these functions can be learned or predefined.
And so the recap of that,
is that we can think of this problem as, um, uh,
as the- as the problem of learning subgraph representations as, um, uh,
as- as a message passing network that has three distinct channels,
and those channels output, um,
channel specific embeddings that are then aggregated to produce
a final subgraph representation Z_s for subgraph
S. So let's now return to our problem of, uh, disease diagnosis.
And the first thing we will do, we will, uh,
ask- we will- we will test whether that method works on
some simulated synthetic datasets to really see whether
the method can truly captured the distinct aspects of subgraph topology.
And the set up for that was the following.
In each case, each dataset started with the base graph, um,
and base graph was then, um,
sprinkled, um, with a large number of subgraphs.
There were two strategies to define subgraphs,
either to- one strategy was- uh,
was the planting strategy where a subgraph was generated
and it was planted on top of the base graph,
or it would be essential- essentially just joined in the form of- uh,
in similar way as it would be stable together with the base graph.
In this particular subgraph- er,
in this particular synthetical datasets,
each sub- for each subgraph,
a label was defined.
And so labels were defined based on
certain network structural properties of those subgraphs.
So one particular synthetic dataset was called density,
where- that was a dataset where, um,
that consists of the underlying base graph
with the- and then a large number of subgraphs,
the labels of those subgraphs were binned values of density network, density metric.
And similar datasets were constructed for- that, uh,
based on- on cut ratio metric, coreness, and component.
Why would you want to do that?
The idea was to do that to test whether
really the most informative channels for certain types of
labels are those channels that really truly determine the label.
So this is really a controlled environment of testing performance of the model.
And so- thus results show that SubGNN is an effective strategy for
learning subgraph embeddings that outperforms
various simple adhoc measures that one could think of.
For example, um, taking a simple average of
node embeddings at- and- and obtain subgrapgh embeddings in this way,
or introduce a meta artificial note and then embed those meta nodes
that could serve as proxies for subgraph embeddings or even doing graph level embeddings.
Finally, this- these results also showed that the SubGNN can
capture well different aspects of subgraph topology.
That was can be studied by looking what- which of the three channels; neighborhood,
position, or structure, would be most informative for predicting certain kinds of labels.
So for the case of density, um,
the label would primarily be determined by,
uh, the neighborhood of the subgraphs.
So what was then tested is whether
the neighborhood channel is most informative for predicting the density labels,
and indeed, that was the case.
Okay. So now that, uh,
we- we know that SubGNN works well,
let's return to our real-world dataset.
So there were four real-world datasets that were designed for the problem of,
uh, um, clinical diagnostic tests.
So each of the four datasets consists of a base graph,
and sub-graphs with associated labels.
So the two, uh,
datasets that I want to bring your attention to are called HPO-METAB and HPO-NEURO.
So these are the datasets where the base graph is the graph of- um,
from the human phenotype ontology.
So this is a graph where nodes are human phenotypes,
and just are hierarchical relationships between
phenotypes and symptoms that we see in human populations.
And then labels are defined by certain- uh, by, uh,
then subgraphs are defined by subgraphs of metabolic,
um, uh, diseases and metabolic disorders that, uh, affect humans.
And in the HPO-NEURO dataset,
then subgraphs represent, um,
uh, various neurodegenerative diseases.
So the goal of those two- uh, uh,
those- these two particular datasets is to, um, define- er,
is to predict subgraph labels that correspond to
various metabolic or neurological diseases
that are consistent with the underlying phenotypes.
So this simulates the environment of diagnosing
patients that might have metabolic or neurological disorders.
So let's see performance of SubGNN,
uh, across those, um,
four datasets with a particular focus on the,
uh, human phenotype ontology,
neuro and metabolic dataset.
And so what we can see is that the performance of SubGNN when- when,
uh, implemented with, uh, um, uh,
when, uh- when compared to, um,
baseline motto- models is substantially better,
and SubGNN outperforms though- those, uh,
baseline met- models considered here by up to 125%,
which is a- which is a really incredible improvement in performance,
which clearly motivates the use of subgraphs rather than simply,
um, considering note level or graph level aggregation.
Okay, um, so this concludes the second part of,
uh, the stock, which was concerned with,
um, using the power of subgraph embeddings and GNS for disease diagnosis tasks.
And in the first- the third part of the talk, I will, uh,
focus on se- important endpoint for drug development,
which is that of drug efficacy.
So in the first part we talked about drug safety but, uh,
in drugs and safety is one of
two cornered endpoints that we care about in drug development process.
In addition to drug safety,
the second important endpoint is efficacy,
where the question is to understand,
whether a certain molecule that might be candidate drug
wi- will really have positive therapeutic effect in patients.
So let's, uh, start with this third part of the talk.
The motivating factor- the- the motivating,
uh, problem for this,
uh, task was, uh,
the question of how to find cures for emerging diseases.
This particular, uh, work was motivated by the onset of the COVID pandemic last year,
uh, where- um, it was- became clear that, uh,
at the- them, uh, at the beginning of the pandemic, uh, there, um,
the need was for rapid de- deployment and
identification of drugs that might have positive therapeutic effect in, uh,
in COVID-19 patients but simply traditional approaches of iterative development,
experimental testing, and clinical validation,
and approval of new drugs were not feasible because they- they, on average,
take over a decade to bring and des- design
a new drug from scratch and bring it to market and that was simply not feasible.
So a more realistic strategy relied on repurposing of drugs,
requiring has to essentially identify
what other drugs that are clinically approved that might have
therapeutic effect in COVID-19 patients
and so the particular study that I will be mentioning is,
uh, is the- is the one that was
conducted bet- that it started at the beginning of March last year,
and the most parts of it were done between March and, uh,
last summer and it really demonstrates,
uh, an opportunity of how, in this particular case,
graph neural networks were able to compress years of work into actually weeks,
in some cases actually days of work by-
by- through very close collaboration between machine learning models,
predictions that are provided by those models and end-users, which in this case,
were virologists and experimental biologists who took
predictions and experimentally screened and intellect.
Okay, so that's- this is our goal for- for this part of the talk.
Let's- let's now try to, uh, uh,
unpack and define the problem of drug repurposing first and then
describe our technical approach for- for- for this problem.
So as I mentioned, the discovery of a new development of
new drug from scratch is a very lengthy resource intensive process.
It, on average, takes more than a decade and it
costs 1-2 billion dollars to design a new drug from scratch.
And so, faced with the skyrocketing costs for developing new drugs,
researchers are looking at ways to repurpose older ones,
to essentially take drugs that are already on the market and ask,
are there any other kind of diseases that those drugs might be effective in?
And that is very appealing strategy because it
would be much shorter and potentially much more effective.
Most famous example of a drug that was repurpose on the market is- is- is
Viagra that was initially designed for treating
cardiovascular diseases but then during clinical trials,
it turned out that there might be other indications that it would be useful for.
And so while in the past, those repurposing, uh,
strategies were primarily coincidental findings,
there is now a concerted effort of,
"Can we systematically go through drugs that are, uh,
clinically approved in the market and identify possible therapeutic opportunities?"
So the core of this question or the core of this problem is the following question,
which we can think of it as a link prediction problem in- in a bar touch graph,
where on the left we have a set of drugs.
Those might be all approved drugs plus investigational experimental drugs, um,
some drugs that fails in clinical trials,
but otherwise are not hugely, uh,
unsafe for patient, and on the right,
we would have tens of thousands of diseases which are codified diseases,
uh, that could be described, for example,
two set of phenotypes that we were discussing in the second part of the talk.
So edges connecting drugs to diseases
here would indicate the known treatment relationships.
The problem that we have here- our goal is to have
a model that would predict what diseases,
uh, a- a new molecule or an existing drug might treat.
So is this a challenging problem?
Why is finding treatments for a new disease in particular,
such a challenging problem.
So when we're conc- we're- when we're concerned with
the question of ho- finding treatment for a new emerging disease,
that means that we want to identify drugs that may treat the disease,
but there are no currently known drugs for that disease.
So, and COVID was a very,
very prominent example of that na- i- the- uh,
la- early, uh, last, uh, year.
And so conceptual- technically that
means- that- that's not surprising because we know in machine learning,
that generalizing to new phenomena is typically hard.
Prevailing graph neural network methods still assume and require
abundant label information in order
to be really trained effectively and achieve high accuracy.
However, I would argue that one of
the definitive factors of frontier of biology and medicine are
problems were labeled examples are incredibly
scarce and this includes emergent diseases such as COVID-19,
rare diseases, hard to diagnose patients,
modeling novel drugs in development and many, many other problems.
So what prevailing methods assume is that we start
with drugs that are already richly labeled.
That assumption is heavily violated in most exciting problems where in
reality what happens in the world that we only have a handful of labels,
uh, for a particular class, if any at all.
So too- for- for the- for the problem of drug repurposing, that, uh,
in the- in the meta da- and the gene and methodology that I will describe,
it really relies on few-short learning and meta-learning.
So I have two slides now with just a very brief high-level overview of that, uh,
of what metal learning is and I assume that many
have already heard about this in o- in other courses.
So, uh, meta-learning is
very effective strategy for- for learning in the cases where there
are a very few small number of
labeled examples available for- for- for tasks and so typically,
the idea of, uh,
meta-learning strategies is the following: we assume that we
have a- meta- we train a meta-learning model over a variety of
learning tasks and that model is optimized
for- to achieve the best performance on a distribution of tasks,
including potentially unseen tasks.
The way to think of meta-learning is that think of
each learning task being defined and associated with the small dataset,
D, that contains both data instances,
feature vectors, and some true labels.
And so meta-learning is then,
uh, concerned with ide- um,
finding the optimal parameters Theta star for the model, uh,
that would minimize the loss function across those,
uh, large number of datasets,
each describing a different task.
So if those models can define success with those parameters,
Theta star can define successfully,
that indicates that the model was able to really
successfully generalize from one task to the other,
from one dataset to the other,
and so if can- if it can do that effectively to the process known as Meta training,
then we can hope that also in the case of at task time,
it will be able to very quickly,
very effectively adapt to a new task,
in our case, that will be a new disease, COVID-19.
So this looks very similar to just normal,
regular machine learning task,
but you can think this actually one data sample
is not a single note, it's not a single, uh,
uh, edge in the graph but essentially,
a data sample is- is small dataset,
which in our case would be a small part of the graph on which the model is straight.
So, as- uh, uh, uh,
a second slide of the background of meta-learning is- is about few-shot learning,
which is just a specific instantiation of
meta-learning in the field of supervised learning and
I mentioned in there just because I want to introduce a no-
s- a one notation and that notation is,
uh, uh, describes the difficulty of a few- a few-shot learning tasks,
where we will say that we are interested in solving a k-shot n-class-
classification problem the- which meaning that we want to solve a task for that,
uh, in a problem for which we only have k labeled examples for each of n classes.
So what is showing here in this slide is an example of
a two-shot three-way image classification where we would have- uh,
where each task would consist of classifying im- class- uh,
images into three classes.
That's why it's two-way image classification and for each class,
we have two examples.
And so the process here would be the during training thought- phase we
def- we sample and define a large number of training tasks,
each of them is an instance of two-shot three-way image classification and
so if the model can be successfully primed to- to solve these, uh, to, uh,
or to- to solve this task during meta-training phase then in,
during Meta testing time, then, uh,
the model would- would quickly adapt to
completely new set of classes for which only a handful of labels are given as well.
The goal that we will have here now that we have this background on few-shot or
making meta-learning is how to- how to make predictions,
uh, on a new graph or a new label set.
So essentially, we want to use few-shot learning,
not on- for image classification- we want- but we want to
use it- use it for graphs. Okay?
Um, so the problem formulation, um,
is- of this problem is the following, uh, and,
um, we are interested in designing a meta-learner.
Uh, that meta-learner will be concerned with, um,
will need to be able to classify an unseen label set,
so that could be lab- a blue label set,
by observing some other label sets in the same graph.
So during, um, training, uh,
the model will be lear- only on
yellow labels and we would expect that the model would be-
would be able to generalize to blu- blue labels that are defined within the same graph,
um, during, uh, test time.
And so the meta that does that is called,
um, is- is called the G-Meta.
The overview of G-Meta,
if I present it in similar way as I have done for few- few-shot learning, uh,
just a few slides ago,
it's very similar to the general few-shot learning setting,
but now we are not concerned with, um,
solving image classification task but we are concerned with solving graph,
um, um, um, learning problems.
Um, in- in- for the case of this, uh,
explanation those would be node classification problems.
But during training time,
we would define a large number of- of tasks, each, um,
from a different- from a particular label set and at task time,
the new task will be presented on label set 3 that would not be seen,
that was not present at training time.
So the key idea that G-Meta uses to solve these, uh,
to solve the few-shot learning problem on- on graphs is that it was specified,
uh, um, you know,
a signature function for each learning task.
That signature function will be, uh,
defined based on the structure of- of,
um, subgraphs that encompass, uh,
labeled examples in each task and then
those subgraphs signature functions will essentially lear- uh, that, uh,
will learn how to map the structure of sampled subgraphs
representing tasks and how- and effectively,
uh, serve, uh, sen- initialization for a GNN.
And so this treated, it can be really effectively-
the strategy of extracting subgraphs that enclose
labeled example and then applying GNN to each subgraph
individually is effective strategy for doing- for doing few-shot learning.
A natural question to ask now is just conceptually,
which is an interesting question,
what is the value of these local subgraphs that we are using
now to transfer knowledge from one task to the other?
And the reasoning for why subgraphs, um,
um, are useful here is the following.
You can think of, there are
two fundamental core sources of information that the GNNs are using.
One is based that on label propagation,
where we can think of nodes that have the same label,
they tend to be nearby in the graphs.
We have seen at the beginning of the lecture today that proteins that tend to
interact the- they are also tend to lit- uh,
be, uh, affiliated with similar phenotypes in similar diseases.
The other core source of GNN power is structural similarity,
where nodes that have the same label,
they might not be nearby in the graph but they
might have similar network shapes in their local neighborhoods.
Okay. So now that we know that,
we know that in the case of few-shot learning where labels are really
scarce that we- that we cannot rely much on label propagation.
It's simply not sufficient.
It's- it's where only a handful of nodes are labeled.
It's very challenging to effectively propagate labels to the entire graph.
So instead, we can much better leverage
structural information and structural equivalence,
which is exactly what these local subgraphs,
uh, capture as a mechanism for transferring knowledge across tasks.
And it is actually possible to show theoretically
that local subgraphs that are a fine- defined around the,
uh, labeled examples capture much of relevant information for prediction.
And beyond theoretical findings,
it has been well, um,
demonstrated by many studies, ah, uh,
in the context of biological networks that typically most useful information for,
uh, that to describes a particular task,
um, in the context of biological network exist in some two,
three-hop neighborhoods around target nodes,
so the use of local subgraphs is then highly motivated.
Okay. So let's return now finally,
uh, to our COVID-19 repurposing problem.
And so I- I will start with description of the dataset on which this,
with the- the- the- this transfer
of this- this few-shot learning strategy that I described was applied.
So the repurposing dataset had,
had three major components.
The first was information on how to even represent COVID-19.
Okay. How can we represent it in the form of a graph or
some structure that- that is machine-readable that we can actually compute over?
And so to do that,
we relied on the study that, uh,
was published in, um, last year in, uh,
in Nature by Gordon et al and colleagues from
UCSF and they- what they have done, they have, uh,
expressed 26 out of 29 proteins in
SARS-CoV2 virus and they have checked
what are human proteins that those viral proteins take.
And so in doing so, they identified
332 human proteins to reach the SARS-CoV2 virus binds.
And so that can be used as essentially
computational- co- computable representation of COVID-19.
COVID-19 can be then represented by that set or subgraph or structure of
332 human proteins that are directly affect by the virus.
So these are known as viral-human protein-protein interactions
because they are actually interaction between the virus and humans.
The second important information was that on,
what are other proteins that those a- pro,
uh, proteins interact with?
So that is the human interactome network
or the human protein-protein interaction network.
And the third piece of information was up, um,
the information about, um,
existing drugs on the market.
So for each and every dru- existing drug we put information on its known targets,
uh, on proteins that those drugs bind to,
and that gave, um,
rise to those drug modules,
which would- would be the set of proteins for each drug that
are- that are targeted when the patient takes the drug.
So COVID-19 ultimately was represented as a network neighborhood of
human PPI network targeted by SARS-CoV-2 virus.
And the few-shot learning strategy was then applied to learn the embedding for COVID, uh,
COVID-19 that was the new task which is that we wanted to,
uh, rank drugs for based on how promising they might be for COVID-19.
The way the model was optimized,
it was optimized based on known drug disease indications for
other drugs and other diseases that are much better understood than COVID-19.
So that process yield an embedding space for approved drugs and diseases.
Uh, one point in that embedding space was the point that represented COVID- 19.
So an effective way to identify oppor- repurposing opportunities for
COVID-19 was to look at what are other, um,
what are drugs that are embedded close to the COVID-19, this em- embedding,
and then prioritize drugs based on that closeness between,
uh, COVID-19 embedding and the drugs embedding.
And that- that gives rise to a ranked list of
drugs where those are the top represents most promising algorithm- uh,
most promising opportunities for repurposing in the context of COVID-19.
The first test to do when you have such a model is to try to get a bit- uh,
some understanding into how accurate those predictions are.
So what this plot is showing,
it is an AUC-ROC, uh,
plot, um, that, um,
shows on the x-axis false positive rate,
on the y-axis true positive rate of a prediction problem,
which was defined as a question of,
can- can these models effectively identify those drugs that were at,
uh, in April 2020 investigated on clinical trials for COVID-19.
And so a number of baseline methods were,
uh, tested for this, uh, question.
Uh, the three GNN-based method- the four GNN-based methods are called,
uh, A1, uh, are denoted A1 up to A4,
and they showed substantial improvement in performance relative
to some other networks medicine-based, uh,
strategies based on diffusions,
random walks, network proximity,
and others that are more commonly deployed in the field of drug repurposing.
Then, finding it by itself, however, while promising,
it's not particularly exciting because the fact that a drug is
investigated in clinical trials for COVID-19 does not really mean that it's successful.
So a real test of the accuracy and
the ability of the predictions being useful is to experimentally validate predictions.
So for that, biologists and
virologists from the National Emerging Infectious Disease Laboratory,
so this is a national laboratory that can work with live viruses and it looks like this.
So it's looks like very kind of almost like some kind of environment in
a space where nobody can enter except those with very particular access.
And they're able to work with live viruses there.
So virologists there took the top predictions made by the model,
and then test- screen those predictions for their efficacy in first African monkey cells,
which are VeroE6 cells,
a very prominent cell line for COVID-19 tests and later in mouse in human cells.
So the result of that was that, uh,
77 drugs that were predicted showed
strong or weak effect and they were active over a broad range of concentrations.
Importantly, those predictions that were
obtained by GNN and actually an assemble of methods,
including some of the other network medicine methods,
gave an order of magnitude higher hit rate among top predictions than prior work.
So prior work would be just brute force tests of all drugs in,
uh, in, uh, in a- in the lab.
And if that is not done in a brute force manner,
but it's rather informed by prioritized list of drugs produced by ML models,
then an order of magnitude,
uh, better hit rate is something that we could expect.
Since we are in the, um, in a course which is concerned about networks,
there is an interesting finding that I want to mention here.
Um, so a natural question to ask is great.
So this analysis identified 77 drugs.
Is there something common to those drugs that inhibited viral growth?
So the answer to that question is yes.
So 76 out of 77 drugs that were predicted
and experimentally showed that they successfully reduced viral infection,
were though kind of drugs that do not directly bind
to those human proteins that the virus directly attacks.
Instead, those drugs rely on network-based action.
Those drugs, such as D3 here,
would target some human proteins that are not directly attacked by the virus.
And those human proteins in turn would interact with some other proteins who in turn,
finally, would be exactly those proteins that the virus attacks.
Why- why am I mentioning that?
It's because these kinds of drugs,
which we now call network drugs are- could not be
identified by traditional strategies for drug purposing,
which in computational biology and computational pharmacology are based on docking.
Okay, so since I'm already running over time,
I will wrap up here.
So to summarize the lecture, um, today, uh,
we talked about interesting applications of
graph neural networks in computational biology and we touched two top- three topics.
We talked about using GNNs for modelings in
drug combinations and identifying adverse events and concerns regarding safety of drugs.
We then talked about opportunities for- that GNNs present for,
uh, finding diagnostic tests and classifying diseases.
And finally, we talked about ways of- for using GNNs to identify, uh,
promising therapeutic opportunities with a particular angle,
which was that of opportunities for never before seen emerging diseases.
So, uh, this concludes my last lecture here,
I'm very happy to take questions. Thank you.
Uh, thank you very much Marinka,
uh, this was awesome.
Um, would you want to comment a bit more about what do you think are, uh,
big challenges, uh, in
bio-medicine that are ready to be attacked by machine learning AI methods?
Wow, that's a very- uh,
that's a very broad question.
So the way I would think of bio-medicine is,
um, in the following way.
So we can broadly think about certain core problems.
So one core problem is,
um, related to diagnosing patients.
Before any kind of treatment can be identified or any kind of drug can be recommended,
we first need to know what are,
uh, what is the disease that the patient has.
So a core challenging problem that is- that is ready to be
tackled from the AML standpoint here is in particularly,
um, uh, we need methods for rapid diagnosis of patients.
In particular, those patients that have, uh,
that are hard to diagnose and have many rare novel diseases.
You might say, well, that might not be lots of patients,
but it turns out that 20% of all patients in the US have these kinds of diseases.
On average, a patient or patients we treat these rare diseases need
to visit seven specialists before they are correctly diagnosed.
That's again a huge problem for healthcare systems,
for individuals, etc, etc.
So now the data-sets are available to expedite the diagnosis tasks considerably.
So I think that's one exciting problem in direction to pursue.
Sorry, just to ask a follow up.
So do you think of this as a
active learning type problem where you also recommend what other tests,
uh, to take so that you can diagnose the disease?
Probably not. Because, uh,
the problem is that if you think of- if you look at those patients,
they already went to many, many tests.
So they more or less, they took lots of different tests and we
don't know what- what else to do.
So it's not that much a question of identifying what lab tests should they conduct,
but the problem is that you- we can ask ourselves why we cannot diagnose patients easily.
The problem is that they are- they are conflicting in some way,
meaning that a patient would come and would present- presents with
phenotypes in symptoms that don't match with any known disease classification, right?
So the way patient comes to the physician,
the physician has a huge book then they- they- they- they- open and they say,
"Okay, this is a- these are the symptoms that I would expect for disease A."
And they compare that with patients in terms, there's no match.
Okay, so no disease A, let's go to disease B,
let's go to disease C. And the problem that it's hard to diagnose patients is that they
somehow have a mixture of symptoms that don't align well with any of the disease.
So it's- it's really more about essentially diagnosing them and the question is,
what is a good diagnosis?
Probably, it's not just saying a patient has disease A,
but it has these- these set of diseases which
or can be learned so kind of distribution
over known diseases that capture the patient well.
Uh-huh.
It's just the first task of identifying what's-
what's wrong with- with the individual, right?
Immediate next question is,
can we identify successful treatments?
So here, all the questions related to drug discovery and
development apply in the sense of can we re-purpose existing drugs?
Can we identify combinations of drugs that might work for a patient?
Can we do that in a more personalized way?
And it's possible now to date,
meaning that the data-sets are ready and available to move away from the one size fits
all treatment recommendation which is still
more or less the kind of work that I've been describing in the lecture today.
It's essentially one size fits all treatment recommendation.
But it's now possible to go to a more precise level,
at least at the stage of patient groups.
Meaning what would work for certain genders, certain ages,
certain combinations of gender, age,
or molecular markers.
Um, I think then- there is another large group of
very important high impactful question in biomedicine to ask,
which are related to, um, uh,
temporal aspects of how diseases progress and how to monitor the,
uh, patients and help them improve their, uh, self-care.
So that- that's- those are
important questions to- to tackle for which datasets are available as well.
And then close collaborations with health care systems would
offer lots of opportunities for kind of, um, tedious work.
[LAUGHTER] Tedious work would be optimizing clinical,
uh, workflows, identifying medical errors.
In, um, uh, then proposing systems that have,
at the end that use active learning to then identify, uh,
what- when to- um, uh,
[NOISE] when to add another drug to patients,
remove a drug from the patient,
when to call the patient to come to the- to the clinic.
Um, so those are
all exciting questions and some new questions have emerged during the pandemic.
And type- an example of a question that is new is that of,
um, [NOISE] uh, remote medical visits.
Um, that is particularly very exciting topic nowadays in the interface of,
uh, AI in medical space.
In particular, the question is,
can we design various AI systems that could help, um, uh,
doctors to perform visits remotely in the sense that,
um, one would have, uh, uh, uh,
it would be possible to take an image with, uh,
the- the- of- um, a patient would take, um,
an image with- with their app,
and that image would be analyzed automatically, um,
on the patient side and then communicated to the physician.
Or and- and here in particular,
active learning strategy is- is most important to ex- for example,
identify what part of the data is most crucial to have in order to make diagnosis, right?
And then can that part of the data being
acquired without the patient necessarily visiting with [NOISE] hospital?
And [OVERLAPPING] that would- that would enable,
um, uh, remote diagno- diagnosis and management.
Uh, this is super cool.
So, uh, I guess a follow-up, uh, question, uh,
is, uh, especially for the first few,
uh, problems that you are mentioning.
Why are graphs essentials?
Like- like, uh, why are these,
uh, graph problems and, you know,
not natural language problems or,
uh, you know, computer vision problems?
Why do you think graphs are essential in this case?
That's a great question.
I tried to shed some light into this question at
the very beginning of the talk- of the very beginning of this lecture.
And, um, so the- the answer to your question would
have- would have- would have be- would have two- two parts.
The first part is that it's very easy to see that,
um, most of the problems in biology are co-dependent,
or in- in the sense of whatever task we pick up, typically,
there- there are substantial relationships between
important entities in that task that need to be modeled and decades
of kind of fundamental biological research has
shown that those dependencies are really important and cannot be ignored.
So the relational structure of the dataset is very universal.
So we have seen that for essentially all three problems discussed in this talk,
that that's indeed the case and it has been
shown over and over again experimentally from protein interaction network,
CRISPR gene editing, disease diagnosis tasks, and so on.
So- so these are relations are an important component of the dataset.
So we would need to leverage models for for those tasks.
Second is, not only that, um,
these relations exist, but they are,
in most cases really informative,
sometimes even more informative than- than structural data,
for example, for the problems of predicting,
uh, proteins, uh, structure.
The recent models put out by DeepMind actually leverage relational,
uh, information about proteins to infer what their structure is, uh, about.
And- and- and so not only the datasets, uh,
and problems are- have strong relational structure,
but that relational structure correlates well with
predictive tasks that we want to produce.
And so then some form of graph-based machine learning,
um, seems very suitable.
Uh, thank you- thank you so much.
Uh, one more, uh,
question, um, is about, uh,
how effective has been to get the medical community to adapt to these methods?
Um, and, uh, uh, yeah,
so that I guess is, uh, the question.
So that's a great question and
the short answer to that is would be it's- it's challenging.
[LAUGHTER] So first to ans- to answer that,
by now we have seen several examples of successful deployments of,
um, this algor- um,
ML algorithms in general,
in clinical settings as well as in large pharma companies.
So that is primarily true,
so far, for image-based models.
Um, there aren't many or I'm not aware of really successful graph-based machine model,
that is applied and used routinely in clinical contexts,
or in, um, at least in public clinical settings.
What- where- where we are currently at this stage is that
typically individual researchers engage with collaborators would
be physicians or clinical researchers and so on and they would work hand in hand on
their particular problems and one could think of that as
validations- experimental validation of predictions but currently,
we are not there yet where these methods would be routinely used,
where graph neural networks or graph ML would be routinely used in, um, in real-world.
Why is that the case?
So I would argue that the main gap that is significant that exists between say,
models publishing Europe's papers,
and then these models being deployed in real-world,
is this lag debt.
Typically, on- when we apply these models to datasets,
they don't generate predictions that users would think of as actionable predictions.
What that means is that,
it's not enough to simply predict whether a drug will treat a disease.
It's very hard to convince somebody that will follow up with
very expensive downstream experiments in the lab
by this high level pointwise based prediction.
So what- what I didn't discuss in the context of COVID repurposing project,
it was extraordinarily important to essentially provide explanations,
allow users to provide feedback to the model,
and essentially build some trust and confidence in the model and
so that has now given rise to this- some relative term of trustworthy machine learning,
which I think has- has some bearing in the context of,
uh, um, biomedical applications.
Uh, thank you. Uh, then,
uh, uh, two more quick questions.
Uh, could you elaborate on the node feature engineering for drug- drug interaction,
uh, the drug combination in the first,
uh, part of your talk?
Um, and then we have one more.
Yes. So, uh, for node features in the first part of the talk,
we had node features for drugs and for proteins.
Node features for drugs were, uh, SMILES
fingerprints of, um, the drugs and, um,
they were- there were not manually engineered by- by us,
but this is a very common descriptor of drug structure that is used in cheminformatics.
It was retrieved from a database called DrugBank.
Uh, for protein nodes, the node features were
binary vectors that contained information
about what molecular pathways those proteins are involved in.
For example, if we know that a protein A is- uh,
participates in a cell apoptosis pathway,
this is a pathway that's incredibly important because it basically tells us- it-
it has a control over when a cell should die or stay alive, right?
So if a protein is a member of that pathway,
then there would be a- a simple indicator one at that,
uh, in particular- in the relevant dimension of the protein feature vector.
All right, thank you. And then,
uh, uh, one, uh, I'll,
uh, next question is about,
uh, the G-Meta learning,
how, uh, uh, somebody is interested,
how does it differ from Bayesian network learning and the EM algorithms?
Could you discuss pros and cons?
Oh, I see. Interesting. So we actually didn't
consider any Bayesian network learning method for the problem of- uh, of G-Meta.
We compared it primarily with, uh,
current work on- um,
that is graph neural network-based for few-shot learning.
I- I can clearly see from this question that it might be interesting to use, um,
some Bayesian model and then learn a distribution of drug effectiveness across diseases,
and then use that to sample a new disease that would approximate COVID-19.
Um, the pros of that would be that using the Bayesian models would perhaps,
uh, give us a better ranking of drugs.
Because ranking of drugs for that problem was determined on
predicted scores returned by GNNs,
and we know- uh,
and returned by G-Meta in particular.
Uh, from several applications,
we know that often the scores that are returned by the model are
not necessarily well calibrated or indicative of probabilities.
So I could imagine that the Bayesian network model might work better for that.
So that would be the pros.
The cons of that, uh, would be that,
um, what he found in the context of,
uh, COVID repurposing is that, uh,
the G-Meta or graph neural network-based models when implemented with very recent,
um, neighborhood sampling strategies from like Cluster-GCN,
or GraphSAGE for example, in other,
they can scale very well to very large datasets,
um, including, for example,
one dataset where we looked at, um,
information not only for humans,
but from- from our 1,800 other [NOISE] species,
and try to translate information from other species and model organisms like zebra,
fish, and monkeys to human.
And I- um, at least in my experience,
in- GNN models scale better to a larger dataset, than some Bayesian models.
Thank you so much, uh,
[NOISE] Marinka for, uh,
the excellent lecture and, uh,
thank you everyone for asking great questions.
Uh, appreciate it a lot. Uh, thank you so much,
and, uh, uh, all the best to Boston.
Great. Thank you for inviting me.
Yeah, thank you. Bye bye.
Bye.
Bye.

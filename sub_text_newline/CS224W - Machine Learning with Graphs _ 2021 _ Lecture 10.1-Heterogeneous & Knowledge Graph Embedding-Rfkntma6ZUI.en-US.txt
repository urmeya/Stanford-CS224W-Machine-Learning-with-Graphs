So today we are going to discuss, uh,
heterogeneous graphs and, uh, knowledge graph embeddings.
And in particular, we will be focusing on methods for,
uh, knowledge graph completion.
So the topic for today is about, uh,
first to talk about,
um, uh, a concept called heterogeneous graphs.
And, uh, basically so far in our course,
we had been handling graphs that have only one edge type and only one node type.
We just said there is nodes and edges and you know nodes are connected with edges.
So now the question would be,
how do we handle graphs,
directed- undirected graphs that have multiple types of, uh,
connections, multiple types of nodes,
multiple types of links between them.
Um, and this is the notion of heterogeneous graphs.
Uh, heterogeneous in a sense that they contain
heterogeneous sets of entities and heterogeneous set of, uh, edges.
Uh, and what we are going to talk today about in the first part is to discuss about, uh,
relational GCN that takes care about different,
uh, edge types, different relation types.
And then we are going to, uh,
talk about more specifically about knowledge graphs,
which are a class of heterogeneous graphs.
Um, how do we do knowledge graph embedding?
And also how do we do, uh,
a very common task which is called knowledge graph completion.
So let's start with, uh,
heterogeneous graphs and discuss relational GCNs.
So relational graph convolutional, uh, neural network.
So a heterogeneous graph is,
uh, defined by a quadruple,
is defined by a set of nodes, uh,
we'll call them V. Uh,
nodes could have, uh, different types.
Uh, it is defined by a set of edges that connect
different nodes and each edge is a- has a different,
uh, uh, relation type.
So this means that edge is now a triple that says node i is connected to node j,
via relation, uh,
R. And then also each node,
um, is of a different type and I can get the type of the node by,
uh, by looking into this, uh, uh,
function or set, uh,
T. So basically I have nodes and edges where every node is labeled by a type,
and every relation, um,
is labeled, uh, by a type.
So what would be some examples of heterogeneous graphs?
Uh, one very common example is in biomedical, um, uh,
field where you can basically take different biomedical entities, um,
and then relationships between them,
and represent this as a heterogeneous graph.
So for example, in- in this case,
I would have different, uh, node types.
I would have diseases,
I would have drugs,
I would have proteins.
I would have different edge types which is different types of relations between these,
uh, different types of entities.
And then you know, an example of the node here could
be the- could be the disease, uh, migraine.
And, uh, then I also have,
uh, different, uh, edge for example,
it would be that, uh,
a particular drug treats a particular, uh, disease.
And then, um, you know,
how can this- and- and then I can have different types of relations between them,
like causes, uh, uh,
and so on and so forth.
So that is one example, uh,
of a-, uh, of a heterogeneous graph,
the different types of entities,
uh, and different, uh,
types of, um, uh,
uh, relationships between them.
You can also think of event graphs as another example of a heterogenous graph,
where again, I could have different types of nodes,
different types of entities.
Uh, like for example,
a node type could be a flight.
I could have different, uh, edge types,
it would be a destination of a flight, um,
and then I could have,
you know, um, the destination,
different airports like the SFO, uh,
and create different types of, uh,
relationships, uh, between these, uh, different entities.
This is, uh, another example of a heterogeneous graph.
So now if we are given these type of a graph where we
have different types of nodes and different types of relations between them,
then we would like to be able for our model to handle this heterogeneity,
in some sense, to take advantage of it,
to learn that, uh,
that you know, treats, uh,
a given drug treating a disease is a different type of relationship,
than a given protein interacting with another, uh, protein.
So, um, wha- how are we going to do this?
Is we are actually going to extend our, um, GCN.
So, um, um, uh, graph convolutional, uh,
neural network, uh, to be able to handle different types of, uh, relationships.
We will call this our relational, uh, GCN.
Um, and we will,
uh, so far, right, we define the, uh,
GCN on one type of a relation and one type of a node type,
so on simple graphs,
and now we would like to expand it to,
uh, more complex heterogeneous graphs.
Um, the way I want to explain this is first to remind you about
the GCN and then it will become very natural, how do we extend it?
So, uh, the first question when you think about a graph neural network is, you know,
how do we- how does a GCN, uh,
operate and how does it update a representation of a given,
uh, target node, uh, in the graph?
And we have discussed these through this notion of, uh, uh,
message passing and- and computational graph,
where for a given target node,
let's say A, here we see its corresponding, uh, computation graph,
um, where every node gets information from its neighbors.
Notice that perhaps from the previous classes we talked about undirected graphs,
here now the graph is directed,
so every node only, uh,
gathers information along the direction of the edges.
So A gets information from B, C,
and D. But for example,
uh, node B, even though, um,
uh, it has two edges adjacent to it,
it only gathers information from C because C points to it and B points to A.
So that's why, for example here, uh,
the graph structure- the- the computation graph structure,
uh, is like this.
So this is now how you can, uh,
take into consideration, uh,
relations, uh, of edges.
Now, uh, that we have, uh,
re- reminded ourselves of, uh, uh,
uh, wha- how to, uh,
unfold, uh, the graph neural network over a directed graph,
we then talked about what does define a single layer of a graph neural network?
And we defined this notion of a message,
where every me- every node, uh,
computes, uh, the message coming,
uh, uh, from its, uh, children and this message,
uh, depends, uh, both on its, uh,
uh, on its message from the previous level,
um, as well as, uh,
the- the neighbors, uh,
that- that it has at the previous level.
And then the way the aggregation of these messages happens is
that- that the node V we will basically take these transformed messages,
uh, from each of the children and aggregate it in some way.
Uh, combine it with its own message and produce the message,
uh, or the embedding of the node, uh, at the next level.
And in order to add expressivity in terms of feature transformations,
not in terms of capturing the graph structure,
but in terms of capturing the feature, uh,
transformations, uh, we can then also add the,
uh, nonlinearity activation functions like sigmoid,
uh, ReLU, uh, and so on.
Um, and this is how we think of,
uh, graph neural networks.
Now, going back to the GCN,
and a- and a single layer of GCN is defined,
uh, by the following formula, right?
It's basically a node goes over its neighbors u
takes the previous layer representations of the nodes,
normalizes it by the- by the, uh,
parent's n-degrees, sums them up,
transforms them, and sends them through a non-linearity.
So, uh, what does this mean is that, uh,
the way you can think of this is that the aggregation of a- of a GCN is,
uh, uh, average pooling type of operator,
because of a summation and normalization here.
And then the message transformation is simple,
uh, linear, uh, transformation.
So now what happens if we have multiple relation types, right?
What if I have a
graph that has multiple types of relations?
Er, the way I will denote this is that I will use edges of different colors,
um, and, you know, here I labeled them as r_1, r_2, r_3.
Where it basically says, aha,
that A and B are connected according to the relation r_1, while,
you know, uh, B and C, er,
are connected by, er,
a relation, er, r_3.
So that's, er, the way, er,
we are going to think of the input graph now.
And the way we can now generalize a
GCN to the- this kind of heterogeneous graph is to use different network transformations,
different neural network, uh, weights,
different neural network parameters for different, er, relation types.
So it means that when we are doing relation transformation,
when we are doing message transformation,
we are not going to apply the same matrix W for every incoming edge,
but we are going to apply a different matrix W, um,
depending on whether- um,
er, what is the kind of the relation that we are considering.
So if we have three different relation types,
we are going to use three different types of, uh,
W matrices; one for relation 1,
one for relation 2, and one for, uh, relation 3.
So now, if I look at how does the, uh,
neural network, uh, computation graph for, uh,
the node A look like,
what is different now is that these transformation- message transformation operators,
these Ws, now have a color associated with them.
So it means that the message coming from, let's say,
node B and the message coming from node D, uh,
towards node A will be transformed using the same operator, the same W,
because the message is traveling along the relation type, uh, uh, r_1.
While, for example, the message from node C that,
uh, is connected to node A with, er,
relation r_2 will be transformed with a different transformation operator, the red one.
The red one is, er, designed for, er, r_2.
So here, um, you see- uh,
you see the difference, right?
Then, for example, um,
this means that now what- how we are going to write this up is that we have-
we have these transformation matrices W that are also indexed,
uh, by the relation type, right?
So in a relational GCN,
the way we write out the message propagation and
aggregation is the following: we basically say, um,
to compute the message of node v at level l plus 1,
we are going to sum up over all the relation types.
For every relation type,
you are going to look at who are the neighbors, uh, of, uh,
node v that- that are connected to node v according to this relation type
r. Then we are going to take the transformation matrix specific to that,
er, relation r to take the,
uh, messages from the- uh,
from the neighbors that are- er,
neighbors u that are connected to v according to- to this relation type r,
and we are are also going then to do the
me- the embedding of the node v from previous layer,
er, and, um, er, er transform it.
Uh, the- the important difference here is that Ws,
before, they used to be only differ layer by layer,
now what happens here is that we have W for every layer,
for every, uh, relation.
So, uh, another thing I did not explain is what is this C, uh,
er, sub v, r. This is simply a normalized node,
er, in degree compared to that, er, relation.
So it's basically the number of incoming, er, er,
relations of a given type r to the node, er,
v. This is still a- a graph neural network.
It is still- I can write it out in
this formalism of a message transformation and aggregation,
where basically each neighbor of a given- uh,
of a given node v transforms its- er,
its previous layer embedding according to the matrix W,
but now W changes for every, er,
re- based on the type of the relation between,
uh, you and me and, er, you know,
here we transform the- the message,
uh, from node v,
from the previous layer to, the, uh,
embedding from the previous layer into the message, uh,
for the- from- for the- from the previous layer,
and then the aggregation is still,
uh, simply a summation, right?
We take these, um,
transformed messages fro- based on the embeddings from the neighbors from previous layer,
where the transformation is relation-specific,
as well as node's own embedding from the previous layer uh, transformed.
We add all these together to get the embedding,
uh, at the next layer.
So this is called,
a relational, er, GCN.
So, um, basically, a relational graph neural
network or a relational graph convolutional neural network,
where the main difference is that now we have different message transformation,
er, operators based on the type of the relationship between a pair of nodes.
And this is denoted by this, uh, subscript,
uh, r that denotes,
er, the relation type.
So, um, this is how we defined a RGCN.
Um, the point is now, right?
We said that for every relation r,
we need L- capital L matrices, meaning, um,
these transformation matrices are different for
every layer of the neural network- of the graph neural network,
and they are different for every relation.
So the problem then becomes if you ask what is the size of this matrix?
The size of the matrix W is simply the embedding dimension, er,
on the- on the lower layer, um,
times the embedding dimension at the- at the upper layer, right?
So it's basically d, er, ^l and d, er,
^l plus 1 are the embedding dimension at
layer l and the embedding dimension at layer, er, l plus 1.
And these can quickly be a few hundred- you know,
these d's are in the order of a few hundred,
maybe up to a- up to 1,000, right?
So now the problem is that I have one such matrix per
every layer and I have one such matrix per every relation type.
And because, uh, heterogeneous graphs,
and in particular, knowledge graphs, can have hundreds,
can have thousands of different, er, relation types,
then you get- uh,
you can get tens of thousands of these different, er, matrices.
And each matrix can ha- is- is dense and can have quite big size.
And the problem then becomes that the number of parameters
of this model- of this RGCN model, um,
tends to explode because, uh,
it grows with the number of different relation types,
and there can be thousands of different relation types.
And one problem is that the model becomes too big, um, to train,
and then the other problem is that the model has too many parameters,
um, and overfitting can quickly become an issue.
So what I wanna discuss next is, er, two approaches,
how to reduce the number of parameters of this RGCN-style model,
um, using two techniques.
One technique will be to use, uh,
block diagonal matrices, uh,
and the other one will be to use basis or,
uh, dictionary learning as it is called.
So let me first talk about, uh,
use of block diagonal, er, matrices.
So the key insight is that we wanna make these Ws- W matrices,
the transformation matrices, we wanna make them sparse.
And one way to make them sparse is to enforce, to have this block diagonal structure.
So basically, non-zero elements are only along the specific blocks,
uh, of this bigger,
er, matrix, er, W.
Uh, if you think about this,
this will now reduce the number of non-zero, uh,
elements- the number of parameters in each W you have to estimate,
because you only have to estimate that the blocks,
uh, these two green blocks,
and you can basically ignore or assumed these,
uh, empty parts of the matrix are 0.
So if you assume,
for example that W is composed from B, uh, low dimensional,
uh, matrices, low-dimensional blocks,
then the number of parameters is going to decrease,
uh, by a factor B.
So if you said B by 10, uh,
you have just reduced the number of free parameters that you need to learn,
uh, or estimate by a factor of 10.
Of course, what do you lose?
What you lose is now that if you think about this as a transformation matrix,
what you lose is that, um,
embedding dimensions that are far apart from each other,
they cannot interact with each other, right?
So this means that, for example here,
only embedding dimensions 1 and 2 can interact with each other,
but not, um, let's say 2 and 3 because they are in different blocks.
So it may require several layers of propagations
and- and different structures of block diagonal matrices to be able for,
a embedding dimension 3 to kind of talk to embedding dimension 2.
Um, so basically what this means is that only nearby neurons or neurons
in the same block can talk to each other and exchange information with each other.
So perhaps your GN- GCN,
GNN may need to be a bit deeper,
but you have reduced, uh,
the number of parameters, uh,
significantly, which will lead to, uh,
faster training, perhaps more robust model,
less overfitting, um, and so on.
So that's, uh, the technique of,
uh, block diagonal matrices, uh,
where basically you reduce the dimensionality of
each W by assuming a block diagonal structure.
Uh, second idea how to, uh, make this, uh,
RGCN more scalable and its learning more tractable is to- to
build on the key insight which is we wanna
share weights across different relations, right?
We don't wanna consider relations as independent from each other,
but we want relations to kind of share weights,
uh, to also share some information.
And the way you can achieve this very
elegantly is that you will represent the weight matrix,
the transformation matrix W for each relation as a linear combination of basis,
uh, transformations or as of basis matrices.
And you call these basis matrices as your dictionary.
So this is also called dictionary learning.
So let me now, uh, explain.
The idea is that now your, uh, W,
uh, r is simply, uh,
weighted summation over this dictionary of matrices V,
um, that- and I have B- B of them, right?
So basically the idea is that I'll have a dictionary of matrices V.
Uh, and then to come up with a- with a, um,
transformation matrix for, um,
node W, then basically what I have to do is, uh,
have these, uh, weight- weight- weight scores,
uh, important weights, uh,
a, that say how important is a given matrix,
uh, V_b, for a given relation R, right?
And then the point is that these, uh,
matrices V are shared across all the relations.
So I can think of matrices V as my basis matrices or as my, uh, dictionary.
And then these, uh, weights, uh, A,
those are importance weights,
uh, for each matrix.
So basically I'll say, the transformation matrix for,
uh, a given, um,
relation R is some kind of linear combination of
these basis matrices where these linear combination weights are learned for every,
uh, for every, uh, relation, right?
So this means that now every relation only needs to
learn these importance, uh, weights, um,
which is just capital V number of scalars rather than,
uh, uh, a- a large number of different, uh, transformation matrices.
So now basically, every relation
specific transformation matrix is simply a linear combination,
uh, from this dictionary of, uh, matrices, uh,
V. And that's, uh,
an elegant way how to reduce, uh,
the number of parameters because b can be relatively small,
let's say, I don't know, 100,
uh, or 10 but,
maybe you still have,
um, uh, 1,000 different, uh, relation types.
And you basically say, I will represent every of the 1,000 relations as
a linear combination from my dictionary of 10, uh, weight matrices.
And that's a very elegant way how to reduce,
uh, the number of, uh, parameters.
So now that we have talked about this, uh,
scalability issue of, uh,
uh, RGCN with two different approaches,
one being, um, the block diagonal structure of the transformation matrix W,
the other one being that we cast learning of W as a dictionary learning problem,
which basically means we assume there is an underlying set of 10- let's say,
10 different basis matrices which we call dictionary,
and then every, uh, uh,
relation specific transformation matrix is simply a linear combination of the, uh,
of the matrices coming from the dictionary also reduces
the number of parameters makes the method more robust and, uh, speeds it up.
So now that we have RGCM, uh,
what we want to do is also talk briefly about,
how do you define various prediction tasks on the,
um, on the graph- on the heterogeneous graph?
So in terms of node classification or anti-classification,
um, it is, uh, it is, uh,
it is all- all, uh,
kind of as it used to be, right?
So, uh, RGCN will compute the representation or an embedding for every node,
uh, for every node, um,
then- then based on that embedding,
we can then create a prediction head, uh, for example,
if you want to classify nodes into k,
uh, different classes, uh,
then the final prediction head will simply be
a linear transform that will basically take the embedding,
multiply it with the weight vector,
uh, pass it through a non, uh,
non-linearity like a sigmoid and we can interpret that as a probability,
uh, of that given class.
So basically we will have a k head output rather than, uh,
um, h, and then have a softmax on top of it,
so that basically we interpret that as a probability that, uh,
that a given node is of the kth,
uh, given type or kth head.
That's how we think of node classification.
For link prediction, things get a bit more tricky because now links have different, ah,
types and we don't want to simply split links, uh,
randomly as we did it so far, uh,
because some types might be very common and other types might be very uncommon, right?
Some relation types might be very common,
some relation types might be very uncommon.
So what we would like to do is for every relation type,
we would like to split it between,
ah, training message edges, uh,
training supervision edges, validation edges, and test edges.
And then we would like to do this for every
of the relation types and then kind of merge, uh,
all of the message edges for the- for the training will, er,
take all the training supervision edges for each separate relation type into
the training supervision edges and then same
for validation edges and for test edges, right?
So kind of the point is that we wanna
independently split edges from
each relation type and then merge it together, uh, these splits.
And the reason we wanna do this is because this means that even for a very infrequent,
ah, very rare, uh,
relation type, some of its- some of the instances of it will be- will be in the training,
some of instances will be in the validation,
and some will be in the test set.
Because the point would be if you just blindly split the edges,
just by chance it can happen that a very rare edge type does not appear in your,
ah, validation set because it's just too rare and by
chance none- none of the edges, uh, landed there.
So that's why we wanna do this,
what is called stratification,
where we- where we independently split, uh,
each relation- edges of each relation type separately,
and then merge it all, ah, together.
Um, so we have these four different edge buckets,
training message as edges,
training supervision edges, validation edges, and test edges.
We do the splitting for each relation type separately
and then merge it all together into the four sets.
And then, you know, everything still, uh,
applies as we talked about, ah, link prediction.
So to tell more or give you more details about how you formalize link prediction,
you look at heterogeneous graphs.
Imagine, you know, I wanna be able to predict, um,
whether there is an edge or what's the probability of an edge between nodes E and A,
um, and that- that edge is of,
ah, type 3- of relation type 3.
Um, so imagine that this edge is a training supervision edge, right?
And let's say that all edge- all other edges are,
uh, training message edges.
So the way I would now, uh,
use the RGCN to score these edges,
I would take the final, uh,
layer embedding of node D. I would take the final layer embedding of node A.
And then I would have a relation specific scoring function f, ah,
that would basically, um,
take these two embeddings and transform them into a real- ah, into a real value.
So one app- approach to do this would be to use this kind of,
ah, uh, linear- uh,
bi-linear form, where basically I take one embedding,
I have the transformation matrix in-between and another embedding,
ah, so that at the end, basically this takes,
take the embedding of node D, transform it,
and then dot-product it with,
uh, embedding of node A.
And I can interpret these,
perhaps send it through some sigmoid or something like that.
I can interpret this simply as the probability that- ah,
there is an edge of relation type,
let's say r_1 between nodes E and A.
So that would be one way, uh,
to- to actually formalize and,
uh, instantiate, ah, this, ah, problem.
Now, um, how exactly am I thinking about these during training?
So let's assume again that this is the edge,
that is a supervision edge.
Um, and let's think that, ah, all other, er,
edges in the graph are- are the training message passing, ah, edges.
So we wanna use the training message-passing edges to predict, ah,
the likelihood or the existence of this,
ah, training, ah, supervision edge of interest.
What we also have to do in link prediction is that we have to create negative instances.
We have to create, ah, negative edges.
So the way we create negative edges is by perturbing the supervision edge.
So for example, if this is the supervision edge,
then one way how we can do- do it is that we corrupt the tail of it, right?
So we maintain the head,
we maintain node E,
but we pick some other node,
ah, that node E is not connected to with the relation of type r_3.
So in our case, we could, for example,
create an edge from E to B or from,
uh, E to D. So these could be our negative edges.
What is important when you create negative edges,
the negative edges should not belong to
training message edges or training supervision edges.
So for example, edge to the node C cannot be a negative edge because
node E is already connected to the node C with relation type 3.
So this is not a negative- uh,
a negative edge because the edge already exists and it's of the same type as here.
So we don't wanna create this, uh, contradiction.
So we have to be a bit careful when sampling these, uh, negative edges.
Now that we have, uh,
created a negative edge by perturbing the tail,
uh, so the end point of the- of the supervision edge.
We can now use,
uh, the GNN model,
the RGCN, to score- to score the positive as well as the negative edges.
And the- you know, the loss function you would wanna optimize is a standard, uh,
cross entropy loss where basically we want to
maximize the scores- the score of the training supervision edge,
we would like to maximize the scores of this guy,
and we would like to minimize the score of negative edges, like for example,
E to B or E to D. So that's how we- we would write down,
ah, the penalty, uh, as I show it here.
Um, and then, er, use the optimizer stochastic gradient descent to
optimize the parameters of RGCN to basically assign high probability,
high score to the training supervision edges and low score,
uh, to negative edges.
Now that we have the model trained,
now assume we move to the validation time.
We wanna validate, uh,
the our- the- the performance of our models.
So let's assume now that
the validation edge that I'm interested in is this edge between node E,
uh, and node D. And I'm interested whether it's of type r_3.
Then in this case,
training message edges and training supervision- supervision edges basically means,
in- in my example,
all existing edges of the graph are used, ah,
for the message propagation and then at the validation time,
I'm basically using all these solid edges to do the message propagation.
And I'm trying to score the value of this particular, uh,
edge from node E to node D, right?
And again, the intuition here is that the score of
this edge should be higher than the score of,
let's say all other, ah,
edges that are- that are in some sense
negative or don't exist in the data set yet from node D. So for example,
this would mean that the score of this node edge from E to D
has to be higher than the one from E to B in our case.
Um, and- and I cannot consider
these other edges because they are already used as a message passing engages in my,
uh, validation, uh, RGCN.
So notice that it is important that
these sets of edges that are independent from each other,
uh, when we score them.
So how would I evaluate this?
Right, I would get- use the RGCN to calculate the score of edge ED,
uh, according to the relation type 3.
I would then calculate the score of all the negative edges.
So in my case,
only two possible negative edges of type r_3 are E to B and E to F, right?
I cannot do A and C because they are already connected according to the relation 3.
So that would be kind of a contradiction.
So I have only two negative edges.
Ah, and the- the- the goal then is basically to obtain a score for all these three edges.
We rank them and hopefully the- the rank,
the output score of the edge, ah,
ED will be higher than the output score of EF and BF.
And then how do I usually evaluate the calculation matrix?
How do I usually, um, evaluate?
Um, I can do either what is called hits, which would be,
how often was the correct positive edge ranked among
the top K of my predicted edges or I can do
a reciprocal rank which is 1 over the rank of the-
of the- of the positive edge ranked among all other, uh, negative edges.
And then you can do mean reciprocal rank,
and the higher the mean reciprocal rank,
the better or high- the higher the hits score, the better.
So let me summarize.
We talked about relational GCN,
which is a graph neural network for heterogeneous graphs.
We talked about how to define it.
We talked about how to have a relation type specific transformation functions.
Uh, we discussed about, uh,
how to do entity classification as well as link prediction tasks.
We also discussed how- discussed how to make a relational GCN more scalable through, uh,
making matrices W, block diagonal or using
it as a- as a linear combination of basis transformations.
So this kind of dictionary learning approach.
And of course, you can take out GCN and extend it in any way you like, right?
You can- you could think that you have- you know,
you could have RGNN,
you could have RGraphSAGE,
you could have R graph attention networks.
So all these, ah,
basic fundamental GNN architectures that we already discussed.
It's kind of natural to extend them to the- to this, er,
multi-relational case, ah, by- by
basically adding relation specific, uh, transformations.
So it's kind of natural
that- how you can play with this model and make it more expressive,
and richer and so on.
